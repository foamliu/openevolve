{"id": "f85b78c7-ca61-490e-a5ca-9e10ec43b63c", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local search for global optimization.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with multiple starting points\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Find best initial candidate\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    cooling_rate = 0.995\n    min_temp = 0.001\n    \n    # Local search parameters\n    local_search_freq = 50\n    local_search_steps = 20\n    step_size = 0.1\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration phase\n            step_scale = (bounds[1] - bounds[0]) * 0.1\n        else:\n            # Local exploitation phase\n            step_scale = (bounds[1] - bounds[0]) * 0.01\n        \n        dx = np.random.normal(0, step_scale)\n        dy = np.random.normal(0, step_scale)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp((best_value - new_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature\n        temp = max(temp * cooling_rate, min_temp)\n        \n        # Periodic local search refinement\n        if i % local_search_freq == 0 and i > 0:\n            local_best_x, local_best_y = best_x, best_y\n            local_best_value = best_value\n            \n            for _ in range(local_search_steps):\n                # Small random steps in neighborhood\n                lx = np.clip(local_best_x + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                ly = np.clip(local_best_y + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                \n                if lvalue < local_best_value:\n                    local_best_x, local_best_y = lx, ly\n                    local_best_value = lvalue\n            \n            # Update global best if local search found better solution\n            if local_best_value < best_value:\n                best_x, best_y, best_value = local_best_x, local_best_y, local_best_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "d9ae64c9-edad-4b0a-a4cc-6428a503ecc4", "generation": 1, "timestamp": 1765003004.0133848, "iteration_found": 4, "metrics": {"runs_successfully": 1.0, "value_score": 0.9063194016353092, "distance_score": 0.20160621337523205, "combined_score": 0.49954730106502265}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 84 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9417587299889709, "distance_score": 0.7565915427363771, "combined_score": 1.2147685971231066}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.2148\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 1.2148\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current algorithm, it's a simple random search that samples points uniformly across the entire search space. This approach is inefficient for finding global minima in complex functions with many local minima because:\n\n1. It doesn't use any information from previous evaluations to guide the search\n2. It has no mechanism to escape local minima\n3. It wastes iterations on unpromising regions\n\nI'll implement a hybrid approach combining simulated annealing with local search to improve exploration and exploitation:\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local search for global optimization.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with multiple starting points\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Find best initial candidate\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    cooling_rate = 0.995\n    min_temp = 0.001\n    \n    # Local search parameters\n    local_search_freq = 50\n    local_search_steps = 20\n    step_size = 0.1\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration phase\n            step_scale = (bounds[1] - bounds[0]) * 0.1\n        else:\n            # Local exploitation phase\n            step_scale = (bounds[1] - bounds[0]) * 0.01\n        \n        dx = np.random.normal(0, step_scale)\n        dy = np.random.normal(0, step_scale)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp((best_value - new_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature\n        temp = max(temp * cooling_rate, min_temp)\n        \n        # Periodic local search refinement\n        if i % local_search_freq == 0 and i > 0:\n            local_best_x, local_best_y = best_x, best_y\n            local_best_value = best_value\n            \n            for _ in range(local_search_steps):\n                # Small random steps in neighborhood\n                lx = np.clip(local_best_x + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                ly = np.clip(local_best_y + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                \n                if lvalue < local_best_value:\n                    local_best_x, local_best_y = lx, ly\n                    local_best_value = lvalue\n            \n            # Update global best if local search found better solution\n            if local_best_value < best_value:\n                best_x, best_y, best_value = local_best_x, local_best_y, local_best_value\n    \n    return best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\nThe key improvements in this implementation:\n\n1. **Multi-start initialization**: Instead of starting from a single random point, we start with 5 candidates and pick the best one, reducing the chance of starting in a poor region.\n\n2. **Simulated annealing**: This allows the algorithm to accept worse solutions with decreasing probability, helping escape local minima in early iterations while converging in later iterations.\n\n3. **Adaptive step sizes**: The algorithm uses larger steps early on for global exploration, then smaller steps later for local exploitation.\n\n4. **Periodic local search**: Every 50 iterations, we perform a focused local search around the current best solution to refine it.\n\n5. **Temperature-based acceptance**: The cooling schedule ensures we explore broadly early on but converge to a good solution.\n\n6. **Boundary handling**: New candidates are clipped to stay within bounds, preventing invalid evaluations.\n\nThis hybrid approach balances exploration and exploitation more effectively than pure random search, making it more likely to find the global minimum while still being computationally efficient."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=1.5789, y=2.8928 with value=-1.4156\", \"distance_to_global\": \"3.9602\", \"solution_quality\": \"Could be improved\"}", "artifact_dir": null, "embedding": null}