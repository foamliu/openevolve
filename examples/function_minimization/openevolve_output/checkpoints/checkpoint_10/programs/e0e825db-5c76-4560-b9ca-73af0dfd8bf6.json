{"id": "e0e825db-5c76-4560-b9ca-73af0dfd8bf6", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with adaptive local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive exploration-exploitation balance\n    exploration_ratio = 0.7  # 70% exploration, 30% exploitation\n    exploration_cutoff = int(iterations * exploration_ratio)\n    \n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration - maintain random search strength\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation around best solution\n            # Adaptive step size that decreases as we get closer to optimum\n            step_size = (bounds[1] - bounds[0]) * 0.1 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final intensive local search\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "d9ae64c9-edad-4b0a-a4cc-6428a503ecc4", "generation": 1, "timestamp": 1765003156.1200356, "iteration_found": 9, "metrics": {"runs_successfully": 1.0, "value_score": 0.999684285695686, "distance_score": 0.998899190462244, "combined_score": 1.4992678499797742, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 61 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9417587299889709, "distance_score": 0.7565915427363771, "combined_score": 1.2147685971231066}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.2148\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.7464 \u2192 1.2148\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 87 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9154, distance_score: 0.2955, combined_score: 0.7464, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 65 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9295, distance_score: 0.4380, combined_score: 0.9554, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n### Program 2 (Score: 0.9554)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9295), Performs well on distance_score (0.4380), Performs well on combined_score (0.9554), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 0.7464)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search,\n    and multi-start random search to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    def local_search(x, y, radius, steps=20):\n        \"\"\"Local gradient-free search around a point\"\"\"\n        best_local = evaluate_function(x, y)\n        best_x, best_y = x, y\n        \n        for _ in range(steps):\n            angle = np.random.uniform(0, 2 * np.pi)\n            r = np.random.uniform(0, radius)\n            new_x = x + r * np.cos(angle)\n            new_y = y + r * np.sin(angle)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            value = evaluate_function(new_x, new_y)\n            if value < best_local:\n                best_local = value\n                best_x, best_y = new_x, new_y\n        \n        return best_x, best_y, best_local\n    \n    # Multi-start approach with 5 initial points\n    num_starts = 5\n    best_overall = float('inf')\n    best_x_overall, best_y_overall = 0, 0\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Simulated annealing parameters\n        temp = 1.0\n        temp_min = 0.001\n        alpha = 0.95\n        \n        steps_per_temp = iterations // (num_starts * 20)\n        \n        while temp > temp_min:\n            for _ in range(steps_per_temp):\n                # Generate neighbor with decreasing radius\n                radius = temp * (bounds[1] - bounds[0]) * 0.1\n                angle = np.random.uniform(0, 2 * np.pi)\n                r = np.random.uniform(0, radius)\n                \n                new_x = x + r * np.cos(angle)\n                new_y = y + r * np.sin(angle)\n                \n                # Keep within bounds\n                new_x = np.clip(new_x, bounds[0], bounds[1])\n                new_y = np.clip(new_y, bounds[0], bounds[1])\n                \n                new_value = evaluate_function(new_x, new_y)\n                \n                # Accept if better, or with probability if worse\n                if new_value < current_value:\n                    x, y, current_value = new_x, new_y, new_value\n                else:\n                    prob = np.exp(-(new_value - current_value) / temp)\n                    if np.random.random() < prob:\n                        x, y, current_value = new_x, new_y, new_value\n            \n            temp *= alpha\n        \n        # Local search refinement\n        x, y, current_value = local_search(x, y, radius=(bounds[1] - bounds[0]) * 0.05)\n        \n        # Update global best\n        if current_value < best_overall:\n            best_overall = current_value\n            best_x_overall, best_y_overall = x, y\n    \n    return best_x_overall, best_y_overall, best_overall\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9154), Performs well on distance_score (0.2955), Performs well on combined_score (0.7464), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3310)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_step = 0.1\n    step_decay = 0.99\n    min_step = 0.001\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.5:\n            # Global exploration\n            if i % 5 == 0:\n                # Multi-start injection\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                x = best_x + np.random.uniform(-1, 1)\n                y = best_y + np.random.uniform(-1, 1)\n        else:\n            # Local refinement\n            x = best_x + np.random.uniform(-local_step, local_step)\n            y = best_y + np.random.uniform(-local_step, local_step)\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        # Accept or reject based on simulated annealing\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n        else:\n            # Accept worse solution with probability\n            delta = value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_value = value\n                best_x, best_y = x, y\n        \n        # Update parameters\n        temp = max(min_temp, temp * temp_decay)\n        local_step = max(min_step, local_step * step_decay)\n    \n    # Final local search refinement\n    for _ in range(100):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3310, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_step = 0.1\n    step_decay = 0.99\n    min_step = 0.001\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.5:\n            # Global exploration\n            if i % 5 == 0:\n                # Multi-start injection\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                x = best_x + np.random.uniform(-1, 1)\n                y = best_y + np.random.uniform(-1, 1)\n        else:\n            # Local refinement\n            x = best_x + np.random.uniform(-local_step, local_step)\n            y = best_y + np.random.uniform(-local_step, local_step)\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        # Accept or reject based on simulated annealing\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n        else:\n            # Accept worse solution with probability\n            delta = value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_value = value\n                best_x, best_y = x, y\n        \n        # Update parameters\n        temp = max(min_temp, temp * temp_decay)\n        local_step = max(min_step, local_step * step_decay)\n    \n    # Final local search refinement\n    for _ in range(100):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 84 lines, Excellent runs_successfully (1.000), Alternative distance_score approach\n\n### Inspiration 2 (Score: 0.9554, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 65 lines, Excellent runs_successfully (1.000), Excellent value_score (0.930)\n\n### Inspiration 3 (Score: 0.7464, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search,\n    and multi-start random search to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    def local_search(x, y, radius, steps=20):\n        \"\"\"Local gradient-free search around a point\"\"\"\n        best_local = evaluate_function(x, y)\n        best_x, best_y = x, y\n        \n        for _ in range(steps):\n            angle = np.random.uniform(0, 2 * np.pi)\n            r = np.random.uniform(0, radius)\n            new_x = x + r * np.cos(angle)\n            new_y = y + r * np.sin(angle)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            value = evaluate_function(new_x, new_y)\n            if value < best_local:\n                best_local = value\n                best_x, best_y = new_x, new_y\n        \n        return best_x, best_y, best_local\n    \n    # Multi-start approach with 5 initial points\n    num_starts = 5\n    best_overall = float('inf')\n    best_x_overall, best_y_overall = 0, 0\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Simulated annealing parameters\n        temp = 1.0\n        temp_min = 0.001\n        alpha = 0.95\n        \n        steps_per_temp = iterations // (num_starts * 20)\n        \n        while temp > temp_min:\n            for _ in range(steps_per_temp):\n                # Generate neighbor with decreasing radius\n                radius = temp * (bounds[1] - bounds[0]) * 0.1\n                angle = np.random.uniform(0, 2 * np.pi)\n                r = np.random.uniform(0, radius)\n                \n                new_x = x + r * np.cos(angle)\n                new_y = y + r * np.sin(angle)\n                \n                # Keep within bounds\n                new_x = np.clip(new_x, bounds[0], bounds[1])\n                new_y = np.clip(new_y, bounds[0], bounds[1])\n                \n                new_value = evaluate_function(new_x, new_y)\n                \n                # Accept if better, or with probability if worse\n                if new_value < current_value:\n                    x, y, current_value = new_x, new_y, new_value\n                else:\n                    prob = np.exp(-(new_value - current_value) / temp)\n                    if np.random.random() < prob:\n                        x, y, current_value = new_x, new_y, new_value\n            \n            temp *= alpha\n        \n        # Local search refinement\n        x, y, current_value = local_search(x, y, radius=(bounds[1] - bounds[0]) * 0.05)\n        \n        # Update global best\n        if current_value < best_overall:\n            best_overall = current_value\n            best_x_overall, best_y_overall = x, y\n    \n    return best_x_overall, best_y_overall, best_overall\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 87 lines, Excellent runs_successfully (1.000), Excellent value_score (0.915)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the evolution history, I can see that the current simple random search algorithm has a fitness score of 1.2148, but the more sophisticated algorithms (Attempt 2 and 3) actually performed worse despite using advanced techniques like simulated annealing. This suggests that complexity alone doesn't guarantee better performance.\n\nThe key insight is that the current algorithm's strength lies in its pure random exploration, which helps avoid getting trapped in local minima. However, it lacks any exploitation phase to refine good solutions. Let me improve it by adding a simple but effective hybrid approach that maintains the global exploration strength while adding local refinement capabilities.\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with adaptive local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive exploration-exploitation balance\n    exploration_ratio = 0.7  # 70% exploration, 30% exploitation\n    exploration_cutoff = int(iterations * exploration_ratio)\n    \n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration - maintain random search strength\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation around best solution\n            # Adaptive step size that decreases as we get closer to optimum\n            step_size = (bounds[1] - bounds[0]) * 0.1 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final intensive local search\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\n**Reasoning for the improvements:**\n\n1. **Multi-start initialization**: Starting with 3 diverse random points instead of 1 gives us a better chance of finding promising regions of the search space.\n\n2. **Exploration-exploitation balance**: The algorithm spends 70% of iterations on pure random search (maintaining the strength of the original approach) and 30% on local refinement around the best solution found.\n\n3. **Adaptive step size**: During the exploitation phase, the step size gradually decreases, allowing for finer search as we converge toward the optimum.\n\n4. **Final intensive local search**: A dedicated final refinement phase with very small steps helps polish the solution.\n\n5. **Maintained simplicity**: Unlike the more complex algorithms that performed worse, this approach keeps the core simplicity while adding just enough sophistication to improve results.\n\nThe key insight is that the original algorithm's strength is its pure randomness, which prevents getting stuck in local minima. By preserving this characteristic for the majority of the search and only adding targeted local refinement at the end, we maintain this advantage while improving the precision of the final solution."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7052, y=0.6772 with value=-1.5187\", \"distance_to_global\": \"0.0014\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7026, y=0.6781\", \"average_distance_to_global\": \"0.0011\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}