{"id": "317ecfaf-fe1f-4741-b7fe-f15ba0864e6c", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    # Track search diversity to guide restart strategy\n    restart_positions = [(best_global_x, best_global_y)]\n    \n    for restart in range(num_restarts):\n        # Intelligent restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Smart restart: avoid previously explored regions\n            attempts = 0\n            while attempts < 10:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n                \n                # Check distance from previous restarts\n                min_distance = min(np.sqrt((x - rx)**2 + (y - ry)**2) for rx, ry in restart_positions)\n                if min_distance > 1.0 or attempts == 9:  # Ensure some diversity\n                    break\n                attempts += 1\n            \n            restart_positions.append((x, y))\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Enhanced temperature schedule with adaptive exploration\n        temp = 5.0 if restart == 0 else 3.0  # Higher temp for first restart\n        temp_decay = 0.98 if i < restart_iterations * 0.5 else 0.95  # Slower cooling early\n        \n        for i in range(restart_iterations):\n            # Adaptive step size with momentum-based exploration\n            progress = i / restart_iterations\n            step_scale = max(0.05, 3.0 * (1 - progress) ** 2)  # Non-linear decay\n            \n            # Dynamic exploration strategy based on improvement history\n            if i % 10 == 0 and i > 50:  # Check every 10 iterations\n                recent_improvement = (best_restart_value - current_value) / abs(best_restart_value)\n                if recent_improvement < 0.001:  # Stuck in local minimum\n                    step_scale *= 2.0  # Increase exploration\n            \n            # Generate candidate with adaptive exploration\n            if progress < 0.6:\n                # Global exploration with occasional large jumps\n                if np.random.random() < 0.1:  # 10% chance of large jump\n                    dx = np.random.normal(0, step_scale * 5)\n                    dy = np.random.normal(0, step_scale * 5)\n                else:\n                    dx = np.random.normal(0, step_scale)\n                    dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement with smaller, more focused steps\n                dx = np.random.normal(0, step_scale * 0.2)\n                dy = np.random.normal(0, step_scale * 0.2)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance criteria with adaptive threshold\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Adaptive acceptance of worse solutions\n                delta = new_value - current_value\n                normalized_delta = delta / (abs(current_value) + 1e-10)\n                \n                # Adjust acceptance probability based on search progress\n                if progress < 0.3:  # Early exploration phase\n                    acceptance_boost = 1.5\n                elif progress < 0.7:  # Middle phase\n                    acceptance_boost = 1.0\n                else:  # Late refinement phase\n                    acceptance_boost = 0.5\n                \n                prob = np.exp(-normalized_delta * acceptance_boost / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "542fc262-1df9-4fa8-8838-52cc92c0146f", "generation": 2, "timestamp": 1765020349.192652, "iteration_found": 11, "metrics": {"runs_successfully": 0.0, "combined_score": 0.0, "error": "local variable 'i' referenced before assignment"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 3 lines\nChange 2: Replace 12 lines with 23 lines\nChange 3: Replace 7 lines with 21 lines\nChange 4: Replace 12 lines with 22 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9996084538159868, "distance_score": 0.9921575100905551, "combined_score": 1.4961772199027399, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.4962\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 1.3441 \u2192 1.4962\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.6990, y=0.6800 with value=-1.5187\n```\n\n### distance_to_global\n```\n0.0054\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.7136, y=0.6869\n```\n\n### average_distance_to_global\n```\n0.0079\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 56 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9824, distance_score: 0.6829, combined_score: 1.3441, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 77 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9993, distance_score: 0.9813, combined_score: 1.4910, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 80 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9996, distance_score: 0.9922, combined_score: 1.4962, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4962)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9996), Performs well on distance_score (0.9922), Performs well on combined_score (1.4962), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4910)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining multiple strategies\n    to escape local minima and find the global minimum.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start strategy: maintain multiple candidates\n    num_candidates = 5\n    candidates = []\n    for _ in range(num_candidates):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Adaptive step size\n    step_size = (bounds[1] - bounds[0]) * 0.1\n    min_step_size = (bounds[1] - bounds[0]) * 0.001\n    \n    # Track best solution\n    best_idx = min(range(num_candidates), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n        \n        # Try to improve each candidate\n        for i in range(num_candidates):\n            x, y, value = candidates[i]\n            \n            # Local refinement with adaptive step\n            if iter > iterations // 4:  # Start local refinement after initial exploration\n                dx = np.random.normal(0, step_size)\n                dy = np.random.normal(0, step_size)\n            else:\n                # Global search with larger steps\n                dx = np.random.uniform(-step_size * 3, step_size * 3)\n                dy = np.random.uniform(-step_size * 3, step_size * 3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            delta = new_value - value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[i] = (new_x, new_y, new_value)\n                \n                # Update global best\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n                    # Reduce step size when we find a better solution\n                    step_size = max(step_size * 0.9, min_step_size)\n        \n        # Occasional random restart to escape deep local minima\n        if iter % (iterations // 10) == 0 and iter > 0:\n            worst_idx = max(range(num_candidates), key=lambda i: candidates[i][2])\n            candidates[worst_idx] = (\n                np.random.uniform(bounds[0], bounds[1]),\n                np.random.uniform(bounds[0], bounds[1]),\n                evaluate_function(candidates[worst_idx][0], candidates[worst_idx][1])\n            )\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9993), Performs well on distance_score (0.9813), Performs well on combined_score (1.4910), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.3441)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * (1 - i / (iterations // num_starts))  # Decrease step size\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down temperature\n            temp *= cooling_rate\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9824), Performs well on distance_score (0.6829), Performs well on combined_score (1.3441), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.2721)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 3 candidates for better coverage\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    cooling_rate = 0.995\n    \n    for i in range(iterations):\n        temp = initial_temp * (cooling_rate ** i)\n        \n        for j, (x, y, current_value) in enumerate(candidates):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/iterations))\n            \n            # Local perturbation with Gaussian noise\n            new_x = x + np.random.normal(0, step_scale)\n            new_y = y + np.random.normal(0, step_scale)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability if worse (simulated annealing)\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                candidates[j] = (new_x, new_y, new_value)\n        \n        # Occasional long-range exploration\n        if i % 50 == 0 and i > 0:\n            idx = np.random.randint(0, 3)\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            value = evaluate_function(x, y)\n            candidates[idx] = (x, y, value)\n    \n    # Return the best candidate found\n    best_candidate = min(candidates, key=lambda x: x[2])\n    return best_candidate\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.2910)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing and multi-start local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start strategy with 5 initial points\n    num_starts = 5\n    best_global_value = float('inf')\n    best_global_x, best_global_y = 0, 0\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_local_value = current_value\n        best_local_x, best_local_y = x, y\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.1\n        cooling_rate = (initial_temp / final_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on temperature\n            step_size = min(1.0, temp / initial_temp) * (bounds[1] - bounds[0]) * 0.1\n            \n            # Generate candidate with local and global components\n            if np.random.random() < 0.7:  # 70% local search\n                dx = np.random.normal(0, step_size)\n                dy = np.random.normal(0, step_size)\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n            else:  # 30% global search\n                new_x = np.random.uniform(bounds[0], bounds[1])\n                new_y = np.random.uniform(bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_local_value:\n                    best_local_value = new_value\n                    best_local_x, best_local_y = new_x, new_y\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                probability = np.exp((current_value - new_value) / temp)\n                if np.random.random() < probability:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp /= cooling_rate\n        \n        # Update global best\n        if best_local_value < best_global_value:\n            best_global_value = best_local_value\n            best_global_x, best_global_y = best_local_x, best_local_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.3441, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * (1 - i / (iterations // num_starts))  # Decrease step size\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down temperature\n            temp *= cooling_rate\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 56 lines, Excellent runs_successfully (1.000), Excellent value_score (0.982)\n\n### Inspiration 2 (Score: 1.4910, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining multiple strategies\n    to escape local minima and find the global minimum.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start strategy: maintain multiple candidates\n    num_candidates = 5\n    candidates = []\n    for _ in range(num_candidates):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Adaptive step size\n    step_size = (bounds[1] - bounds[0]) * 0.1\n    min_step_size = (bounds[1] - bounds[0]) * 0.001\n    \n    # Track best solution\n    best_idx = min(range(num_candidates), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n        \n        # Try to improve each candidate\n        for i in range(num_candidates):\n            x, y, value = candidates[i]\n            \n            # Local refinement with adaptive step\n            if iter > iterations // 4:  # Start local refinement after initial exploration\n                dx = np.random.normal(0, step_size)\n                dy = np.random.normal(0, step_size)\n            else:\n                # Global search with larger steps\n                dx = np.random.uniform(-step_size * 3, step_size * 3)\n                dy = np.random.uniform(-step_size * 3, step_size * 3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            delta = new_value - value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[i] = (new_x, new_y, new_value)\n                \n                # Update global best\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n                    # Reduce step size when we find a better solution\n                    step_size = max(step_size * 0.9, min_step_size)\n        \n        # Occasional random restart to escape deep local minima\n        if iter % (iterations // 10) == 0 and iter > 0:\n            worst_idx = max(range(num_candidates), key=lambda i: candidates[i][2])\n            candidates[worst_idx] = (\n                np.random.uniform(bounds[0], bounds[1]),\n                np.random.uniform(bounds[0], bounds[1]),\n                evaluate_function(candidates[worst_idx][0], candidates[worst_idx][1])\n            )\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 77 lines, Excellent runs_successfully (1.000), Excellent value_score (0.999)\n\n### Inspiration 3 (Score: 1.2910, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing and multi-start local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start strategy with 5 initial points\n    num_starts = 5\n    best_global_value = float('inf')\n    best_global_x, best_global_y = 0, 0\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_local_value = current_value\n        best_local_x, best_local_y = x, y\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.1\n        cooling_rate = (initial_temp / final_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on temperature\n            step_size = min(1.0, temp / initial_temp) * (bounds[1] - bounds[0]) * 0.1\n            \n            # Generate candidate with local and global components\n            if np.random.random() < 0.7:  # 70% local search\n                dx = np.random.normal(0, step_size)\n                dy = np.random.normal(0, step_size)\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n            else:  # 30% global search\n                new_x = np.random.uniform(bounds[0], bounds[1])\n                new_y = np.random.uniform(bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_local_value:\n                    best_local_value = new_value\n                    best_local_x, best_local_y = new_x, new_y\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                probability = np.exp((current_value - new_value) / temp)\n                if np.random.random() < probability:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp /= cooling_rate\n        \n        # Update global best\n        if best_local_value < best_global_value:\n            best_global_value = best_local_value\n            best_global_x, best_global_y = best_local_x, best_local_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 70 lines, Excellent runs_successfully (1.000), Excellent value_score (0.978)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance history, I can see that the algorithm is already performing quite well (fitness score of 1.4962). However, there are several targeted improvements that could help push it even closer to the global optimum and improve reliability.\n\nThe main issues I identify are:\n\n1. **Limited temperature range**: The current temperature starts at 2.0 and decays by 0.95, which may not provide enough exploration early on or enough refinement late in the search.\n\n2. **Fixed restart strategy**: The algorithm uses a simple sequential restart approach that doesn't adapt based on search progress.\n\n3. **Step size adaptation**: While there's some adaptive step sizing, it could be more sophisticated to better balance exploration vs exploitation.\n\nHere are my targeted improvements:\n\n## Improvement 1: Enhanced Temperature Schedule and Adaptive Exploration\n\n```python\n<<<<<<< SEARCH\n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n=======\n        # Enhanced temperature schedule with adaptive exploration\n        temp = 5.0 if restart == 0 else 3.0  # Higher temp for first restart\n        temp_decay = 0.98 if i < restart_iterations * 0.5 else 0.95  # Slower cooling early\n>>>>>>> REPLACE\n```\n\n## Improvement 2: Smarter Step Size Strategy with Momentum\n\n```python\n<<<<<<< SEARCH\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n=======\n            # Adaptive step size with momentum-based exploration\n            progress = i / restart_iterations\n            step_scale = max(0.05, 3.0 * (1 - progress) ** 2)  # Non-linear decay\n            \n            # Dynamic exploration strategy based on improvement history\n            if i % 10 == 0 and i > 50:  # Check every 10 iterations\n                recent_improvement = (best_restart_value - current_value) / abs(best_restart_value)\n                if recent_improvement < 0.001:  # Stuck in local minimum\n                    step_scale *= 2.0  # Increase exploration\n            \n            # Generate candidate with adaptive exploration\n            if progress < 0.6:\n                # Global exploration with occasional large jumps\n                if np.random.random() < 0.1:  # 10% chance of large jump\n                    dx = np.random.normal(0, step_scale * 5)\n                    dy = np.random.normal(0, step_scale * 5)\n                else:\n                    dx = np.random.normal(0, step_scale)\n                    dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement with smaller, more focused steps\n                dx = np.random.normal(0, step_scale * 0.2)\n                dy = np.random.normal(0, step_scale * 0.2)\n>>>>>>> REPLACE\n```\n\n## Improvement 3: Intelligent Restart Strategy with Memory\n\n```python\n<<<<<<< SEARCH\n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n=======\n    # Track search diversity to guide restart strategy\n    restart_positions = [(best_global_x, best_global_y)]\n    \n    for restart in range(num_restarts):\n        # Intelligent restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Smart restart: avoid previously explored regions\n            attempts = 0\n            while attempts < 10:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n                \n                # Check distance from previous restarts\n                min_distance = min(np.sqrt((x - rx)**2 + (y - ry)**2) for rx, ry in restart_positions)\n                if min_distance > 1.0 or attempts == 9:  # Ensure some diversity\n                    break\n                attempts += 1\n            \n            restart_positions.append((x, y))\n>>>>>>> REPLACE\n```\n\n## Improvement 4: Enhanced Acceptance Criteria with Adaptive Threshold\n\n```python\n<<<<<<< SEARCH\n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n=======\n            # Enhanced acceptance criteria with adaptive threshold\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Adaptive acceptance of worse solutions\n                delta = new_value - current_value\n                normalized_delta = delta / (abs(current_value) + 1e-10)\n                \n                # Adjust acceptance probability based on search progress\n                if progress < 0.3:  # Early exploration phase\n                    acceptance_boost = 1.5\n                elif progress < 0.7:  # Middle phase\n                    acceptance_boost = 1.0\n                else:  # Late refinement phase\n                    acceptance_boost = 0.5\n                \n                prob = np.exp(-normalized_delta * acceptance_boost / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n>>>>>>> REPLACE\n```\n\nThese improvements aim to:\n\n1. **Better temperature management**: Higher initial temperatures and adaptive cooling rates provide better exploration early on while allowing for finer refinement later.\n\n2. **Smarter exploration**: The momentum-based step size adaptation and occasional large jumps help escape local minima more effectively.\n\n3. **Intelligent restart strategy**: Avoiding previously explored regions ensures better coverage of the search space.\n\n4. **Adaptive acceptance criteria**: The normalized delta and progress-based acceptance boost help balance exploration vs exploitation throughout the search process.\n\nThese changes maintain the overall structure of the successful algorithm while adding more sophisticated adaptation mechanisms that should help find the global minimum more reliably, especially for functions with many local minima."]}}, "artifacts_json": "{\"error_type\": \"UnboundLocalError\", \"error_message\": \"Stage 1: local variable 'i' referenced before assignment\", \"full_traceback\": \"Traceback (most recent call last):\\n  File \\\"D:\\\\Users\\\\Administrator\\\\openevolve\\\\examples/function_minimization/evaluator.py\\\", line 294, in evaluate_stage1\\n    result = run_with_timeout(program.run_search, timeout_seconds=5)\\n  File \\\"D:\\\\Users\\\\Administrator\\\\openevolve\\\\examples/function_minimization/evaluator.py\\\", line 30, in run_with_timeout\\n    result = future.result(timeout=timeout_seconds)\\n  File \\\"C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\concurrent\\\\futures\\\\_base.py\\\", line 451, in result\\n    return self.__get_result()\\n  File \\\"C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\concurrent\\\\futures\\\\_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n  File \\\"C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\concurrent\\\\futures\\\\thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n  File \\\"D:\\\\TEMP\\\\tmpf1y_p41a.py\\\", line 133, in run_search\\n    x, y, value = search_algorithm()\\n  File \\\"D:\\\\TEMP\\\\tmpf1y_p41a.py\\\", line 53, in search_algorithm\\n    temp_decay = 0.98 if i < restart_iterations * 0.5 else 0.95  # Slower cooling early\\nUnboundLocalError: local variable 'i' referenced before assignment\\n\", \"suggestion\": \"Unexpected error occurred. Check the traceback for specific issue\"}", "artifact_dir": null, "embedding": null}