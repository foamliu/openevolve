{"id": "f8f10638-ccb5-41a8-9ee2-d73351f1f2e8", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 4 diverse points\n    candidates = []\n    for _ in range(4):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    x, y = best_x, best_y\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    no_improvement_count = 0\n    \n    for i in range(iterations):\n        # Occasional random restart to escape deep local minima\n        if no_improvement_count > 100 and i < iterations * 0.8:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            no_improvement_count = 0\n        \n        # Adaptive step size based on temperature and progress\n        progress = i / iterations\n        base_sigma = 2.0 * T / T_init + 0.1\n        # Increase exploration early, focus later\n        if progress < 0.3:\n            sigma = base_sigma * 1.5\n        elif progress < 0.7:\n            sigma = base_sigma\n        else:\n            sigma = base_sigma * 0.5\n        \n        # Local Gaussian move\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        current_value = evaluate_function(x, y)\n        delta = value - current_value\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n        else:\n            no_improvement_count += 1\n\n        T *= decay\n\n    # Intensive local refinement phase\n    refinement_iterations = min(100, iterations // 10)\n    for _ in range(refinement_iterations):\n        # Very small steps around best solution\n        step_size = 0.01\n        x_new = best_x + np.random.uniform(-step_size, step_size)\n        y_new = best_y + np.random.uniform(-step_size, step_size)\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n        \n        value = evaluate_function(x_new, y_new)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x_new, y_new\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "9157d41a-ff1a-4abf-9201-7a4b2d418118", "generation": 2, "timestamp": 1765003493.4056044, "iteration_found": 15, "metrics": {"runs_successfully": 1.0, "value_score": 0.9804317511218716, "distance_score": 0.5556757697633085, "combined_score": 1.028302327787914, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 11 lines\nChange 2: Replace 20 lines with 43 lines\nChange 3: Replace return best_x, best_y, best_value with 16 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.8494633242189635, "distance_score": 0.7265767136253556, "combined_score": 1.1204321519741285}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.1204\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.3431 \u2192 1.1204. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.9291, y=0.3764 with value=-1.3418\n```\n\n### distance_to_global\n```\n0.3763\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 55 lines\nChange 2: Replace return best_x, best_y, best_value with 27 lines\nChange 3: Replace 3 lines with 5 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9781, distance_score: 0.6879, combined_score: 1.3431, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 75 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9900, distance_score: 0.7141, combined_score: 1.3638, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 61 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9989, combined_score: 1.4993, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4993)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with adaptive local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive exploration-exploitation balance\n    exploration_ratio = 0.7  # 70% exploration, 30% exploitation\n    exploration_cutoff = int(iterations * exploration_ratio)\n    \n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration - maintain random search strength\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation around best solution\n            # Adaptive step size that decreases as we get closer to optimum\n            step_size = (bounds[1] - bounds[0]) * 0.1 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final intensive local search\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9989), Performs well on combined_score (1.4993), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.3638)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Improved random search with adaptive local exploration and momentum.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n    \n    # Track recent improvements for momentum\n    recent_improvements = []\n    no_improvement_count = 0\n    \n    for i in range(iterations):\n        # Adaptive radius based on progress\n        progress = i / iterations\n        base_radius = (bounds[1] - bounds[0]) * (1 - progress * 0.9)\n        \n        # Mix global and local search based on recent progress\n        if no_improvement_count > 50 or i < iterations * 0.1:\n            # Global exploration when stuck or early in search\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploration around best point\n            angle = np.random.uniform(0, 2 * np.pi)\n            radius = np.random.uniform(0, base_radius * 0.1)\n            x = best_x + radius * np.cos(angle)\n            y = best_y + radius * np.sin(angle)\n            \n            # Add momentum from recent improvements\n            if len(recent_improvements) >= 2:\n                dx = recent_improvements[-1][0] - recent_improvements[-2][0]\n                dy = recent_improvements[-1][1] - recent_improvements[-2][1]\n                momentum_strength = min(0.5, base_radius * 0.05)\n                x += dx * momentum_strength\n                y += dy * momentum_strength\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            # Store improvement for momentum calculation\n            recent_improvements.append((x, y))\n            if len(recent_improvements) > 5:\n                recent_improvements.pop(0)\n            \n            best_value = value\n            best_x, best_y = x, y\n            no_improvement_count = 0\n        else:\n            no_improvement_count += 1\n    \n    # Final local refinement\n    for _ in range(min(100, iterations // 10)):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9900), Performs well on distance_score (0.7141), Performs well on combined_score (1.3638), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.3431)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = 0.95\n    \n    best_global = min(candidates, key=lambda x: x[2])\n    best_x, best_y, best_value = best_global\n    \n    for start_idx in range(num_starts):\n        x, y, current_value = candidates[start_idx]\n        temp = initial_temp\n        \n        for step in range(iterations // num_starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, 0.5 + temp / initial_temp)\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance probability\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            \n            # Cool down\n            temp = max(final_temp, temp * cooling_rate)\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update with momentum\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9781), Performs well on distance_score (0.6879), Performs well on combined_score (1.3431), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.1204)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4993, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with adaptive local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive exploration-exploitation balance\n    exploration_ratio = 0.7  # 70% exploration, 30% exploitation\n    exploration_cutoff = int(iterations * exploration_ratio)\n    \n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration - maintain random search strength\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation around best solution\n            # Adaptive step size that decreases as we get closer to optimum\n            step_size = (bounds[1] - bounds[0]) * 0.1 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final intensive local search\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 61 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 2 (Score: 1.0141, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with local exploitation and multi-start strategy.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start from best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive search strategy\n    for i in range(iterations):\n        # Mix global exploration and local exploitation\n        if i < iterations // 2:\n            # Global exploration phase\n            if i % 10 == 0:\n                # Occasional random restart to escape local minima\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random steps\n                step_size = (bounds[1] - bounds[0]) * 0.2\n                x = best_x + np.random.uniform(-step_size, step_size)\n                y = best_y + np.random.uniform(-step_size, step_size)\n        else:\n            # Local exploitation phase\n            step_size = (bounds[1] - bounds[0]) * 0.05 * (1 - i/iterations)\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        # Accept if better (no worse solutions to avoid getting stuck)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final local refinement\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 66 lines, Excellent runs_successfully (1.000), Excellent value_score (0.956)\n\n### Inspiration 3 (Score: 0.9554, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 65 lines, Excellent runs_successfully (1.000), Excellent value_score (0.930)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the history, I can see that the fitness declined from 1.3431 to 1.1204. The current implementation uses simulated annealing but appears to be getting stuck in local minima. Let me analyze the top performers and suggest improvements.\n\nFrom the top performers, I notice that Program 1 (Score: 1.4993) uses a hybrid approach with multi-start initialization, adaptive exploration-exploitation balance, and intensive local refinement. The key insight is that pure simulated annealing needs better exploration strategies and more aggressive local refinement.\n\nHere are my improvements:\n\n## Change 1: Multi-start initialization with better diversity\n\nThe current single-start approach is prone to getting stuck. Multi-start initialization provides better coverage of the search space.\n\n```python\n<<<<<<< SEARCH\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n=======\n    # Multi-start initialization with 4 diverse points\n    candidates = []\n    for _ in range(4):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    x, y = best_x, best_y\n>>>>>>> REPLACE\n```\n\n## Change 2: Adaptive exploration-exploitation with occasional restarts\n\nThe current SA gets stuck because it doesn't have mechanisms to escape deep local minima. Adding occasional random restarts and adaptive step sizes helps.\n\n```python\n<<<<<<< SEARCH\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n=======\n    T = T_init\n    no_improvement_count = 0\n    \n    for i in range(iterations):\n        # Occasional random restart to escape deep local minima\n        if no_improvement_count > 100 and i < iterations * 0.8:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            no_improvement_count = 0\n        \n        # Adaptive step size based on temperature and progress\n        progress = i / iterations\n        base_sigma = 2.0 * T / T_init + 0.1\n        # Increase exploration early, focus later\n        if progress < 0.3:\n            sigma = base_sigma * 1.5\n        elif progress < 0.7:\n            sigma = base_sigma\n        else:\n            sigma = base_sigma * 0.5\n        \n        # Local Gaussian move\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        current_value = evaluate_function(x, y)\n        delta = value - current_value\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n        else:\n            no_improvement_count += 1\n\n        T *= decay\n>>>>>>> REPLACE\n```\n\n## Change 3: Intensive local refinement phase\n\nThe top performers all include a final intensive local search phase to fine-tune the solution.\n\n```python\n<<<<<<< SEARCH\n    return best_x, best_y, best_value\n=======\n    # Intensive local refinement phase\n    refinement_iterations = min(100, iterations // 10)\n    for _ in range(refinement_iterations):\n        # Very small steps around best solution\n        step_size = 0.01\n        x_new = best_x + np.random.uniform(-step_size, step_size)\n        y_new = best_y + np.random.uniform(-step_size, step_size)\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n        \n        value = evaluate_function(x_new, y_new)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x_new, y_new\n    \n    return best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\n## Reasoning:\n\n1. **Multi-start initialization**: Provides better coverage of the search space and reduces the chance of starting in a poor region.\n\n2. **Adaptive exploration-exploitation**: The algorithm now adjusts its behavior based on progress - more exploration early, more exploitation later.\n\n3. **Random restarts**: When stuck in local minima (detected by no improvement count), the algorithm performs random restarts to escape.\n\n4. **Temperature-adaptive step sizes**: Step sizes are adjusted based on both temperature and search progress for better exploration.\n\n5. **Intensive local refinement**: A final focused search around the best solution helps fine-tune the result.\n\nThese changes combine the strengths of the top performers while maintaining the simplicity and reliability of the simulated annealing approach. The key insight is that pure SA needs mechanisms to escape deep local minima and better initialization strategies."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7005, y=0.6788 with value=-1.5187\", \"distance_to_global\": \"0.0036\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7041, y=0.6791\", \"average_distance_to_global\": \"0.7996\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}