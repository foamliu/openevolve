{"id": "16c50224-14c7-4e57-87b8-a6e86139a8a5", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local search for global optimization.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with multiple starting points\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Find best initial candidate\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 5.0\n    cooling_rate = 0.98\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_search_freq = 50\n    local_search_steps = 20\n    step_size = 0.1\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration phase\n            step_scale = (bounds[1] - bounds[0]) * 0.1\n        else:\n            # Local exploitation phase\n            step_scale = (bounds[1] - bounds[0]) * 0.01\n        \n        dx = np.random.normal(0, step_scale)\n        dy = np.random.normal(0, step_scale)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            delta = new_value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature\n        temp = max(temp * cooling_rate, min_temp)\n        \n        # Periodic local search refinement\n        if i % local_search_freq == 0 and i > 0:\n            local_best_x, local_best_y = best_x, best_y\n            local_best_value = best_value\n            \n            for _ in range(local_search_steps):\n                # Small random steps in neighborhood\n                lx = np.clip(local_best_x + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                ly = np.clip(local_best_y + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                \n                if lvalue < local_best_value:\n                    local_best_x, local_best_y = lx, ly\n                    local_best_value = lvalue\n            \n            # Update global best if local search found better solution\n            if local_best_value < best_value:\n                best_x, best_y, best_value = local_best_x, local_best_y, local_best_value\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient calculation\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update position with gradient descent\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Only update if improvement is found\n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            # Reduce learning rate if no improvement\n            learning_rate *= 0.5\n            if learning_rate < 1e-6:\n                break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "f85b78c7-ca61-490e-a5ca-9e10ec43b63c", "generation": 2, "timestamp": 1765003391.741508, "iteration_found": 11, "metrics": {"runs_successfully": 1.0, "value_score": 0.905645237320538, "distance_score": 0.199854848779366, "combined_score": 0.4986632655232554}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace # EVOLVE-BLOCK-END with 31 lines\nChange 2: Replace 3 lines with 5 lines\nChange 3: Replace 4 lines with 4 lines\nChange 4: Replace 8 lines with 9 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9063194016353092, "distance_score": 0.20160621337523205, "combined_score": 0.49954730106502265}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 0.4995\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.9554 \u2192 0.4995. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=1.5789, y=2.8928 with value=-1.4156\n```\n\n### distance_to_global\n```\n3.9602\n```\n\n### solution_quality\n```\nCould be improved\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 65 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9295, distance_score: 0.4380, combined_score: 0.9554, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 55 lines\nChange 2: Replace return best_x, best_y, best_value with 27 lines\nChange 3: Replace 3 lines with 5 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9781, distance_score: 0.6879, combined_score: 1.3431, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.3431)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = 0.95\n    \n    best_global = min(candidates, key=lambda x: x[2])\n    best_x, best_y, best_value = best_global\n    \n    for start_idx in range(num_starts):\n        x, y, current_value = candidates[start_idx]\n        temp = initial_temp\n        \n        for step in range(iterations // num_starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, 0.5 + temp / initial_temp)\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance probability\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            \n            # Cool down\n            temp = max(final_temp, temp * cooling_rate)\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update with momentum\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9781), Performs well on distance_score (0.6879), Performs well on combined_score (1.3431), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n### Program 3 (Score: 0.9554)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9295), Performs well on distance_score (0.4380), Performs well on combined_score (0.9554), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local search for global optimization.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with multiple starting points\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Find best initial candidate\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    cooling_rate = 0.995\n    min_temp = 0.001\n    \n    # Local search parameters\n    local_search_freq = 50\n    local_search_steps = 20\n    step_size = 0.1\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration phase\n            step_scale = (bounds[1] - bounds[0]) * 0.1\n        else:\n            # Local exploitation phase\n            step_scale = (bounds[1] - bounds[0]) * 0.01\n        \n        dx = np.random.normal(0, step_scale)\n        dy = np.random.normal(0, step_scale)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp((best_value - new_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature\n        temp = max(temp * cooling_rate, min_temp)\n        \n        # Periodic local search refinement\n        if i % local_search_freq == 0 and i > 0:\n            local_best_x, local_best_y = best_x, best_y\n            local_best_value = best_value\n            \n            for _ in range(local_search_steps):\n                # Small random steps in neighborhood\n                lx = np.clip(local_best_x + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                ly = np.clip(local_best_y + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                \n                if lvalue < local_best_value:\n                    local_best_x, local_best_y = lx, ly\n                    local_best_value = lvalue\n            \n            # Update global best if local search found better solution\n            if local_best_value < best_value:\n                best_x, best_y, best_value = local_best_x, local_best_y, local_best_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 0.3310)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_step = 0.1\n    step_decay = 0.99\n    min_step = 0.001\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.5:\n            # Global exploration\n            if i % 5 == 0:\n                # Multi-start injection\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                x = best_x + np.random.uniform(-1, 1)\n                y = best_y + np.random.uniform(-1, 1)\n        else:\n            # Local refinement\n            x = best_x + np.random.uniform(-local_step, local_step)\n            y = best_y + np.random.uniform(-local_step, local_step)\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        # Accept or reject based on simulated annealing\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n        else:\n            # Accept worse solution with probability\n            delta = value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_value = value\n                best_x, best_y = x, y\n        \n        # Update parameters\n        temp = max(min_temp, temp * temp_decay)\n        local_step = max(min_step, local_step * step_decay)\n    \n    # Final local search refinement\n    for _ in range(100):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.3431, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = 0.95\n    \n    best_global = min(candidates, key=lambda x: x[2])\n    best_x, best_y, best_value = best_global\n    \n    for start_idx in range(num_starts):\n        x, y, current_value = candidates[start_idx]\n        temp = initial_temp\n        \n        for step in range(iterations // num_starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, 0.5 + temp / initial_temp)\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance probability\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            \n            # Cool down\n            temp = max(final_temp, temp * cooling_rate)\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update with momentum\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.978), Excellent combined_score (1.343)\n\n### Inspiration 2 (Score: 0.3310, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_step = 0.1\n    step_decay = 0.99\n    min_step = 0.001\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.5:\n            # Global exploration\n            if i % 5 == 0:\n                # Multi-start injection\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                x = best_x + np.random.uniform(-1, 1)\n                y = best_y + np.random.uniform(-1, 1)\n        else:\n            # Local refinement\n            x = best_x + np.random.uniform(-local_step, local_step)\n            y = best_y + np.random.uniform(-local_step, local_step)\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        # Accept or reject based on simulated annealing\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n        else:\n            # Accept worse solution with probability\n            delta = value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_value = value\n                best_x, best_y = x, y\n        \n        # Update parameters\n        temp = max(min_temp, temp * temp_decay)\n        local_step = max(min_step, local_step * step_decay)\n    \n    # Final local search refinement\n    for _ in range(100):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 84 lines, Excellent runs_successfully (1.000), Alternative distance_score approach\n\n### Inspiration 3 (Score: 1.2148, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.942), Excellent combined_score (1.215)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local search for global optimization.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with multiple starting points\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Find best initial candidate\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    cooling_rate = 0.995\n    min_temp = 0.001\n    \n    # Local search parameters\n    local_search_freq = 50\n    local_search_steps = 20\n    step_size = 0.1\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration phase\n            step_scale = (bounds[1] - bounds[0]) * 0.1\n        else:\n            # Local exploitation phase\n            step_scale = (bounds[1] - bounds[0]) * 0.01\n        \n        dx = np.random.normal(0, step_scale)\n        dy = np.random.normal(0, step_scale)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp((best_value - new_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature\n        temp = max(temp * cooling_rate, min_temp)\n        \n        # Periodic local search refinement\n        if i % local_search_freq == 0 and i > 0:\n            local_best_x, local_best_y = best_x, best_y\n            local_best_value = best_value\n            \n            for _ in range(local_search_steps):\n                # Small random steps in neighborhood\n                lx = np.clip(local_best_x + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                ly = np.clip(local_best_y + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                \n                if lvalue < local_best_value:\n                    local_best_x, local_best_y = lx, ly\n                    local_best_value = lvalue\n            \n            # Update global best if local search found better solution\n            if local_best_value < best_value:\n                best_x, best_y, best_value = local_best_x, local_best_y, local_best_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance history, I can see that the fitness score has declined from 0.9554 to 0.4995. The main issue appears to be that while the algorithm has good exploration capabilities, it's not effectively refining solutions to reach the global minimum.\n\nBased on the analysis of the top-performing programs, I notice that the most successful approach (Score: 1.3431) combines simulated annealing with gradient descent refinement. The current program lacks this crucial gradient-based refinement step that helps escape shallow local minima and fine-tune the solution.\n\nHere are my targeted improvements:\n\n## Change 1: Add gradient descent refinement function\n\n```python\n<<<<<<< SEARCH\n# EVOLVE-BLOCK-END\n=======\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient calculation\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update position with gradient descent\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Only update if improvement is found\n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            # Reduce learning rate if no improvement\n            learning_rate *= 0.5\n            if learning_rate < 1e-6:\n                break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n>>>>>>> REPLACE\n```\n\n## Change 2: Modify run_search to include gradient refinement\n\n```python\n<<<<<<< SEARCH\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n=======\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n>>>>>>> REPLACE\n```\n\n## Change 3: Improve simulated annealing parameters and acceptance logic\n\n```python\n<<<<<<< SEARCH\n    # Simulated annealing parameters\n    temp = 1.0\n    cooling_rate = 0.995\n    min_temp = 0.001\n=======\n    # Simulated annealing parameters\n    temp = 5.0\n    cooling_rate = 0.98\n    min_temp = 0.01\n>>>>>>> REPLACE\n```\n\n## Change 4: Fix the acceptance probability calculation (current version has inverted logic)\n\n```python\n<<<<<<< SEARCH\n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp((best_value - new_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n=======\n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            delta = new_value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n>>>>>>> REPLACE\n```\n\n## Reasoning:\n\n1. **Gradient Descent Refinement**: The most successful program (Score: 1.3431) uses gradient descent refinement after the main search. This helps fine-tune the solution and escape shallow local minima by following the gradient toward better solutions.\n\n2. **Fixed Acceptance Probability**: The current code has an inverted acceptance probability calculation that could cause issues with the simulated annealing logic. The correct formula should be `exp(-delta/temp)` where delta is `new_value - best_value`.\n\n3. **Better Annealing Parameters**: Starting with a higher temperature (5.0 instead of 1.0) and using a slightly faster cooling rate (0.98 instead of 0.995) allows for better exploration early on while still converging to good solutions.\n\n4. **Adaptive Learning Rate**: The gradient descent refinement includes an adaptive learning rate that decreases when no improvement is found, preventing oscillation around the minimum.\n\nThese changes maintain the core structure of the current algorithm while adding the crucial refinement step that has proven successful in the top-performing programs. The combination of global exploration (simulated annealing) and local refinement (gradient descent) should significantly improve the ability to find the global minimum."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=1.5377, y=3.0275 with value=-1.4148\", \"distance_to_global\": \"4.0036\", \"solution_quality\": \"Could be improved\"}", "artifact_dir": null, "embedding": null}