{"id": "27401c42-8010-40af-bc43-f99ae37f18c1", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "83068c20-5aad-4498-8a80-c247e4815524", "generation": 2, "timestamp": 1764998889.9044702, "iteration_found": 9, "metrics": {"runs_successfully": 1.0, "value_score": 0.9992729865876615, "distance_score": 0.980994872445515, "combined_score": 1.490902432541228, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 66 lines with 98 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.7379266093380525, "distance_score": 0.200461439175422, "combined_score": 0.4183524330184003}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 0.4184\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4184\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=0.7160, y=-2.4924 with value=-1.1639\n```\n\n### distance_to_global\n```\n3.9885\n```\n\n### solution_quality\n```\nCould be improved\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 66 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.7379, distance_score: 0.2005, combined_score: 0.4184\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 87 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9345, distance_score: 0.6112, combined_score: 1.0207, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n### Program 2 (Score: 1.0207)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9345), Performs well on distance_score (0.6112), Performs well on combined_score (1.0207), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 0.4184)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(0.5, 2.0 * temp)\n            new_x = x + np.random.normal(0, step_size)\n            new_y = y + np.random.normal(0, step_size)\n            \n            # Keep within bounds with reflection\n            new_x = max(bounds[0], min(bounds[1], new_x))\n            new_y = max(bounds[0], min(bounds[1], new_y))\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on temperature\n            if new_value < current_value or np.random.random() < np.exp((current_value - new_value) / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n            \n            temp *= cooling_rate\n            \n            # Local search refinement every 50 iterations\n            if i % 50 == 0 and i > 0:\n                for _ in range(10):\n                    lx = best_x + np.random.normal(0, 0.1)\n                    ly = best_y + np.random.normal(0, 0.1)\n                    lx = max(bounds[0], min(bounds[1], lx))\n                    ly = max(bounds[0], min(bounds[1], ly))\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.7379), Performs well on distance_score (0.2005), Performs well on combined_score (0.4184)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3529)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with adaptive cooling and local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 different initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Track best for this start\n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        step_size = 0.5\n        \n        for i in range(iterations // starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * temp / initial_temp\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Keep within bounds with reflection\n            candidate_x = bounds[0] + abs(candidate_x - bounds[0]) if candidate_x < bounds[0] else candidate_x\n            candidate_x = bounds[1] - abs(candidate_x - bounds[1]) if candidate_x > bounds[1] else candidate_x\n            candidate_y = bounds[0] + abs(candidate_y - bounds[0]) if candidate_y < bounds[0] else candidate_y\n            candidate_y = bounds[1] - abs(candidate_y - bounds[1]) if candidate_y > bounds[1] else candidate_y\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Probabilistic acceptance\n                delta = candidate_value - current_value\n                if np.random.random() < np.exp(-delta / temp):\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down\n            temp *= cooling_rate\n            \n            # Adaptive step size based on progress\n            if i % 50 == 0 and i > 0:\n                step_size *= 0.95\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.0207, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 87 lines, Excellent runs_successfully (1.000), Excellent value_score (0.934)\n\n### Inspiration 2 (Score: 1.2148, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.942), Excellent combined_score (1.215)\n\n### Inspiration 3 (Score: 0.3529, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with adaptive cooling and local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 different initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Track best for this start\n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        step_size = 0.5\n        \n        for i in range(iterations // starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * temp / initial_temp\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Keep within bounds with reflection\n            candidate_x = bounds[0] + abs(candidate_x - bounds[0]) if candidate_x < bounds[0] else candidate_x\n            candidate_x = bounds[1] - abs(candidate_x - bounds[1]) if candidate_x > bounds[1] else candidate_x\n            candidate_y = bounds[0] + abs(candidate_y - bounds[0]) if candidate_y < bounds[0] else candidate_y\n            candidate_y = bounds[1] - abs(candidate_y - bounds[1]) if candidate_y > bounds[1] else candidate_y\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Probabilistic acceptance\n                delta = candidate_value - current_value\n                if np.random.random() < np.exp(-delta / temp):\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down\n            temp *= cooling_rate\n            \n            # Adaptive step size based on progress\n            if i % 50 == 0 and i > 0:\n                step_size *= 0.95\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 70 lines, Excellent runs_successfully (1.000), Alternative distance_score approach\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(0.5, 2.0 * temp)\n            new_x = x + np.random.normal(0, step_size)\n            new_y = y + np.random.normal(0, step_size)\n            \n            # Keep within bounds with reflection\n            new_x = max(bounds[0], min(bounds[1], new_x))\n            new_y = max(bounds[0], min(bounds[1], new_y))\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on temperature\n            if new_value < current_value or np.random.random() < np.exp((current_value - new_value) / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n            \n            temp *= cooling_rate\n            \n            # Local search refinement every 50 iterations\n            if i % 50 == 0 and i > 0:\n                for _ in range(10):\n                    lx = best_x + np.random.normal(0, 0.1)\n                    ly = best_y + np.random.normal(0, 0.1)\n                    lx = max(bounds[0], min(bounds[1], lx))\n                    ly = max(bounds[0], min(bounds[1], ly))\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance history, I can see that the algorithm is getting stuck in local minima (fitness score of 0.4184, distance to global minimum of 3.9885). The issue is that while the current approach uses simulated annealing with local search, it's not aggressive enough in escaping local minima.\n\nLet me analyze the key problems:\n\n1. **Insufficient exploration**: The current algorithm only does 3 multi-starts, which may not cover the search space adequately\n2. **Weak restart mechanism**: There's no systematic way to escape when stuck in deep local minima\n3. **Limited local search**: The local search happens too infrequently (every 50 iterations) and with small radius\n4. **Temperature cooling too slow**: The cooling rate of 0.995 may cause the algorithm to get trapped\n\nHere's my targeted improvement:\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(0.5, 2.0 * temp)\n            new_x = x + np.random.normal(0, step_size)\n            new_y = y + np.random.normal(0, step_size)\n            \n            # Keep within bounds with reflection\n            new_x = max(bounds[0], min(bounds[1], new_x))\n            new_y = max(bounds[0], min(bounds[1], new_y))\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on temperature\n            if new_value < current_value or np.random.random() < np.exp((current_value - new_value) / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n            \n            temp *= cooling_rate\n            \n            # Local search refinement every 50 iterations\n            if i % 50 == 0 and i > 0:\n                for _ in range(10):\n                    lx = best_x + np.random.normal(0, 0.1)\n                    ly = best_y + np.random.normal(0, 0.1)\n                    lx = max(bounds[0], min(bounds[1], lx))\n                    ly = max(bounds[0], min(bounds[1], ly))\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n>>>>>>> REPLACE\n```\n\n**Key improvements made:**\n\n1. **Increased exploration**: More multi-starts (7 instead of 3) for better coverage of the search space\n2. **Higher initial temperature**: Started at 5.0 instead of 1.0 to allow more aggressive exploration early on\n3. **Heavy-tailed perturbations**: Used Cauchy distribution instead of Gaussian for better escape from local minima\n4. **More frequent local search**: Every 20 iterations instead of 50, with more samples (15 instead of 10)\n5. **Aggressive restart mechanism**: Detects stagnation and jumps to new locations with 70% probability\n6. **Adaptive parameters**: Step size and local search radius decrease over time for fine-tuning\n7. **Better stagnation detection**: Tracks both iteration count and actual improvement\n\nThese changes should help the algorithm escape local minima more effectively while maintaining the ability to converge to good solutions. The combination of more exploration, better escape mechanisms, and adaptive parameters should significantly improve the fitness score."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.6830, y=0.6844 with value=-1.5184\", \"distance_to_global\": \"0.0220\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.6884, y=0.6747\", \"average_distance_to_global\": \"0.0194\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}