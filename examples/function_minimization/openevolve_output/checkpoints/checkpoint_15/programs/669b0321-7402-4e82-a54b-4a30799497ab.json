{"id": "669b0321-7402-4e82-a54b-4a30799497ab", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 3\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start from the best initial point\n    best_global = min(candidates, key=lambda x: x[2])\n    x, y, best_value = best_global\n    best_x, best_y = x, y\n\n    # SA parameters\n    T_init = 10.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for i in range(iterations):\n        # Adaptive step size based on temperature and progress\n        progress = i / iterations\n        sigma = max(0.1, 3.0 * (T / T_init) * (1 - progress) + 0.1)\n        \n        # Generate candidate with occasional large jumps for exploration\n        if np.random.random() < 0.1 and i < iterations // 2:  # 10% chance of large jump in first half\n            x_new = x + np.random.uniform(-1.0, 1.0)\n            y_new = y + np.random.uniform(-1.0, 1.0)\n        else:\n            x_new = x + np.random.normal(0, sigma)\n            y_new = y + np.random.normal(0, sigma)\n        \n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    # Local search refinement\n    refinement_steps = 100\n    refinement_radius = 0.1\n    \n    for _ in range(refinement_steps):\n        # Generate small random perturbations\n        dx = np.random.uniform(-refinement_radius, refinement_radius)\n        dy = np.random.uniform(-refinement_radius, refinement_radius)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "9157d41a-ff1a-4abf-9201-7a4b2d418118", "generation": 2, "timestamp": 1765003339.3391721, "iteration_found": 12, "metrics": {"runs_successfully": 1.0, "value_score": 0.9102568825652596, "distance_score": 0.2003498355073892, "combined_score": 0.5010352509936892}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 16 lines with 25 lines\nChange 2: Replace 25 lines with 33 lines\nChange 3: Replace return best_x, best_y, best_value with 17 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.8494633242189635, "distance_score": 0.7265767136253556, "combined_score": 1.1204321519741285}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.1204\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 1.1204\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.9291, y=0.3764 with value=-1.3418\n```\n\n### distance_to_global\n```\n0.3763\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 44 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.8495, distance_score: 0.7266, combined_score: 1.1204\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 55 lines\nChange 2: Replace return best_x, best_y, best_value with 27 lines\nChange 3: Replace 3 lines with 5 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9781, distance_score: 0.6879, combined_score: 1.3431, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.3431)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = 0.95\n    \n    best_global = min(candidates, key=lambda x: x[2])\n    best_x, best_y, best_value = best_global\n    \n    for start_idx in range(num_starts):\n        x, y, current_value = candidates[start_idx]\n        temp = initial_temp\n        \n        for step in range(iterations // num_starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, 0.5 + temp / initial_temp)\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance probability\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            \n            # Cool down\n            temp = max(final_temp, temp * cooling_rate)\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update with momentum\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9781), Performs well on distance_score (0.6879), Performs well on combined_score (1.3431), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n### Program 3 (Score: 1.1204)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.8495), Performs well on distance_score (0.7266), Performs well on combined_score (1.1204)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local search for global optimization.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with multiple starting points\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Find best initial candidate\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    cooling_rate = 0.995\n    min_temp = 0.001\n    \n    # Local search parameters\n    local_search_freq = 50\n    local_search_steps = 20\n    step_size = 0.1\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration phase\n            step_scale = (bounds[1] - bounds[0]) * 0.1\n        else:\n            # Local exploitation phase\n            step_scale = (bounds[1] - bounds[0]) * 0.01\n        \n        dx = np.random.normal(0, step_scale)\n        dy = np.random.normal(0, step_scale)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp((best_value - new_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature\n        temp = max(temp * cooling_rate, min_temp)\n        \n        # Periodic local search refinement\n        if i % local_search_freq == 0 and i > 0:\n            local_best_x, local_best_y = best_x, best_y\n            local_best_value = best_value\n            \n            for _ in range(local_search_steps):\n                # Small random steps in neighborhood\n                lx = np.clip(local_best_x + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                ly = np.clip(local_best_y + np.random.uniform(-step_size, step_size), \n                           bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                \n                if lvalue < local_best_value:\n                    local_best_x, local_best_y = lx, ly\n                    local_best_value = lvalue\n            \n            # Update global best if local search found better solution\n            if local_best_value < best_value:\n                best_x, best_y, best_value = local_best_x, local_best_y, local_best_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 0.9554)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.3431, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = 0.95\n    \n    best_global = min(candidates, key=lambda x: x[2])\n    best_x, best_y, best_value = best_global\n    \n    for start_idx in range(num_starts):\n        x, y, current_value = candidates[start_idx]\n        temp = initial_temp\n        \n        for step in range(iterations // num_starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, 0.5 + temp / initial_temp)\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance probability\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            \n            # Cool down\n            temp = max(final_temp, temp * cooling_rate)\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update with momentum\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.978), Excellent combined_score (1.343)\n\n### Inspiration 2 (Score: 0.9554, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 65 lines, Excellent runs_successfully (1.000), Excellent value_score (0.930)\n\n### Inspiration 3 (Score: 1.2148, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.942), Excellent combined_score (1.215)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and its performance history, I can see it's a simulated annealing approach that achieves a fitness score of 1.1204. The top performer (Program 1) achieved 1.3431 by using multi-start initialization and gradient descent refinement. I'll improve the current program by incorporating key elements from the best performers while maintaining the simplicity of the current approach.\n\nHere are my targeted improvements:\n\n## 1. Add Multi-Start Initialization\n\nThe current program starts from a single random point, which can lead to getting trapped in local minima. Adding multiple starting points will provide better coverage of the search space.\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 3\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start from the best initial point\n    best_global = min(candidates, key=lambda x: x[2])\n    x, y, best_value = best_global\n    best_x, best_y = x, y\n>>>>>>> REPLACE\n```\n\n## 2. Improve Temperature Schedule and Step Size Adaptation\n\nThe current temperature schedule is fixed. I'll add adaptive step sizing that changes based on the current temperature and iteration progress, allowing for better exploration early on and fine-tuning later.\n\n```python\n<<<<<<< SEARCH\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n=======\n    # SA parameters\n    T_init = 10.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for i in range(iterations):\n        # Adaptive step size based on temperature and progress\n        progress = i / iterations\n        sigma = max(0.1, 3.0 * (T / T_init) * (1 - progress) + 0.1)\n        \n        # Generate candidate with occasional large jumps for exploration\n        if np.random.random() < 0.1 and i < iterations // 2:  # 10% chance of large jump in first half\n            x_new = x + np.random.uniform(-1.0, 1.0)\n            y_new = y + np.random.uniform(-1.0, 1.0)\n        else:\n            x_new = x + np.random.normal(0, sigma)\n            y_new = y + np.random.normal(0, sigma)\n        \n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n>>>>>>> REPLACE\n```\n\n## 3. Add Local Search Refinement\n\nAfter the simulated annealing phase, add a local search refinement to fine-tune the solution. This is inspired by the gradient descent approach in Program 1 but uses a simpler pattern search method.\n\n```python\n<<<<<<< SEARCH\n    return best_x, best_y, best_value\n=======\n    # Local search refinement\n    refinement_steps = 100\n    refinement_radius = 0.1\n    \n    for _ in range(refinement_steps):\n        # Generate small random perturbations\n        dx = np.random.uniform(-refinement_radius, refinement_radius)\n        dy = np.random.uniform(-refinement_radius, refinement_radius)\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n    \n    return best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\n## Summary of Improvements\n\nThese changes address the key weaknesses in the current algorithm:\n\n1. **Multi-start initialization**: Instead of starting from one random point, we start from 3 different points and pick the best one. This increases the chance of finding a good region of the search space.\n\n2. **Adaptive exploration**: The step size now adapts based on both temperature and iteration progress, with occasional large jumps for better exploration in the early stages.\n\n3. **Higher initial temperature**: Increased from 5.0 to 10.0 to allow more exploration in the beginning.\n\n4. **Local refinement**: Added a simple local search phase after simulated annealing to fine-tune the solution.\n\nThese improvements maintain the simplicity of the original approach while incorporating successful elements from the top performers, particularly the multi-start strategy and local refinement concepts. The changes should help escape local minima more effectively and improve the overall fitness score."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=1.5907, y=2.9309 with value=-1.4204\", \"distance_to_global\": \"3.9913\", \"solution_quality\": \"Could be improved\"}", "artifact_dir": null, "embedding": null}