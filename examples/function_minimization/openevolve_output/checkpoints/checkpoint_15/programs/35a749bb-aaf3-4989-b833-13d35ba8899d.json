{"id": "35a749bb-aaf3-4989-b833-13d35ba8899d", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Use a cross pattern search around current best\n                patterns = [(0.1, 0), (-0.1, 0), (0, 0.1), (0, -0.1), \n                           (0.07, 0.07), (-0.07, -0.07), (0.07, -0.07), (-0.07, 0.07)]\n                \n                for dx, dy in patterns:\n                    local_x = best_x + dx * local_search_radius\n                    local_y = best_y + dy * local_search_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Adaptive restart with temperature boost\n            if i > 100 and i % 50 == 0:\n                # Check if we're making progress\n                if abs(best_value - evaluate_function(x, y)) < 1e-6:\n                    # Instead of random restart, boost temperature and move to best found\n                    temp = min(1.0, temp * 2)  # Boost temperature\n                    # Move to best position with some exploration\n                    x = best_x + np.random.normal(0, 0.1)\n                    y = best_y + np.random.normal(0, 0.1)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n        \n        # Final intensification around best point found\n        intensification_radius = 0.05\n        for _ in range(20):\n            int_x = best_x + np.random.normal(0, intensification_radius)\n            int_y = best_y + np.random.normal(0, intensification_radius)\n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "167a46d8-0621-4c96-8435-42f7099bfd45", "generation": 2, "timestamp": 1764999045.104301, "iteration_found": 10, "metrics": {"runs_successfully": 1.0, "value_score": 0.9898326518573491, "distance_score": 0.7109070235904382, "combined_score": 1.362282649508709, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 12 lines with 16 lines\nChange 2: Replace 9 lines with 12 lines\nChange 3: Replace 6 lines with 12 lines\nChange 4: Replace 3 lines with 15 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9344676348558394, "distance_score": 0.6111942120585718, "combined_score": 1.0207104972545895, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.0207\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4334 \u2192 1.0207\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.6290, y=0.6705 with value=-1.5147\n```\n\n### distance_to_global\n```\n0.0754\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=1.5639, y=2.9896\n```\n\n### average_distance_to_global\n```\n0.6361\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 77 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.7684, distance_score: 0.2017, combined_score: 0.4334\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 87 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9345, distance_score: 0.6112, combined_score: 1.0207, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n### Program 2 (Score: 1.0207)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9345), Performs well on distance_score (0.6112), Performs well on combined_score (1.0207), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 0.4334)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining simulated annealing,\n    local search, and multiple restart strategies.\n    \"\"\"\n    def local_search(x, y, step_size=0.1, local_iter=50):\n        \"\"\"Local gradient-free search around current point\"\"\"\n        best_val = evaluate_function(x, y)\n        best_x, best_y = x, y\n        \n        for _ in range(local_iter):\n            # Try small random steps\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n            new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            if new_val < best_val:\n                best_val = new_val\n                best_x, best_y = new_x, new_y\n                \n        return best_x, best_y, best_val\n    \n    # Multi-start strategy with different initial points\n    num_restarts = 5\n    restart_points = [(np.random.uniform(bounds[0], bounds[1]), \n                      np.random.uniform(bounds[0], bounds[1])) for _ in range(num_restarts)]\n    \n    global_best = float('inf')\n    global_best_x, global_best_y = 0, 0\n    \n    for start_x, start_y in restart_points:\n        x, y = start_x, start_y\n        current_value = evaluate_function(x, y)\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = 0.95\n        \n        temp = initial_temp\n        steps_per_temp = iterations // (num_restarts * 20)\n        \n        for step in range(iterations // num_restarts):\n            # Generate candidate point\n            step_size = temp / initial_temp * (bounds[1] - bounds[0]) / 10\n            candidate_x = np.clip(x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            candidate_y = np.clip(y + np.random.normal(0, step_size), bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y = candidate_x, candidate_y\n                current_value = candidate_value\n            else:\n                # Accept worse solution with probability\n                probability = np.exp(-(candidate_value - current_value) / temp)\n                if np.random.random() < probability:\n                    x, y = candidate_x, candidate_y\n                    current_value = candidate_value\n            \n            # Cool down temperature\n            if step % steps_per_temp == 0:\n                temp = max(final_temp, temp * cooling_rate)\n        \n        # Local refinement around best point found\n        x, y, local_value = local_search(x, y)\n        \n        # Update global best\n        if local_value < global_best:\n            global_best = local_value\n            global_best_x, global_best_y = x, y\n    \n    return global_best_x, global_best_y, global_best\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.7684), Performs well on distance_score (0.2017), Performs well on combined_score (0.4334)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3529)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with adaptive cooling and local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 different initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Track best for this start\n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        step_size = 0.5\n        \n        for i in range(iterations // starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * temp / initial_temp\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Keep within bounds with reflection\n            candidate_x = bounds[0] + abs(candidate_x - bounds[0]) if candidate_x < bounds[0] else candidate_x\n            candidate_x = bounds[1] - abs(candidate_x - bounds[1]) if candidate_x > bounds[1] else candidate_x\n            candidate_y = bounds[0] + abs(candidate_y - bounds[0]) if candidate_y < bounds[0] else candidate_y\n            candidate_y = bounds[1] - abs(candidate_y - bounds[1]) if candidate_y > bounds[1] else candidate_y\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Probabilistic acceptance\n                delta = candidate_value - current_value\n                if np.random.random() < np.exp(-delta / temp):\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down\n            temp *= cooling_rate\n            \n            # Adaptive step size based on progress\n            if i % 50 == 0 and i > 0:\n                step_size *= 0.95\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 0.4184)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(0.5, 2.0 * temp)\n            new_x = x + np.random.normal(0, step_size)\n            new_y = y + np.random.normal(0, step_size)\n            \n            # Keep within bounds with reflection\n            new_x = max(bounds[0], min(bounds[1], new_x))\n            new_y = max(bounds[0], min(bounds[1], new_y))\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on temperature\n            if new_value < current_value or np.random.random() < np.exp((current_value - new_value) / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n            \n            temp *= cooling_rate\n            \n            # Local search refinement every 50 iterations\n            if i % 50 == 0 and i > 0:\n                for _ in range(10):\n                    lx = best_x + np.random.normal(0, 0.1)\n                    ly = best_y + np.random.normal(0, 0.1)\n                    lx = max(bounds[0], min(bounds[1], lx))\n                    ly = max(bounds[0], min(bounds[1], ly))\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.2148, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.942), Excellent combined_score (1.215)\n\n### Inspiration 2 (Score: 0.4334, Type: Experimental)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining simulated annealing,\n    local search, and multiple restart strategies.\n    \"\"\"\n    def local_search(x, y, step_size=0.1, local_iter=50):\n        \"\"\"Local gradient-free search around current point\"\"\"\n        best_val = evaluate_function(x, y)\n        best_x, best_y = x, y\n        \n        for _ in range(local_iter):\n            # Try small random steps\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n            new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            if new_val < best_val:\n                best_val = new_val\n                best_x, best_y = new_x, new_y\n                \n        return best_x, best_y, best_val\n    \n    # Multi-start strategy with different initial points\n    num_restarts = 5\n    restart_points = [(np.random.uniform(bounds[0], bounds[1]), \n                      np.random.uniform(bounds[0], bounds[1])) for _ in range(num_restarts)]\n    \n    global_best = float('inf')\n    global_best_x, global_best_y = 0, 0\n    \n    for start_x, start_y in restart_points:\n        x, y = start_x, start_y\n        current_value = evaluate_function(x, y)\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = 0.95\n        \n        temp = initial_temp\n        steps_per_temp = iterations // (num_restarts * 20)\n        \n        for step in range(iterations // num_restarts):\n            # Generate candidate point\n            step_size = temp / initial_temp * (bounds[1] - bounds[0]) / 10\n            candidate_x = np.clip(x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            candidate_y = np.clip(y + np.random.normal(0, step_size), bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y = candidate_x, candidate_y\n                current_value = candidate_value\n            else:\n                # Accept worse solution with probability\n                probability = np.exp(-(candidate_value - current_value) / temp)\n                if np.random.random() < probability:\n                    x, y = candidate_x, candidate_y\n                    current_value = candidate_value\n            \n            # Cool down temperature\n            if step % steps_per_temp == 0:\n                temp = max(final_temp, temp * cooling_rate)\n        \n        # Local refinement around best point found\n        x, y, local_value = local_search(x, y)\n        \n        # Update global best\n        if local_value < global_best:\n            global_best = local_value\n            global_best_x, global_best_y = x, y\n    \n    return global_best_x, global_best_y, global_best\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 77 lines, Excellent runs_successfully (1.000), Alternative distance_score approach\n\n### Inspiration 3 (Score: 0.3529, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with adaptive cooling and local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 different initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Track best for this start\n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        step_size = 0.5\n        \n        for i in range(iterations // starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * temp / initial_temp\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Keep within bounds with reflection\n            candidate_x = bounds[0] + abs(candidate_x - bounds[0]) if candidate_x < bounds[0] else candidate_x\n            candidate_x = bounds[1] - abs(candidate_x - bounds[1]) if candidate_x > bounds[1] else candidate_x\n            candidate_y = bounds[0] + abs(candidate_y - bounds[0]) if candidate_y < bounds[0] else candidate_y\n            candidate_y = bounds[1] - abs(candidate_y - bounds[1]) if candidate_y > bounds[1] else candidate_y\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Probabilistic acceptance\n                delta = candidate_value - current_value\n                if np.random.random() < np.exp(-delta / temp):\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down\n            temp *= cooling_rate\n            \n            # Adaptive step size based on progress\n            if i % 50 == 0 and i > 0:\n                step_size *= 0.95\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 70 lines, Excellent runs_successfully (1.000), Alternative distance_score approach\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the execution results, I can see that while the algorithm achieves 100% success rate and finds solutions very close to the global minimum (distance < 0.5), there's room for improvement in the actual fitness score. The current approach uses simulated annealing with local search, but I notice several areas where we can enhance the exploration and exploitation balance.\n\nHere are my targeted improvements:\n\n## 1. Replace the local search mechanism with a more focused gradient-free pattern search\n\nThe current local search uses random sampling, but a systematic pattern search can be more efficient:\n\n```python\n<<<<<<< SEARCH\n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n=======\n            # Pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Use a cross pattern search around current best\n                patterns = [(0.1, 0), (-0.1, 0), (0, 0.1), (0, -0.1), \n                           (0.07, 0.07), (-0.07, -0.07), (0.07, -0.07), (-0.07, 0.07)]\n                \n                for dx, dy in patterns:\n                    local_x = best_x + dx * local_search_radius\n                    local_y = best_y + dy * local_search_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n>>>>>>> REPLACE\n```\n\n## 2. Improve the restart mechanism with adaptive temperature reset\n\nInstead of random restarts, use a more intelligent restart that considers the search history:\n\n```python\n<<<<<<< SEARCH\n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n=======\n            # Adaptive restart with temperature boost\n            if i > 100 and i % 50 == 0:\n                # Check if we're making progress\n                if abs(best_value - evaluate_function(x, y)) < 1e-6:\n                    # Instead of random restart, boost temperature and move to best found\n                    temp = min(1.0, temp * 2)  # Boost temperature\n                    # Move to best position with some exploration\n                    x = best_x + np.random.normal(0, 0.1)\n                    y = best_y + np.random.normal(0, 0.1)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n>>>>>>> REPLACE\n```\n\n## 3. Enhance the candidate generation with adaptive mutation\n\nMake the candidate generation more sophisticated by using a combination of Gaussian and uniform perturbations:\n\n```python\n<<<<<<< SEARCH\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n=======\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n>>>>>>> REPLACE\n```\n\n## 4. Add a final intensification phase\n\nAdd a final local search phase at the end of each restart to ensure we've fully explored the best region:\n\n```python\n<<<<<<< SEARCH\n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n=======\n        # Final intensification around best point found\n        intensification_radius = 0.05\n        for _ in range(20):\n            int_x = best_x + np.random.normal(0, intensification_radius)\n            int_y = best_y + np.random.normal(0, intensification_radius)\n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\nThese improvements focus on:\n\n1. **Better local search**: Pattern search is more systematic than random sampling\n2. **Smarter restarts**: Instead of random jumps, we boost temperature and move to the best found position\n3. **Enhanced exploration**: Mixing Gaussian and uniform perturbations balances local and global search\n4. **Final intensification**: Ensures thorough exploration around the best solution found\n\nThe changes maintain the overall structure while improving the search efficiency and reliability, which should help achieve better fitness scores by finding solutions even closer to the global optimum."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7057, y=0.6780 with value=-1.5187\", \"distance_to_global\": \"0.0017\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7075, y=0.6800\", \"average_distance_to_global\": \"0.4067\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}