{"id": "8f69abab-7c19-442b-ad47-4bfeb945d8ad", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with adaptive cooling and local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 different initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Track best for this start\n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        step_size = 0.5\n        \n        for i in range(iterations // starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * temp / initial_temp\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Keep within bounds with reflection\n            candidate_x = bounds[0] + abs(candidate_x - bounds[0]) if candidate_x < bounds[0] else candidate_x\n            candidate_x = bounds[1] - abs(candidate_x - bounds[1]) if candidate_x > bounds[1] else candidate_x\n            candidate_y = bounds[0] + abs(candidate_y - bounds[0]) if candidate_y < bounds[0] else candidate_y\n            candidate_y = bounds[1] - abs(candidate_y - bounds[1]) if candidate_y > bounds[1] else candidate_y\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Probabilistic acceptance\n                delta = candidate_value - current_value\n                if np.random.random() < np.exp(-delta / temp):\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down\n            temp *= cooling_rate\n            \n            # Adaptive step size based on progress\n            if i % 50 == 0 and i > 0:\n                step_size *= 0.95\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "8511c1a0-9902-4124-b11c-a07a13f246ea", "generation": 1, "timestamp": 1764998705.022927, "iteration_found": 1, "metrics": {"runs_successfully": 1.0, "value_score": 0.5993046176840473, "distance_score": 0.20372927513874722, "combined_score": 0.3528595845327418}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 70 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9417587299889709, "distance_score": 0.7565915427363771, "combined_score": 1.2147685971231066}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.2148\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 1.2148\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["I'll improve the search algorithm to better escape local minima by implementing a simulated annealing approach with adaptive step sizes and multiple restart strategies.\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with adaptive cooling and local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with 3 different initial points\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        # Track best for this start\n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n        \n        temp = initial_temp\n        step_size = 0.5\n        \n        for i in range(iterations // starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * temp / initial_temp\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Keep within bounds with reflection\n            candidate_x = bounds[0] + abs(candidate_x - bounds[0]) if candidate_x < bounds[0] else candidate_x\n            candidate_x = bounds[1] - abs(candidate_x - bounds[1]) if candidate_x > bounds[1] else candidate_x\n            candidate_y = bounds[0] + abs(candidate_y - bounds[0]) if candidate_y < bounds[0] else candidate_y\n            candidate_y = bounds[1] - abs(candidate_y - bounds[1]) if candidate_y > bounds[1] else candidate_y\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Probabilistic acceptance\n                delta = candidate_value - current_value\n                if np.random.random() < np.exp(-delta / temp):\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down\n            temp *= cooling_rate\n            \n            # Adaptive step size based on progress\n            if i % 50 == 0 and i > 0:\n                step_size *= 0.95\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n>>>>>>> REPLACE\n```\n\nThis improved algorithm incorporates several key enhancements:\n\n1. **Simulated Annealing**: Uses temperature-based probabilistic acceptance to escape local minima early in the search, then gradually focuses on exploitation as temperature decreases.\n\n2. **Multi-start Strategy**: Runs the optimization from 3 different starting points to explore different regions of the search space, reducing the chance of missing the global minimum.\n\n3. **Adaptive Step Size**: The step size decreases both with temperature and periodically every 50 iterations, allowing coarse exploration early and fine-tuning later.\n\n4. **Boundary Handling**: Uses reflection to keep candidates within bounds rather than simple clipping, which can help explore boundary regions better.\n\n5. **Radial Candidate Generation**: Uses polar coordinates for candidate generation, providing more natural exploration in 2D space compared to independent axis updates.\n\nThe algorithm maintains the same interface but significantly improves the ability to escape local minima and find better solutions for the complex function with multiple local optima."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=1.1235, y=3.3764 with value=-0.8504\", \"distance_to_global\": \"3.9085\", \"solution_quality\": \"Could be improved\"}", "artifact_dir": null, "embedding": null}