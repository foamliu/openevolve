{"id": "80f604f3-3b63-42b8-8531-ce98a3453f57", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    x, y = best_x, best_y\n\n    # SA parameters with adaptive cooling\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n    \n    # Adaptive exploration ratio\n    exploration_cutoff = int(iterations * 0.7)\n    \n    T = T_init\n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration phase\n            if i % 5 == 0:\n                # Inject random restart for better exploration\n                x_new = np.random.uniform(bounds[0], bounds[1])\n                y_new = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                sigma = 1.0 * T / T_init + 0.5\n                x_new = x + np.random.normal(0, sigma)\n                y_new = y + np.random.normal(0, sigma)\n        else:\n            # Local exploitation phase with shrinking step size\n            step_size = 0.5 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x_new = x + np.random.normal(0, step_size)\n            y_new = y + np.random.normal(0, step_size)\n        \n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    # Final intensive local search for precision\n    for _ in range(100):\n        step_size = 0.01\n        x_new = best_x + np.random.uniform(-step_size, step_size)\n        y_new = best_y + np.random.uniform(-step_size, step_size)\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n        value = evaluate_function(x_new, y_new)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x_new, y_new\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "9157d41a-ff1a-4abf-9201-7a4b2d418118", "generation": 2, "timestamp": 1765003477.566226, "iteration_found": 14, "metrics": {"runs_successfully": 1.0, "value_score": 0.9996845048023165, "distance_score": 0.9987906941250843, "combined_score": 1.499219190958025, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 16 lines with 22 lines\nChange 2: Replace 25 lines with 41 lines\nChange 3: Replace return best_x, best_y, best_value with 13 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.8494633242189635, "distance_score": 0.7265767136253556, "combined_score": 1.1204321519741285}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.1204\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.3431 \u2192 1.1204. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.9291, y=0.3764 with value=-1.3418\n```\n\n### distance_to_global\n```\n0.3763\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 55 lines\nChange 2: Replace return best_x, best_y, best_value with 27 lines\nChange 3: Replace 3 lines with 5 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9781, distance_score: 0.6879, combined_score: 1.3431, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 75 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9900, distance_score: 0.7141, combined_score: 1.3638, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 61 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9989, combined_score: 1.4993, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4993)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with adaptive local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive exploration-exploitation balance\n    exploration_ratio = 0.7  # 70% exploration, 30% exploitation\n    exploration_cutoff = int(iterations * exploration_ratio)\n    \n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration - maintain random search strength\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation around best solution\n            # Adaptive step size that decreases as we get closer to optimum\n            step_size = (bounds[1] - bounds[0]) * 0.1 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final intensive local search\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9989), Performs well on combined_score (1.4993), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.3638)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Improved random search with adaptive local exploration and momentum.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n    \n    # Track recent improvements for momentum\n    recent_improvements = []\n    no_improvement_count = 0\n    \n    for i in range(iterations):\n        # Adaptive radius based on progress\n        progress = i / iterations\n        base_radius = (bounds[1] - bounds[0]) * (1 - progress * 0.9)\n        \n        # Mix global and local search based on recent progress\n        if no_improvement_count > 50 or i < iterations * 0.1:\n            # Global exploration when stuck or early in search\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploration around best point\n            angle = np.random.uniform(0, 2 * np.pi)\n            radius = np.random.uniform(0, base_radius * 0.1)\n            x = best_x + radius * np.cos(angle)\n            y = best_y + radius * np.sin(angle)\n            \n            # Add momentum from recent improvements\n            if len(recent_improvements) >= 2:\n                dx = recent_improvements[-1][0] - recent_improvements[-2][0]\n                dy = recent_improvements[-1][1] - recent_improvements[-2][1]\n                momentum_strength = min(0.5, base_radius * 0.05)\n                x += dx * momentum_strength\n                y += dy * momentum_strength\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            # Store improvement for momentum calculation\n            recent_improvements.append((x, y))\n            if len(recent_improvements) > 5:\n                recent_improvements.pop(0)\n            \n            best_value = value\n            best_x, best_y = x, y\n            no_improvement_count = 0\n        else:\n            no_improvement_count += 1\n    \n    # Final local refinement\n    for _ in range(min(100, iterations // 10)):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9900), Performs well on distance_score (0.7141), Performs well on combined_score (1.3638), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.3431)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    candidates = []\n    \n    for _ in range(num_starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = 0.95\n    \n    best_global = min(candidates, key=lambda x: x[2])\n    best_x, best_y, best_value = best_global\n    \n    for start_idx in range(num_starts):\n        x, y, current_value = candidates[start_idx]\n        temp = initial_temp\n        \n        for step in range(iterations // num_starts):\n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, 0.5 + temp / initial_temp)\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance probability\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                x, y, current_value = new_x, new_y, new_value\n                \n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            \n            # Cool down\n            temp = max(final_temp, temp * cooling_rate)\n    \n    return best_x, best_y, best_value\n\n\ndef gradient_descent_refine(x, y, bounds=(-5, 5), learning_rate=0.01, steps=50):\n    \"\"\"Refine solution using gradient descent with numerical derivatives\"\"\"\n    current_x, current_y = x, y\n    current_value = evaluate_function(current_x, current_y)\n    \n    for _ in range(steps):\n        # Numerical gradient\n        eps = 1e-6\n        grad_x = (evaluate_function(current_x + eps, current_y) - \n                  evaluate_function(current_x - eps, current_y)) / (2 * eps)\n        grad_y = (evaluate_function(current_x, current_y + eps) - \n                  evaluate_function(current_x, current_y - eps)) / (2 * eps)\n        \n        # Update with momentum\n        new_x = np.clip(current_x - learning_rate * grad_x, bounds[0], bounds[1])\n        new_y = np.clip(current_y - learning_rate * grad_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < current_value:\n            current_x, current_y, current_value = new_x, new_y, new_value\n        else:\n            break\n    \n    return current_x, current_y, current_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    # Refine with gradient descent\n    x, y, value = gradient_descent_refine(x, y)\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9781), Performs well on distance_score (0.6879), Performs well on combined_score (1.3431), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.1204)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4993, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with adaptive local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Adaptive exploration-exploitation balance\n    exploration_ratio = 0.7  # 70% exploration, 30% exploitation\n    exploration_cutoff = int(iterations * exploration_ratio)\n    \n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration - maintain random search strength\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation around best solution\n            # Adaptive step size that decreases as we get closer to optimum\n            step_size = (bounds[1] - bounds[0]) * 0.1 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x = best_x + np.random.uniform(-step_size, step_size)\n            y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Final intensive local search\n    for _ in range(50):\n        step_size = 0.01\n        x = best_x + np.random.uniform(-step_size, step_size)\n        y = best_y + np.random.uniform(-step_size, step_size)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 61 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 2 (Score: 0.9554, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced optimization using simulated annealing with local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    candidates = []\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Select best starting point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search radius adaptation\n    radius = 0.5\n    radius_min = 0.01\n    \n    for i in range(iterations):\n        # Generate candidate with adaptive step size\n        if i < iterations // 3:\n            # Global exploration\n            new_x = np.random.uniform(bounds[0], bounds[1])\n            new_y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Local exploitation with decreasing radius\n            step_size = radius * (1 - i/iterations)\n            new_x = best_x + np.random.uniform(-step_size, step_size)\n            new_y = best_y + np.random.uniform(-step_size, step_size)\n            \n            # Ensure bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n        else:\n            # Accept worse solutions with decreasing probability\n            prob = np.exp(-(new_value - best_value) / temp)\n            if np.random.random() < prob:\n                best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Cool down temperature and radius\n        temp = max(temp * alpha, temp_min)\n        radius = max(radius * 0.998, radius_min)\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 65 lines, Excellent runs_successfully (1.000), Excellent value_score (0.930)\n\n### Inspiration 3 (Score: 0.3310, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_step = 0.1\n    step_decay = 0.99\n    min_step = 0.001\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.5:\n            # Global exploration\n            if i % 5 == 0:\n                # Multi-start injection\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                x = best_x + np.random.uniform(-1, 1)\n                y = best_y + np.random.uniform(-1, 1)\n        else:\n            # Local refinement\n            x = best_x + np.random.uniform(-local_step, local_step)\n            y = best_y + np.random.uniform(-local_step, local_step)\n        \n        # Ensure bounds\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        \n        value = evaluate_function(x, y)\n        \n        # Accept or reject based on simulated annealing\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n        else:\n            # Accept worse solution with probability\n            delta = value - best_value\n            prob = np.exp(-delta / temp)\n            if np.random.random() < prob:\n                best_value = value\n                best_x, best_y = x, y\n        \n        # Update parameters\n        temp = max(min_temp, temp * temp_decay)\n        local_step = max(min_step, local_step * step_decay)\n    \n    # Final local search refinement\n    for _ in range(100):\n        x = best_x + np.random.uniform(-0.01, 0.01)\n        y = best_y + np.random.uniform(-0.01, 0.01)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 84 lines, Excellent runs_successfully (1.000), Alternative distance_score approach\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and its fitness score of 1.1204, I can see it's using a basic simulated annealing approach. The recent decline from 1.3431 suggests we need to improve the search strategy. Let me analyze the top performers and suggest targeted improvements.\n\nKey observations from the best programs:\n1. Top performer (1.4993) uses multi-start initialization with 3 diverse points and adaptive exploration-exploitation balance\n2. The current program lacks multi-start capability and has a simple cooling schedule\n3. The best programs combine global exploration with intensive local refinement\n\nHere are my improvements:\n\n## Change 1: Add Multi-Start Initialization\n\nThe current single-start approach limits exploration of the search space. Adding multiple diverse starting points will help find better basins.\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialise\n    x = np.random.uniform(bounds[0], bounds[1])\n    y = np.random.uniform(bounds[0], bounds[1])\n    best_x, best_y = x, y\n    best_value = evaluate_function(x, y)\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simulated-annealing search that escapes local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 3 diverse points\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Start with best initial point\n    best_x, best_y, best_value = min(candidates, key=lambda x: x[2])\n    x, y = best_x, best_y\n>>>>>>> REPLACE\n```\n\n## Change 2: Improve Annealing Schedule and Add Adaptive Step Sizes\n\nThe current fixed cooling schedule is too simple. We need adaptive step sizes that balance exploration and exploitation better.\n\n```python\n<<<<<<< SEARCH\n    # SA parameters\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n\n    T = T_init\n    for _ in range(iterations):\n        # Local Gaussian move (std shrinks with T)\n        sigma = 2.0 * T / T_init + 0.1\n        x_new = x + np.random.normal(0, sigma)\n        y_new = y + np.random.normal(0, sigma)\n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n=======\n    # SA parameters with adaptive cooling\n    T_init = 5.0\n    T_final = 1e-3\n    decay = (T_final / T_init) ** (1.0 / iterations)\n    \n    # Adaptive exploration ratio\n    exploration_cutoff = int(iterations * 0.7)\n    \n    T = T_init\n    for i in range(iterations):\n        if i < exploration_cutoff:\n            # Global exploration phase\n            if i % 5 == 0:\n                # Inject random restart for better exploration\n                x_new = np.random.uniform(bounds[0], bounds[1])\n                y_new = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Large random step\n                sigma = 1.0 * T / T_init + 0.5\n                x_new = x + np.random.normal(0, sigma)\n                y_new = y + np.random.normal(0, sigma)\n        else:\n            # Local exploitation phase with shrinking step size\n            step_size = 0.5 * (1 - (i - exploration_cutoff) / (iterations - exploration_cutoff))\n            x_new = x + np.random.normal(0, step_size)\n            y_new = y + np.random.normal(0, step_size)\n        \n        # Reflect inside bounds\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n\n        value = evaluate_function(x_new, y_new)\n        delta = value - evaluate_function(x, y)\n\n        # Accept / reject\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            x, y = x_new, y_new\n            if value < best_value:\n                best_x, best_y, best_value = x, y, value\n\n        T *= decay\n>>>>>>> REPLACE\n```\n\n## Change 3: Add Intensive Local Refinement\n\nThe current algorithm lacks a final intensive local search phase, which is crucial for finding the precise minimum.\n\n```python\n<<<<<<< SEARCH\n    return best_x, best_y, best_value\n=======\n    # Final intensive local search for precision\n    for _ in range(100):\n        step_size = 0.01\n        x_new = best_x + np.random.uniform(-step_size, step_size)\n        y_new = best_y + np.random.uniform(-step_size, step_size)\n        x_new = np.clip(x_new, bounds[0], bounds[1])\n        y_new = np.clip(y_new, bounds[0], bounds[1])\n        value = evaluate_function(x_new, y_new)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x_new, y_new\n    \n    return best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\n## Summary of Improvements\n\n1. **Multi-start initialization**: Starts from 3 diverse points instead of 1, increasing the chance of finding better basins\n2. **Adaptive exploration-exploitation**: Uses 70% of iterations for global exploration with periodic random restarts, then 30% for focused local search\n3. **Improved annealing schedule**: Better step size adaptation based on both temperature and iteration progress\n4. **Final intensive refinement**: 100 iterations of fine-grained local search around the best solution found\n\nThese changes maintain the simplicity of the simulated annealing approach while incorporating proven strategies from the top performers, particularly the adaptive balance between exploration and exploitation that made the 1.4993-scoring program successful."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7037, y=0.6776 with value=-1.5187\", \"distance_to_global\": \"0.0005\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7038, y=0.6769\", \"average_distance_to_global\": \"0.0012\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}