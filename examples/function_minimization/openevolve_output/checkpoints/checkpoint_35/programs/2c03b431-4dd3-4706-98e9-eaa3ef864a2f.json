{"id": "2c03b431-4dd3-4706-98e9-eaa3ef864a2f", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining multi-start, simulated annealing, and local refinement.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 3 initial points for coverage\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on temperature and progress\n            progress = i / (iterations // starts)\n            step_size = temp * (1 - 0.8 * progress)\n            \n            # Generate candidate with mixed exploration\n            if np.random.random() < 0.7:\n                # Local Gaussian perturbation\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Global uniform jump\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature\n            temp = max(min_temp, temp * cooling_rate)\n        \n        # Local refinement around best point\n        for _ in range(20):\n            local_x = best_x + np.random.normal(0, 0.1)\n            local_y = best_y + np.random.normal(0, 0.1)\n            local_x = np.clip(local_x, bounds[0], bounds[1])\n            local_y = np.clip(local_y, bounds[0], bounds[1])\n            local_value = evaluate_function(local_x, local_y)\n            if local_value < best_value:\n                best_x, best_y, best_value = local_x, local_y, local_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "8511c1a0-9902-4124-b11c-a07a13f246ea", "generation": 1, "timestamp": 1764999498.976911, "iteration_found": 24, "metrics": {"runs_successfully": 1.0, "value_score": 0.988988241329391, "distance_score": 0.7036612252499093, "combined_score": 1.3583887323595023, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 80 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9417587299889709, "distance_score": 0.7565915427363771, "combined_score": 1.2147685971231066}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.2148\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4909 \u2192 1.2148. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 66 lines with 98 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9993, distance_score: 0.9810, combined_score: 1.4909, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 4 lines with 5 lines\nChange 2: Replace 5 lines with 9 lines\nChange 3: Replace 3 lines with 15 lines\nChange 4: Replace 9 lines with 14 lines\nChange 5: Replace return best_global_x, best_global_y, best_global_value with 33 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9939, combined_score: 1.4970, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 4 lines with 5 lines\nChange 2: Replace 10 lines with 20 lines\nChange 3: Replace 7 lines with 12 lines\nChange 4: Replace 3 lines with 4 lines\nChange 5: Replace 9 lines with 20 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9966, combined_score: 1.4982, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4982)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive annealing with proper cooling schedule\n        temp = 1.0  # Start higher for better exploration\n        min_temp = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on progress\n            progress = i / (iterations // starts)\n            adaptive_step = step_size * (1 - 0.8 * progress)\n            \n            # Mix of Gaussian and uniform exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for smooth exploration\n                candidate_x = x + np.random.normal(0, adaptive_step)\n                candidate_y = y + np.random.normal(0, adaptive_step)\n            else:\n                # Uniform random direction for broader exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = adaptive_step * np.random.uniform(0.5, 1.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            \n            # Clip to bounds (more stable than wrap-around for this function)\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance with Metropolis criterion\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with probability based on temperature and difference\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature and adapt step size\n            if i % 10 == 0:\n                temp = max(min_temp, temp * cooling_rate)\n                step_size *= 0.98  # More gradual reduction\n        \n        # Enhanced local search with gradient-aware refinement\n        for refinement_iter in range(30):\n            # Mix of random and gradient-based local search\n            if refinement_iter < 20:\n                # Random local exploration\n                lx = best_x + np.random.normal(0, 0.03)\n                ly = best_y + np.random.normal(0, 0.03)\n            else:\n                # Gradient-based refinement\n                eps = 0.001\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                lx = best_x - 0.01 * dx + np.random.normal(0, 0.01)\n                ly = best_y - 0.01 * dy + np.random.normal(0, 0.01)\n            \n            lx = np.clip(lx, bounds[0], bounds[1])\n            ly = np.clip(ly, bounds[0], bounds[1])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9966), Performs well on combined_score (1.4982), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4970)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive simulated annealing with temperature schedule\n        temp = 2.0  # Higher initial temperature for better exploration\n        temp_min = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Generate candidate with mixed exploration strategy\n            if np.random.random() < 0.7:  # 70% local exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = step_size * np.random.uniform(0.1, 0.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            else:  # 30% global jumps using Cauchy distribution\n                candidate_x = x + np.random.standard_cauchy() * step_size * 0.2\n                candidate_y = y + np.random.standard_cauchy() * step_size * 0.2\n            \n            # Toroidal boundary handling (wrap around)\n            candidate_x = bounds[0] + (candidate_x - bounds[0]) % (bounds[1] - bounds[0])\n            candidate_y = bounds[0] + (candidate_y - bounds[0]) % (bounds[1] - bounds[0])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Simplified acceptance - always accept improvements, sometimes accept worse\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            elif np.random.random() < temp:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Dynamic cooling and step size adjustment\n            if i % 20 == 0:\n                step_size *= 0.95\n                temp = max(temp_min, temp * cooling_rate)\n            \n            # Restart mechanism when stuck\n            if i % 50 == 0 and i > 0:\n                # Check if we've made significant progress recently\n                if current_value == best_value:  # No improvement\n                    # Random restart with high probability\n                    if np.random.random() < 0.3:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        current_value = evaluate_function(x, y)\n                        temp = min(1.0, temp * 2)  # Increase temperature for exploration\n        \n        # Adaptive local search refinement\n        local_radius = 0.3\n        for refinement_iter in range(30):\n            # Gradually shrink search radius\n            current_radius = local_radius * (1 - refinement_iter / 30) * (1 - start/starts)\n            lx = best_x + np.random.normal(0, current_radius)\n            ly = best_y + np.random.normal(0, current_radius)\n            lx = bounds[0] + (lx - bounds[0]) % (bounds[1] - bounds[0])\n            ly = bounds[0] + (ly - bounds[0]) % (bounds[1] - bounds[0])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n                # Reset radius when improvement found\n                local_radius = min(0.3, local_radius * 1.1)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    # Final gradient-based refinement\n    x, y = best_global_x, best_global_y\n    eps = 0.001\n    for _ in range(100):\n        # Compute gradient\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        # Gradient descent step with adaptive learning rate\n        learning_rate = 0.01 * (1 - _ / 100)\n        new_x = x - learning_rate * dx\n        new_y = y - learning_rate * dy\n        \n        # Handle boundaries with reflection\n        if new_x < bounds[0] or new_x > bounds[1]:\n            dx = -dx  # Reflect gradient\n            new_x = x + learning_rate * dx\n        if new_y < bounds[0] or new_y > bounds[1]:\n            dy = -dy  # Reflect gradient\n            new_y = y + learning_rate * dy\n        \n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        if new_value < best_global_value:\n            best_global_value = new_value\n            best_global_x, best_global_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9939), Performs well on combined_score (1.4970), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4909)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9993), Performs well on distance_score (0.9810), Performs well on combined_score (1.4909), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3623)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Use a cross pattern search around current best\n                patterns = [(0.1, 0), (-0.1, 0), (0, 0.1), (0, -0.1), \n                           (0.07, 0.07), (-0.07, -0.07), (0.07, -0.07), (-0.07, 0.07)]\n                \n                for dx, dy in patterns:\n                    local_x = best_x + dx * local_search_radius\n                    local_y = best_y + dy * local_search_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Adaptive restart with temperature boost\n            if i > 100 and i % 50 == 0:\n                # Check if we're making progress\n                if abs(best_value - evaluate_function(x, y)) < 1e-6:\n                    # Instead of random restart, boost temperature and move to best found\n                    temp = min(1.0, temp * 2)  # Boost temperature\n                    # Move to best position with some exploration\n                    x = best_x + np.random.normal(0, 0.1)\n                    y = best_y + np.random.normal(0, 0.1)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n        \n        # Final intensification around best point found\n        intensification_radius = 0.05\n        for _ in range(20):\n            int_x = best_x + np.random.normal(0, intensification_radius)\n            int_y = best_y + np.random.normal(0, intensification_radius)\n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4674)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.998  # Slower cooling for better exploration\n        min_temp = 0.001\n        temp_reheat_factor = 1.5  # Allow reheating when stuck\n        \n        # Local search parameters\n        local_search_freq = 15  # More frequent local searches\n        local_search_radius = 0.2  # Larger initial radius\n        local_search_decay = 0.95\n        \n        # Adaptive parameters\n        consecutive_rejects = 0\n        last_improvement_iter = 0\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress and success rate\n            progress = i / (iterations // num_starts)\n            base_step_size = min(2.0, 0.5 * temp) * (bounds[1] - bounds[0]) / 5\n            \n            # Adjust step size based on recent success\n            if consecutive_rejects > 10:\n                step_size = base_step_size * 2.0  # Increase exploration when stuck\n            else:\n                step_size = base_step_size * (1 + 0.5 * np.sin(progress * np.pi))  # Oscillate step size\n            \n            # Generate candidate with mixed Gaussian and uniform perturbation\n            if np.random.random() < 0.7:\n                # Gaussian for smooth exploration\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform for jumping between basins\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                consecutive_rejects = 0\n                last_improvement_iter = i\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, best_value = x, y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                accept_prob = np.exp(improvement / temp) * (1 - 0.3 * progress)\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n                    consecutive_rejects = 0\n                else:\n                    consecutive_rejects += 1\n            \n            # Enhanced local search with gradient information\n            if i % local_search_freq == 0 and i > 0:\n                # Compute approximate gradient\n                eps = 0.001\n                dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                \n                # Perform gradient-based local search\n                for local_iter in range(15):\n                    if local_iter < 5:\n                        # Gradient descent steps\n                        grad_step_size = local_search_radius * (1 - local_iter / 5)\n                        local_x = x - grad_step_size * dx\n                        local_y = y - grad_step_size * dy\n                    else:\n                        # Random local exploration\n                        local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                        local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    \n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                \n                # Decay local search radius\n                local_search_radius *= local_search_decay\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Intelligent restart strategy\n            if i > 50 and (i - last_improvement_iter) > 30:\n                # Check if we're stuck\n                if consecutive_rejects > 20 or (i - last_improvement_iter) > 60:\n                    # Smart restart: jump to area around best solution with noise\n                    restart_noise = min(1.0, temp * 2)\n                    x = best_x + np.random.normal(0, restart_noise)\n                    y = best_y + np.random.normal(0, restart_noise)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    \n                    # Reheat temperature slightly\n                    temp = min(2.0, temp * temp_reheat_factor)\n                    consecutive_rejects = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.7845, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and adaptive sampling.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    best_x, best_y = None, None\n    best_value = float('inf')\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search momentum\n    vx, vy = 0, 0\n    momentum = 0.7\n    learning_rate = 0.1\n    \n    x, y = best_x, best_y\n    current_value = best_value\n    \n    for i in range(iterations):\n        # Adaptive step size based on progress\n        progress = i / iterations\n        step_size = (1 - progress) * 2.0 + 0.1\n        \n        # Simulated annealing exploration\n        if np.random.random() < 0.3 or temp > 0.5:\n            # Global exploration with temperature-based acceptance\n            new_x = x + np.random.uniform(-step_size, step_size)\n            new_y = y + np.random.uniform(-step_size, step_size)\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability based on temperature\n            if new_value < current_value or np.random.random() < np.exp(-(new_value - current_value) / temp):\n                x, y = new_x, new_y\n                current_value = new_value\n                vx, vy = 0, 0  # Reset momentum\n        else:\n            # Local gradient-based search with momentum\n            eps = 0.001\n            dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n            dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n            \n            # Update momentum\n            vx = momentum * vx - learning_rate * dx\n            vy = momentum * vy - learning_rate * dy\n            \n            # Apply momentum with decay\n            new_x = x + vx\n            new_y = y + vy\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < current_value:\n                x, y = new_x, new_y\n                current_value = new_value\n        \n        # Update best\n        if current_value < best_value:\n            best_value = current_value\n            best_x, best_y = x, y\n        \n        # Cool down temperature\n        temp = max(temp_min, temp * alpha)\n    \n    # Final local refinement\n    refinement_steps = 50\n    x, y = best_x, best_y\n    for _ in range(refinement_steps):\n        eps = 0.0001\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        new_x = x - 0.01 * dx\n        new_y = y - 0.01 * dy\n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_value = new_value\n            best_x, best_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 107 lines, Excellent runs_successfully (1.000), Excellent value_score (0.938)\n\n### Inspiration 2 (Score: 1.4909, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 66 lines with 98 lines, Excellent runs_successfully (1.000), Excellent value_score (0.999)\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search algorithm combining simulated annealing and local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize multiple candidates for diversity\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Sort by value and keep best\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_radius = 0.5\n    local_decay = 0.98\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.1:\n            # Global exploration with simulated annealing\n            candidate = candidates[np.random.randint(0, len(candidates))]\n            curr_x, curr_y, curr_val = candidate\n            \n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, local_radius * (5 - 4 * i/iterations))\n            new_x = curr_x + np.random.uniform(-step_size, step_size)\n            new_y = curr_y + np.random.uniform(-step_size, step_size)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_val < curr_val or np.random.random() < np.exp(-(new_val - curr_val) / temp):\n                # Update candidate\n                candidates[candidates.index(candidate)] = (new_x, new_y, new_val)\n                \n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        else:\n            # Local refinement around best solution\n            for _ in range(3):  # Multiple local attempts\n                new_x = best_x + np.random.uniform(-local_radius, local_radius)\n                new_y = best_y + np.random.uniform(-local_radius, local_radius)\n                \n                new_x = np.clip(new_x, bounds[0], bounds[1])\n                new_y = np.clip(new_y, bounds[0], bounds[1])\n                \n                new_val = evaluate_function(new_x, new_y)\n                \n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n        \n        # Update temperature and local radius\n        temp = max(min_temp, temp * temp_decay)\n        local_radius *= local_decay\n        \n        # Occasionally inject random diversity\n        if i % 100 == 0 and i > 0:\n            idx = np.random.randint(0, len(candidates))\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            candidates[idx] = (x, y, evaluate_function(x, y))\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 87 lines with 83 lines, Alternative runs_successfully approach, Alternative combined_score approach\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the evolution history, I can see that the current implementation is a very basic random search that gets stuck in local minima. The top-performing programs use sophisticated hybrid approaches combining simulated annealing, local search, and multi-start strategies. I'll implement a targeted improvement that incorporates key elements from the successful programs while keeping the code concise.\n\n## Analysis of Successful Approaches\n\nThe top performers (scores 1.49+) all share these key features:\n1. **Multi-start initialization** - Multiple random starts to cover the search space\n2. **Simulated annealing** - Temperature-based acceptance of worse solutions to escape local minima\n3. **Adaptive local search** - Gradient-based or pattern-based refinement around promising areas\n4. **Restart mechanisms** - Detecting when stuck and restarting exploration\n\n## Proposed Improvements\n\nI'll implement a hybrid approach that balances exploration and exploitation while keeping the code relatively simple:\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining multi-start, simulated annealing, and local refinement.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 3 initial points for coverage\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on temperature and progress\n            progress = i / (iterations // starts)\n            step_size = temp * (1 - 0.8 * progress)\n            \n            # Generate candidate with mixed exploration\n            if np.random.random() < 0.7:\n                # Local Gaussian perturbation\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Global uniform jump\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature\n            temp = max(min_temp, temp * cooling_rate)\n        \n        # Local refinement around best point\n        for _ in range(20):\n            local_x = best_x + np.random.normal(0, 0.1)\n            local_y = best_y + np.random.normal(0, 0.1)\n            local_x = np.clip(local_x, bounds[0], bounds[1])\n            local_y = np.clip(local_y, bounds[0], bounds[1])\n            local_value = evaluate_function(local_x, local_y)\n            if local_value < best_value:\n                best_x, best_y, best_value = local_x, local_y, local_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n>>>>>>> REPLACE\n```\n\n## Key Improvements Explained\n\n1. **Multi-start Strategy**: Using 3 independent starts provides better coverage of the search space compared to single-start approaches.\n\n2. **Simulated Annealing**: The temperature-based acceptance criterion allows the algorithm to escape local minima by occasionally accepting worse solutions early in the search.\n\n3. **Adaptive Step Size**: The step size decreases with both temperature and progress, enabling broader exploration early and finer refinement later.\n\n4. **Mixed Exploration**: Combining Gaussian perturbations (good for local search) with uniform jumps (good for escaping basins) provides balanced exploration.\n\n5. **Local Refinement**: A final local search phase around the best found solution helps fine-tune the result.\n\n6. **Efficient Resource Allocation**: Dividing iterations among multiple starts ensures each gets sufficient exploration time.\n\nThis approach maintains a good balance between sophistication and simplicity, incorporating proven elements from the top-performing programs while avoiding excessive complexity that could harm reliability or performance."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.6845, y=0.6919 with value=-1.5182\", \"distance_to_global\": \"0.0240\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.6841, y=0.6763\", \"average_distance_to_global\": \"0.4211\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}