{"id": "8733f5b5-548b-4043-8061-790aca26f03f", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Strategic acceptance of worse solutions\n                accept_prob = np.exp(improvement / temp) * (1 - 0.4 * (i / (iterations // num_starts)))\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n            \n            # Adaptive pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Adaptive radius based on progress and success\n                progress = i / (iterations // num_starts)\n                adaptive_radius = local_search_radius * (1 - 0.7 * progress)\n                \n                # Use a more efficient adaptive pattern\n                patterns = [(1, 0), (-1, 0), (0, 1), (0, -1), \n                           (0.7, 0.7), (-0.7, -0.7), (0.7, -0.7), (-0.7, 0.7)]\n                \n                improved = False\n                for dx, dy in patterns:\n                    local_x = best_x + dx * adaptive_radius\n                    local_y = best_y + dy * adaptive_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                        improved = True\n                \n                # Expand search if we found improvement\n                if improved:\n                    local_search_radius = min(0.2, local_search_radius * 1.1)\n                else:\n                    local_search_radius *= 0.9\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Intelligent restart mechanism\n            if i > 50 and i % 30 == 0:\n                # Track improvement history\n                recent_improvement = abs(best_value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Smart restart: combine exploration and exploitation\n                    if np.random.random() < 0.6:\n                        # Explorative restart: jump to new area\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                    else:\n                        # Exploitative restart: perturb around best\n                        x = best_x + np.random.normal(0, 0.2)\n                        y = best_y + np.random.normal(0, 0.2)\n                    \n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    # Moderate temperature boost\n                    temp = min(0.5, temp * 1.5)\n        \n        # Optimized final intensification with gradient guidance\n        intensification_radius = 0.08\n        for refine_iter in range(25):\n            # Mix of random and gradient-directed search\n            if refine_iter < 15:\n                # Random local exploration with decreasing radius\n                current_radius = intensification_radius * (1 - refine_iter / 25)\n                int_x = best_x + np.random.normal(0, current_radius)\n                int_y = best_y + np.random.normal(0, current_radius)\n            else:\n                # Gradient-based refinement\n                eps = 0.0005\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                # Small gradient step with noise\n                int_x = best_x - 0.005 * dx + np.random.normal(0, 0.01)\n                int_y = best_y - 0.005 * dy + np.random.normal(0, 0.01)\n            \n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "35a749bb-aaf3-4989-b833-13d35ba8899d", "generation": 3, "timestamp": 1764999718.413304, "iteration_found": 23, "metrics": {"runs_successfully": 1.0, "value_score": 0.9996718901566005, "distance_score": 0.9964948714761932, "combined_score": 1.4981766097817373, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 16 lines with 28 lines\nChange 2: Replace 6 lines with 12 lines\nChange 3: Replace 11 lines with 24 lines\nChange 4: Replace 12 lines with 20 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9898326518573491, "distance_score": 0.7109070235904382, "combined_score": 1.362282649508709, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.3623\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4909 \u2192 1.3623. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.7057, y=0.6780 with value=-1.5187\n```\n\n### distance_to_global\n```\n0.0017\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.7075, y=0.6800\n```\n\n### average_distance_to_global\n```\n0.4067\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 66 lines with 98 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9993, distance_score: 0.9810, combined_score: 1.4909, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 4 lines with 5 lines\nChange 2: Replace 5 lines with 9 lines\nChange 3: Replace 3 lines with 15 lines\nChange 4: Replace 9 lines with 14 lines\nChange 5: Replace return best_global_x, best_global_y, best_global_value with 33 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9939, combined_score: 1.4970, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 4 lines with 5 lines\nChange 2: Replace 10 lines with 20 lines\nChange 3: Replace 7 lines with 12 lines\nChange 4: Replace 3 lines with 4 lines\nChange 5: Replace 9 lines with 20 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9966, combined_score: 1.4982, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4982)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive annealing with proper cooling schedule\n        temp = 1.0  # Start higher for better exploration\n        min_temp = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on progress\n            progress = i / (iterations // starts)\n            adaptive_step = step_size * (1 - 0.8 * progress)\n            \n            # Mix of Gaussian and uniform exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for smooth exploration\n                candidate_x = x + np.random.normal(0, adaptive_step)\n                candidate_y = y + np.random.normal(0, adaptive_step)\n            else:\n                # Uniform random direction for broader exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = adaptive_step * np.random.uniform(0.5, 1.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            \n            # Clip to bounds (more stable than wrap-around for this function)\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance with Metropolis criterion\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with probability based on temperature and difference\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature and adapt step size\n            if i % 10 == 0:\n                temp = max(min_temp, temp * cooling_rate)\n                step_size *= 0.98  # More gradual reduction\n        \n        # Enhanced local search with gradient-aware refinement\n        for refinement_iter in range(30):\n            # Mix of random and gradient-based local search\n            if refinement_iter < 20:\n                # Random local exploration\n                lx = best_x + np.random.normal(0, 0.03)\n                ly = best_y + np.random.normal(0, 0.03)\n            else:\n                # Gradient-based refinement\n                eps = 0.001\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                lx = best_x - 0.01 * dx + np.random.normal(0, 0.01)\n                ly = best_y - 0.01 * dy + np.random.normal(0, 0.01)\n            \n            lx = np.clip(lx, bounds[0], bounds[1])\n            ly = np.clip(ly, bounds[0], bounds[1])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9966), Performs well on combined_score (1.4982), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4970)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive simulated annealing with temperature schedule\n        temp = 2.0  # Higher initial temperature for better exploration\n        temp_min = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Generate candidate with mixed exploration strategy\n            if np.random.random() < 0.7:  # 70% local exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = step_size * np.random.uniform(0.1, 0.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            else:  # 30% global jumps using Cauchy distribution\n                candidate_x = x + np.random.standard_cauchy() * step_size * 0.2\n                candidate_y = y + np.random.standard_cauchy() * step_size * 0.2\n            \n            # Toroidal boundary handling (wrap around)\n            candidate_x = bounds[0] + (candidate_x - bounds[0]) % (bounds[1] - bounds[0])\n            candidate_y = bounds[0] + (candidate_y - bounds[0]) % (bounds[1] - bounds[0])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Simplified acceptance - always accept improvements, sometimes accept worse\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            elif np.random.random() < temp:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Dynamic cooling and step size adjustment\n            if i % 20 == 0:\n                step_size *= 0.95\n                temp = max(temp_min, temp * cooling_rate)\n            \n            # Restart mechanism when stuck\n            if i % 50 == 0 and i > 0:\n                # Check if we've made significant progress recently\n                if current_value == best_value:  # No improvement\n                    # Random restart with high probability\n                    if np.random.random() < 0.3:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        current_value = evaluate_function(x, y)\n                        temp = min(1.0, temp * 2)  # Increase temperature for exploration\n        \n        # Adaptive local search refinement\n        local_radius = 0.3\n        for refinement_iter in range(30):\n            # Gradually shrink search radius\n            current_radius = local_radius * (1 - refinement_iter / 30) * (1 - start/starts)\n            lx = best_x + np.random.normal(0, current_radius)\n            ly = best_y + np.random.normal(0, current_radius)\n            lx = bounds[0] + (lx - bounds[0]) % (bounds[1] - bounds[0])\n            ly = bounds[0] + (ly - bounds[0]) % (bounds[1] - bounds[0])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n                # Reset radius when improvement found\n                local_radius = min(0.3, local_radius * 1.1)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    # Final gradient-based refinement\n    x, y = best_global_x, best_global_y\n    eps = 0.001\n    for _ in range(100):\n        # Compute gradient\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        # Gradient descent step with adaptive learning rate\n        learning_rate = 0.01 * (1 - _ / 100)\n        new_x = x - learning_rate * dx\n        new_y = y - learning_rate * dy\n        \n        # Handle boundaries with reflection\n        if new_x < bounds[0] or new_x > bounds[1]:\n            dx = -dx  # Reflect gradient\n            new_x = x + learning_rate * dx\n        if new_y < bounds[0] or new_y > bounds[1]:\n            dy = -dy  # Reflect gradient\n            new_y = y + learning_rate * dy\n        \n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        if new_value < best_global_value:\n            best_global_value = new_value\n            best_global_x, best_global_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9939), Performs well on combined_score (1.4970), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4909)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9993), Performs well on distance_score (0.9810), Performs well on combined_score (1.4909), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4674)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.998  # Slower cooling for better exploration\n        min_temp = 0.001\n        temp_reheat_factor = 1.5  # Allow reheating when stuck\n        \n        # Local search parameters\n        local_search_freq = 15  # More frequent local searches\n        local_search_radius = 0.2  # Larger initial radius\n        local_search_decay = 0.95\n        \n        # Adaptive parameters\n        consecutive_rejects = 0\n        last_improvement_iter = 0\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress and success rate\n            progress = i / (iterations // num_starts)\n            base_step_size = min(2.0, 0.5 * temp) * (bounds[1] - bounds[0]) / 5\n            \n            # Adjust step size based on recent success\n            if consecutive_rejects > 10:\n                step_size = base_step_size * 2.0  # Increase exploration when stuck\n            else:\n                step_size = base_step_size * (1 + 0.5 * np.sin(progress * np.pi))  # Oscillate step size\n            \n            # Generate candidate with mixed Gaussian and uniform perturbation\n            if np.random.random() < 0.7:\n                # Gaussian for smooth exploration\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform for jumping between basins\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                consecutive_rejects = 0\n                last_improvement_iter = i\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, best_value = x, y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                accept_prob = np.exp(improvement / temp) * (1 - 0.3 * progress)\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n                    consecutive_rejects = 0\n                else:\n                    consecutive_rejects += 1\n            \n            # Enhanced local search with gradient information\n            if i % local_search_freq == 0 and i > 0:\n                # Compute approximate gradient\n                eps = 0.001\n                dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                \n                # Perform gradient-based local search\n                for local_iter in range(15):\n                    if local_iter < 5:\n                        # Gradient descent steps\n                        grad_step_size = local_search_radius * (1 - local_iter / 5)\n                        local_x = x - grad_step_size * dx\n                        local_y = y - grad_step_size * dy\n                    else:\n                        # Random local exploration\n                        local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                        local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    \n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                \n                # Decay local search radius\n                local_search_radius *= local_search_decay\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Intelligent restart strategy\n            if i > 50 and (i - last_improvement_iter) > 30:\n                # Check if we're stuck\n                if consecutive_rejects > 20 or (i - last_improvement_iter) > 60:\n                    # Smart restart: jump to area around best solution with noise\n                    restart_noise = min(1.0, temp * 2)\n                    x = best_x + np.random.normal(0, restart_noise)\n                    y = best_y + np.random.normal(0, restart_noise)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    \n                    # Reheat temperature slightly\n                    temp = min(2.0, temp * temp_reheat_factor)\n                    consecutive_rejects = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.3623)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Use a cross pattern search around current best\n                patterns = [(0.1, 0), (-0.1, 0), (0, 0.1), (0, -0.1), \n                           (0.07, 0.07), (-0.07, -0.07), (0.07, -0.07), (-0.07, 0.07)]\n                \n                for dx, dy in patterns:\n                    local_x = best_x + dx * local_search_radius\n                    local_y = best_y + dy * local_search_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Adaptive restart with temperature boost\n            if i > 100 and i % 50 == 0:\n                # Check if we're making progress\n                if abs(best_value - evaluate_function(x, y)) < 1e-6:\n                    # Instead of random restart, boost temperature and move to best found\n                    temp = min(1.0, temp * 2)  # Boost temperature\n                    # Move to best position with some exploration\n                    x = best_x + np.random.normal(0, 0.1)\n                    y = best_y + np.random.normal(0, 0.1)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n        \n        # Final intensification around best point found\n        intensification_radius = 0.05\n        for _ in range(20):\n            int_x = best_x + np.random.normal(0, intensification_radius)\n            int_y = best_y + np.random.normal(0, intensification_radius)\n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4982, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive annealing with proper cooling schedule\n        temp = 1.0  # Start higher for better exploration\n        min_temp = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on progress\n            progress = i / (iterations // starts)\n            adaptive_step = step_size * (1 - 0.8 * progress)\n            \n            # Mix of Gaussian and uniform exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for smooth exploration\n                candidate_x = x + np.random.normal(0, adaptive_step)\n                candidate_y = y + np.random.normal(0, adaptive_step)\n            else:\n                # Uniform random direction for broader exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = adaptive_step * np.random.uniform(0.5, 1.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            \n            # Clip to bounds (more stable than wrap-around for this function)\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance with Metropolis criterion\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with probability based on temperature and difference\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature and adapt step size\n            if i % 10 == 0:\n                temp = max(min_temp, temp * cooling_rate)\n                step_size *= 0.98  # More gradual reduction\n        \n        # Enhanced local search with gradient-aware refinement\n        for refinement_iter in range(30):\n            # Mix of random and gradient-based local search\n            if refinement_iter < 20:\n                # Random local exploration\n                lx = best_x + np.random.normal(0, 0.03)\n                ly = best_y + np.random.normal(0, 0.03)\n            else:\n                # Gradient-based refinement\n                eps = 0.001\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                lx = best_x - 0.01 * dx + np.random.normal(0, 0.01)\n                ly = best_y - 0.01 * dy + np.random.normal(0, 0.01)\n            \n            lx = np.clip(lx, bounds[0], bounds[1])\n            ly = np.clip(ly, bounds[0], bounds[1])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (1.000), Excellent distance_score (0.997)\n\n### Inspiration 2 (Score: 0.7845, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and adaptive sampling.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    best_x, best_y = None, None\n    best_value = float('inf')\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search momentum\n    vx, vy = 0, 0\n    momentum = 0.7\n    learning_rate = 0.1\n    \n    x, y = best_x, best_y\n    current_value = best_value\n    \n    for i in range(iterations):\n        # Adaptive step size based on progress\n        progress = i / iterations\n        step_size = (1 - progress) * 2.0 + 0.1\n        \n        # Simulated annealing exploration\n        if np.random.random() < 0.3 or temp > 0.5:\n            # Global exploration with temperature-based acceptance\n            new_x = x + np.random.uniform(-step_size, step_size)\n            new_y = y + np.random.uniform(-step_size, step_size)\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability based on temperature\n            if new_value < current_value or np.random.random() < np.exp(-(new_value - current_value) / temp):\n                x, y = new_x, new_y\n                current_value = new_value\n                vx, vy = 0, 0  # Reset momentum\n        else:\n            # Local gradient-based search with momentum\n            eps = 0.001\n            dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n            dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n            \n            # Update momentum\n            vx = momentum * vx - learning_rate * dx\n            vy = momentum * vy - learning_rate * dy\n            \n            # Apply momentum with decay\n            new_x = x + vx\n            new_y = y + vy\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < current_value:\n                x, y = new_x, new_y\n                current_value = new_value\n        \n        # Update best\n        if current_value < best_value:\n            best_value = current_value\n            best_x, best_y = x, y\n        \n        # Cool down temperature\n        temp = max(temp_min, temp * alpha)\n    \n    # Final local refinement\n    refinement_steps = 50\n    x, y = best_x, best_y\n    for _ in range(refinement_steps):\n        eps = 0.0001\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        new_x = x - 0.01 * dx\n        new_y = y - 0.01 * dy\n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_value = new_value\n            best_x, best_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 107 lines, Excellent runs_successfully (1.000), Excellent value_score (0.938)\n\n### Inspiration 3 (Score: 1.3601, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simple but effective annealing with constant temperature\n        # This prevents premature convergence\n        temp = 0.5\n        step_size = 1.0\n        \n        for i in range(iterations // starts):\n            # Generate candidate with uniform random direction\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * np.random.uniform(0.1, 1.0)\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Toroidal boundary handling (wrap around)\n            candidate_x = bounds[0] + (candidate_x - bounds[0]) % (bounds[1] - bounds[0])\n            candidate_y = bounds[0] + (candidate_y - bounds[0]) % (bounds[1] - bounds[0])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Simplified acceptance - always accept improvements, sometimes accept worse\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            elif np.random.random() < temp:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Gradually reduce step size but keep temperature constant\n            if i % 20 == 0:\n                step_size *= 0.95\n        \n        # Final local search refinement only at the end of each start\n        for _ in range(20):\n            lx = best_x + np.random.normal(0, 0.05)\n            ly = best_y + np.random.normal(0, 0.05)\n            lx = bounds[0] + (lx - bounds[0]) % (bounds[1] - bounds[0])\n            ly = bounds[0] + (ly - bounds[0]) % (bounds[1] - bounds[0])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 66 lines with 68 lines, Excellent runs_successfully (1.000), Excellent value_score (0.990)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Use a cross pattern search around current best\n                patterns = [(0.1, 0), (-0.1, 0), (0, 0.1), (0, -0.1), \n                           (0.07, 0.07), (-0.07, -0.07), (0.07, -0.07), (-0.07, 0.07)]\n                \n                for dx, dy in patterns:\n                    local_x = best_x + dx * local_search_radius\n                    local_y = best_y + dy * local_search_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Adaptive restart with temperature boost\n            if i > 100 and i % 50 == 0:\n                # Check if we're making progress\n                if abs(best_value - evaluate_function(x, y)) < 1e-6:\n                    # Instead of random restart, boost temperature and move to best found\n                    temp = min(1.0, temp * 2)  # Boost temperature\n                    # Move to best position with some exploration\n                    x = best_x + np.random.normal(0, 0.1)\n                    y = best_y + np.random.normal(0, 0.1)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n        \n        # Final intensification around best point found\n        intensification_radius = 0.05\n        for _ in range(20):\n            int_x = best_x + np.random.normal(0, intensification_radius)\n            int_y = best_y + np.random.normal(0, intensification_radius)\n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance data, I can see that while it has good reliability (100% success rate), there's room for improvement in the fitness score. The current implementation is quite good but has some inefficiencies that can be optimized.\n\nHere are my targeted improvements:\n\n## 1. Optimize the pattern search local refinement\n\nThe current pattern search uses a fixed set of patterns with a single radius. I'll make it adaptive and more efficient:\n\n```python\n<<<<<<< SEARCH\n            # Pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Use a cross pattern search around current best\n                patterns = [(0.1, 0), (-0.1, 0), (0, 0.1), (0, -0.1), \n                           (0.07, 0.07), (-0.07, -0.07), (0.07, -0.07), (-0.07, 0.07)]\n                \n                for dx, dy in patterns:\n                    local_x = best_x + dx * local_search_radius\n                    local_y = best_y + dy * local_search_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n=======\n            # Adaptive pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Adaptive radius based on progress and success\n                progress = i / (iterations // num_starts)\n                adaptive_radius = local_search_radius * (1 - 0.7 * progress)\n                \n                # Use a more efficient adaptive pattern\n                patterns = [(1, 0), (-1, 0), (0, 1), (0, -1), \n                           (0.7, 0.7), (-0.7, -0.7), (0.7, -0.7), (-0.7, 0.7)]\n                \n                improved = False\n                for dx, dy in patterns:\n                    local_x = best_x + dx * adaptive_radius\n                    local_y = best_y + dy * adaptive_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                        improved = True\n                \n                # Expand search if we found improvement\n                if improved:\n                    local_search_radius = min(0.2, local_search_radius * 1.1)\n                else:\n                    local_search_radius *= 0.9\n>>>>>>> REPLACE\n```\n\n## 2. Improve the acceptance criteria and temperature scheduling\n\nThe current acceptance is too permissive. I'll make it more strategic:\n\n```python\n<<<<<<< SEARCH\n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n=======\n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Strategic acceptance of worse solutions\n                accept_prob = np.exp(improvement / temp) * (1 - 0.4 * (i / (iterations // num_starts)))\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n>>>>>>> REPLACE\n```\n\n## 3. Optimize the final intensification phase\n\nThe current intensification is too uniform. I'll make it more targeted:\n\n```python\n<<<<<<< SEARCH\n        # Final intensification around best point found\n        intensification_radius = 0.05\n        for _ in range(20):\n            int_x = best_x + np.random.normal(0, intensification_radius)\n            int_y = best_y + np.random.normal(0, intensification_radius)\n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n=======\n        # Optimized final intensification with gradient guidance\n        intensification_radius = 0.08\n        for refine_iter in range(25):\n            # Mix of random and gradient-directed search\n            if refine_iter < 15:\n                # Random local exploration with decreasing radius\n                current_radius = intensification_radius * (1 - refine_iter / 25)\n                int_x = best_x + np.random.normal(0, current_radius)\n                int_y = best_y + np.random.normal(0, current_radius)\n            else:\n                # Gradient-based refinement\n                eps = 0.0005\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                # Small gradient step with noise\n                int_x = best_x - 0.005 * dx + np.random.normal(0, 0.01)\n                int_y = best_y - 0.005 * dy + np.random.normal(0, 0.01)\n            \n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n>>>>>>> REPLACE\n```\n\n## 4. Improve the restart mechanism\n\nThe current restart is too simple. I'll make it more intelligent:\n\n```python\n<<<<<<< SEARCH\n            # Adaptive restart with temperature boost\n            if i > 100 and i % 50 == 0:\n                # Check if we're making progress\n                if abs(best_value - evaluate_function(x, y)) < 1e-6:\n                    # Instead of random restart, boost temperature and move to best found\n                    temp = min(1.0, temp * 2)  # Boost temperature\n                    # Move to best position with some exploration\n                    x = best_x + np.random.normal(0, 0.1)\n                    y = best_y + np.random.normal(0, 0.1)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n=======\n            # Intelligent restart mechanism\n            if i > 50 and i % 30 == 0:\n                # Track improvement history\n                recent_improvement = abs(best_value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Smart restart: combine exploration and exploitation\n                    if np.random.random() < 0.6:\n                        # Explorative restart: jump to new area\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                    else:\n                        # Exploitative restart: perturb around best\n                        x = best_x + np.random.normal(0, 0.2)\n                        y = best_y + np.random.normal(0, 0.2)\n                    \n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    # Moderate temperature boost\n                    temp = min(0.5, temp * 1.5)\n>>>>>>> REPLACE\n```\n\nThese improvements focus on:\n\n1. **Adaptive local search**: The pattern search now adapts its radius based on success and progress, making it more efficient\n2. **Strategic acceptance**: Better balance between exploration and exploitation with adaptive acceptance probability\n3. **Gradient-guided refinement**: The final intensification uses gradient information for more targeted search\n4. **Intelligent restarts**: More sophisticated restart mechanism that balances exploration and exploitation\n\nThese changes should improve the fitness score by making the algorithm more efficient at finding the global minimum while maintaining its reliability."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7015, y=0.6769 with value=-1.5187\", \"distance_to_global\": \"0.0027\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7042, y=0.6788\", \"average_distance_to_global\": \"0.0035\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}