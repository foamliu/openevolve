{"id": "47926744-2de9-4727-bcef-738e0534db8c", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.998  # Slower cooling for better exploration\n        min_temp = 0.001\n        temp_reheat_factor = 1.5  # Allow reheating when stuck\n        \n        # Local search parameters\n        local_search_freq = 15  # More frequent local searches\n        local_search_radius = 0.2  # Larger initial radius\n        local_search_decay = 0.95\n        \n        # Adaptive parameters\n        consecutive_rejects = 0\n        last_improvement_iter = 0\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress and success rate\n            progress = i / (iterations // num_starts)\n            base_step_size = min(2.0, 0.5 * temp) * (bounds[1] - bounds[0]) / 5\n            \n            # Adjust step size based on recent success\n            if consecutive_rejects > 10:\n                step_size = base_step_size * 2.0  # Increase exploration when stuck\n            else:\n                step_size = base_step_size * (1 + 0.5 * np.sin(progress * np.pi))  # Oscillate step size\n            \n            # Generate candidate with mixed Gaussian and uniform perturbation\n            if np.random.random() < 0.7:\n                # Gaussian for smooth exploration\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform for jumping between basins\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                consecutive_rejects = 0\n                last_improvement_iter = i\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, best_value = x, y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                accept_prob = np.exp(improvement / temp) * (1 - 0.3 * progress)\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n                    consecutive_rejects = 0\n                else:\n                    consecutive_rejects += 1\n            \n            # Enhanced local search with gradient information\n            if i % local_search_freq == 0 and i > 0:\n                # Compute approximate gradient\n                eps = 0.001\n                dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                \n                # Perform gradient-based local search\n                for local_iter in range(15):\n                    if local_iter < 5:\n                        # Gradient descent steps\n                        grad_step_size = local_search_radius * (1 - local_iter / 5)\n                        local_x = x - grad_step_size * dx\n                        local_y = y - grad_step_size * dy\n                    else:\n                        # Random local exploration\n                        local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                        local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    \n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                \n                # Decay local search radius\n                local_search_radius *= local_search_decay\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Intelligent restart strategy\n            if i > 50 and (i - last_improvement_iter) > 30:\n                # Check if we're stuck\n                if consecutive_rejects > 20 or (i - last_improvement_iter) > 60:\n                    # Smart restart: jump to area around best solution with noise\n                    restart_noise = min(1.0, temp * 2)\n                    x = best_x + np.random.normal(0, restart_noise)\n                    y = best_y + np.random.normal(0, restart_noise)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    \n                    # Reheat temperature slightly\n                    temp = min(2.0, temp * temp_reheat_factor)\n                    consecutive_rejects = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "167a46d8-0621-4c96-8435-42f7099bfd45", "generation": 2, "timestamp": 1764999070.3063705, "iteration_found": 13, "metrics": {"runs_successfully": 1.0, "value_score": 0.9943570327681079, "distance_score": 0.9370226250137967, "combined_score": 1.4674279558322894, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 14 lines\nChange 2: Replace 6 lines with 19 lines\nChange 3: Replace 6 lines with 18 lines\nChange 4: Replace 12 lines with 29 lines\nChange 5: Replace 9 lines with 15 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9344676348558394, "distance_score": 0.6111942120585718, "combined_score": 1.0207104972545895, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.0207\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 1.0207\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.6290, y=0.6705 with value=-1.5147\n```\n\n### distance_to_global\n```\n0.0754\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=1.5639, y=2.9896\n```\n\n### average_distance_to_global\n```\n0.6361\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 87 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9345, distance_score: 0.6112, combined_score: 1.0207, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 83 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9574, distance_score: 0.8183, combined_score: 1.2625\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2625)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search algorithm combining simulated annealing and local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize multiple candidates for diversity\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Sort by value and keep best\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_radius = 0.5\n    local_decay = 0.98\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.1:\n            # Global exploration with simulated annealing\n            candidate = candidates[np.random.randint(0, len(candidates))]\n            curr_x, curr_y, curr_val = candidate\n            \n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, local_radius * (5 - 4 * i/iterations))\n            new_x = curr_x + np.random.uniform(-step_size, step_size)\n            new_y = curr_y + np.random.uniform(-step_size, step_size)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_val < curr_val or np.random.random() < np.exp(-(new_val - curr_val) / temp):\n                # Update candidate\n                candidates[candidates.index(candidate)] = (new_x, new_y, new_val)\n                \n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        else:\n            # Local refinement around best solution\n            for _ in range(3):  # Multiple local attempts\n                new_x = best_x + np.random.uniform(-local_radius, local_radius)\n                new_y = best_y + np.random.uniform(-local_radius, local_radius)\n                \n                new_x = np.clip(new_x, bounds[0], bounds[1])\n                new_y = np.clip(new_y, bounds[0], bounds[1])\n                \n                new_val = evaluate_function(new_x, new_y)\n                \n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Update temperature and local radius\n        temp = max(min_temp, temp * temp_decay)\n        local_radius *= local_decay\n        \n        # Occasionally inject random diversity\n        if i % 100 == 0 and i > 0:\n            idx = np.random.randint(0, len(candidates))\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            candidates[idx] = (x, y, evaluate_function(x, y))\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9574), Performs well on distance_score (0.8183), Performs well on combined_score (1.2625)\n\n### Program 2 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n### Program 3 (Score: 1.0207)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9345), Performs well on distance_score (0.6112), Performs well on combined_score (1.0207), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.0131)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Enhanced random search with local exploitation and restart mechanism.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start approach with intelligent restarts\n    num_restarts = 5\n    iter_per_restart = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize new starting point with some bias toward promising areas\n        if restart == 0:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            # Start near the best found so far with some randomness\n            x = best_global_x + np.random.normal(0, 0.5)\n            y = best_global_y + np.random.normal(0, 0.5)\n            x = np.clip(x, bounds[0], bounds[1])\n            y = np.clip(y, bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_local_x, best_local_y, best_local_value = x, y, current_value\n        \n        # Local search phase with decreasing exploration\n        for i in range(iter_per_restart):\n            # Adaptive search radius based on progress\n            progress = (restart * iter_per_restart + i) / iterations\n            search_radius = 2.0 * (1 - progress) + 0.1\n            \n            # Generate candidate with local perturbation\n            candidate_x = x + np.random.uniform(-search_radius, search_radius)\n            candidate_y = y + np.random.uniform(-search_radius, search_radius)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or occasionally accept worse to escape local minima\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_local_value:\n                    best_local_x, best_local_y, best_local_value = candidate_x, candidate_y, candidate_value\n            elif np.random.random() < 0.05 * (1 - progress):  # Decreasing acceptance of worse solutions\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n        \n        # Update global best\n        if best_local_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_local_x, best_local_y, best_local_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 0.7845)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and adaptive sampling.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    best_x, best_y = None, None\n    best_value = float('inf')\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search momentum\n    vx, vy = 0, 0\n    momentum = 0.7\n    learning_rate = 0.1\n    \n    x, y = best_x, best_y\n    current_value = best_value\n    \n    for i in range(iterations):\n        # Adaptive step size based on progress\n        progress = i / iterations\n        step_size = (1 - progress) * 2.0 + 0.1\n        \n        # Simulated annealing exploration\n        if np.random.random() < 0.3 or temp > 0.5:\n            # Global exploration with temperature-based acceptance\n            new_x = x + np.random.uniform(-step_size, step_size)\n            new_y = y + np.random.uniform(-step_size, step_size)\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability based on temperature\n            if new_value < current_value or np.random.random() < np.exp(-(new_value - current_value) / temp):\n                x, y = new_x, new_y\n                current_value = new_value\n                vx, vy = 0, 0  # Reset momentum\n        else:\n            # Local gradient-based search with momentum\n            eps = 0.001\n            dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n            dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n            \n            # Update momentum\n            vx = momentum * vx - learning_rate * dx\n            vy = momentum * vy - learning_rate * dy\n            \n            # Apply momentum with decay\n            new_x = x + vx\n            new_y = y + vy\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < current_value:\n                x, y = new_x, new_y\n                current_value = new_value\n        \n        # Update best\n        if current_value < best_value:\n            best_value = current_value\n            best_x, best_y = x, y\n        \n        # Cool down temperature\n        temp = max(temp_min, temp * alpha)\n    \n    # Final local refinement\n    refinement_steps = 50\n    x, y = best_x, best_y\n    for _ in range(refinement_steps):\n        eps = 0.0001\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        new_x = x - 0.01 * dx\n        new_y = y - 0.01 * dy\n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_value = new_value\n            best_x, best_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.2625, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search algorithm combining simulated annealing and local refinement.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize multiple candidates for diversity\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Sort by value and keep best\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    # Local search parameters\n    local_radius = 0.5\n    local_decay = 0.98\n    \n    for i in range(iterations):\n        # Choose search strategy based on temperature\n        if temp > 0.1:\n            # Global exploration with simulated annealing\n            candidate = candidates[np.random.randint(0, len(candidates))]\n            curr_x, curr_y, curr_val = candidate\n            \n            # Generate neighbor with adaptive step size\n            step_size = min(2.0, local_radius * (5 - 4 * i/iterations))\n            new_x = curr_x + np.random.uniform(-step_size, step_size)\n            new_y = curr_y + np.random.uniform(-step_size, step_size)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_val < curr_val or np.random.random() < np.exp(-(new_val - curr_val) / temp):\n                # Update candidate\n                candidates[candidates.index(candidate)] = (new_x, new_y, new_val)\n                \n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        else:\n            # Local refinement around best solution\n            for _ in range(3):  # Multiple local attempts\n                new_x = best_x + np.random.uniform(-local_radius, local_radius)\n                new_y = best_y + np.random.uniform(-local_radius, local_radius)\n                \n                new_x = np.clip(new_x, bounds[0], bounds[1])\n                new_y = np.clip(new_y, bounds[0], bounds[1])\n                \n                new_val = evaluate_function(new_x, new_y)\n                \n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Update temperature and local radius\n        temp = max(min_temp, temp * temp_decay)\n        local_radius *= local_decay\n        \n        # Occasionally inject random diversity\n        if i % 100 == 0 and i > 0:\n            idx = np.random.randint(0, len(candidates))\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            candidates[idx] = (x, y, evaluate_function(x, y))\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 83 lines, Excellent runs_successfully (1.000), Excellent value_score (0.957)\n\n### Inspiration 2 (Score: 1.2148, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.942), Excellent combined_score (1.215)\n\n### Inspiration 3 (Score: 0.7845, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing, local search, and adaptive sampling.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization with 5 random points\n    starts = 5\n    best_x, best_y = None, None\n    best_value = float('inf')\n    \n    for _ in range(starts):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n    \n    # Simulated annealing parameters\n    temp = 1.0\n    temp_min = 0.001\n    alpha = 0.995\n    \n    # Local search momentum\n    vx, vy = 0, 0\n    momentum = 0.7\n    learning_rate = 0.1\n    \n    x, y = best_x, best_y\n    current_value = best_value\n    \n    for i in range(iterations):\n        # Adaptive step size based on progress\n        progress = i / iterations\n        step_size = (1 - progress) * 2.0 + 0.1\n        \n        # Simulated annealing exploration\n        if np.random.random() < 0.3 or temp > 0.5:\n            # Global exploration with temperature-based acceptance\n            new_x = x + np.random.uniform(-step_size, step_size)\n            new_y = y + np.random.uniform(-step_size, step_size)\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability based on temperature\n            if new_value < current_value or np.random.random() < np.exp(-(new_value - current_value) / temp):\n                x, y = new_x, new_y\n                current_value = new_value\n                vx, vy = 0, 0  # Reset momentum\n        else:\n            # Local gradient-based search with momentum\n            eps = 0.001\n            dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n            dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n            \n            # Update momentum\n            vx = momentum * vx - learning_rate * dx\n            vy = momentum * vy - learning_rate * dy\n            \n            # Apply momentum with decay\n            new_x = x + vx\n            new_y = y + vy\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < current_value:\n                x, y = new_x, new_y\n                current_value = new_value\n        \n        # Update best\n        if current_value < best_value:\n            best_value = current_value\n            best_x, best_y = x, y\n        \n        # Cool down temperature\n        temp = max(temp_min, temp * alpha)\n    \n    # Final local refinement\n    refinement_steps = 50\n    x, y = best_x, best_y\n    for _ in range(refinement_steps):\n        eps = 0.0001\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        new_x = x - 0.01 * dx\n        new_y = y - 0.01 * dy\n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_value = new_value\n            best_x, best_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 107 lines, Excellent runs_successfully (1.000), Excellent value_score (0.938)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n            \n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and its performance metrics, I can see that while it has good reliability (100% success rate), its fitness score of 1.0207 indicates room for improvement. The key issues appear to be:\n\n1. The algorithm is getting stuck in local minima despite restart mechanisms\n2. The search strategy could be more adaptive to the function's characteristics\n3. The local search refinement could be more targeted\n\nHere are my targeted improvements:\n\n## Change 1: Improve Search Strategy with Adaptive Temperature and Step Size\n\nThe current temperature and step size adaptation is too linear. I'll implement a more sophisticated approach that adapts based on search progress and success rate.\n\n```python\n<<<<<<< SEARCH\n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n=======\n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.998  # Slower cooling for better exploration\n        min_temp = 0.001\n        temp_reheat_factor = 1.5  # Allow reheating when stuck\n        \n        # Local search parameters\n        local_search_freq = 15  # More frequent local searches\n        local_search_radius = 0.2  # Larger initial radius\n        local_search_decay = 0.95\n        \n        # Adaptive parameters\n        consecutive_rejects = 0\n        last_improvement_iter = 0\n>>>>>>> REPLACE\n```\n\n## Change 2: Enhance Candidate Generation with Function-Specific Knowledge\n\nThe function has sinusoidal components that create many local minima. I'll add a more sophisticated candidate generation that considers the function's structure.\n\n```python\n<<<<<<< SEARCH\n            # Adaptive step size based on progress\n            step_size = min(1.0, 0.5 * temp) * (bounds[1] - bounds[0])\n            \n            # Generate candidate with Gaussian perturbation\n            candidate_x = x + np.random.normal(0, step_size)\n            candidate_y = y + np.random.normal(0, step_size)\n=======\n            # Adaptive step size based on progress and success rate\n            progress = i / (iterations // num_starts)\n            base_step_size = min(2.0, 0.5 * temp) * (bounds[1] - bounds[0]) / 5\n            \n            # Adjust step size based on recent success\n            if consecutive_rejects > 10:\n                step_size = base_step_size * 2.0  # Increase exploration when stuck\n            else:\n                step_size = base_step_size * (1 + 0.5 * np.sin(progress * np.pi))  # Oscillate step size\n            \n            # Generate candidate with mixed Gaussian and uniform perturbation\n            if np.random.random() < 0.7:\n                # Gaussian for smooth exploration\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform for jumping between basins\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n>>>>>>> REPLACE\n```\n\n## Change 3: Improve Acceptance Criteria and Stagnation Detection\n\nBetter handling of when to accept worse solutions and more intelligent restart logic.\n\n```python\n<<<<<<< SEARCH\n            # Acceptance criteria (simulated annealing)\n            if candidate_value < value or np.random.random() < np.exp((value - candidate_value) / temp):\n                x, y, value = candidate_x, candidate_y, candidate_value\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, value\n=======\n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                consecutive_rejects = 0\n                last_improvement_iter = i\n                \n                if value < best_value:\n                    best_x, best_y, best_value = x, y, best_value = x, y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                accept_prob = np.exp(improvement / temp) * (1 - 0.3 * progress)\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n                    consecutive_rejects = 0\n                else:\n                    consecutive_rejects += 1\n>>>>>>> REPLACE\n```\n\n## Change 4: Enhanced Local Search with Gradient Information\n\nAdd gradient-based local search for more efficient refinement near promising areas.\n\n```python\n<<<<<<< SEARCH\n            # Periodic local search refinement\n            if i % local_search_freq == 0 and i > 0:\n                for _ in range(10):\n                    local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n=======\n            # Enhanced local search with gradient information\n            if i % local_search_freq == 0 and i > 0:\n                # Compute approximate gradient\n                eps = 0.001\n                dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                \n                # Perform gradient-based local search\n                for local_iter in range(15):\n                    if local_iter < 5:\n                        # Gradient descent steps\n                        grad_step_size = local_search_radius * (1 - local_iter / 5)\n                        local_x = x - grad_step_size * dx\n                        local_y = y - grad_step_size * dy\n                    else:\n                        # Random local exploration\n                        local_x = x + np.random.uniform(-local_search_radius, local_search_radius)\n                        local_y = y + np.random.uniform(-local_search_radius, local_search_radius)\n                    \n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                \n                # Decay local search radius\n                local_search_radius *= local_search_decay\n>>>>>>> REPLACE\n```\n\n## Change 5: More Intelligent Restart Strategy\n\nReplace the simple restart mechanism with a more sophisticated approach that considers search history.\n\n```python\n<<<<<<< SEARCH\n            # Restart if stuck (detected by slow improvement)\n            if i > 100 and i % 50 == 0:\n                recent_improvement = abs(value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Jump to new random location with small probability\n                    if np.random.random() < 0.1:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        value = evaluate_function(x, y)\n=======\n            # Intelligent restart strategy\n            if i > 50 and (i - last_improvement_iter) > 30:\n                # Check if we're stuck\n                if consecutive_rejects > 20 or (i - last_improvement_iter) > 60:\n                    # Smart restart: jump to area around best solution with noise\n                    restart_noise = min(1.0, temp * 2)\n                    x = best_x + np.random.normal(0, restart_noise)\n                    y = best_y + np.random.normal(0, restart_noise)\n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    \n                    # Reheat temperature slightly\n                    temp = min(2.0, temp * temp_reheat_factor)\n                    consecutive_rejects = 0\n>>>>>>> REPLACE\n```\n\nThese improvements focus on:\n\n1. **Better exploration-exploitation balance** through adaptive temperature and step size control\n2. **Smarter candidate generation** that mixes Gaussian and uniform perturbations\n3. **Enhanced local search** using gradient information for more efficient refinement\n4. **More intelligent restart logic** that considers search history and stagnation patterns\n5. **Function-aware adaptations** that account for the sinusoidal nature of the objective function\n\nThe goal is to maintain the high reliability while improving the solution quality and distance to global minimum, which should increase the overall fitness score."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.6669, y=0.6591 with value=-1.5168\", \"distance_to_global\": \"0.0416\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.6532, y=0.6831\", \"average_distance_to_global\": \"0.0672\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}