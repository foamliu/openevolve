{"id": "752beb5a-cdc7-450f-8f2e-fb80306bf8e1", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Efficient hybrid search with multiple candidates and adaptive cooling.\n    \"\"\"\n    # Initialize 8 diverse candidates with Latin Hypercube sampling\n    candidates = []\n    for i in range(8):\n        # Latin Hypercube sampling for better coverage\n        x = bounds[0] + (i + 0.5) * (bounds[1] - bounds[0]) / 8 + np.random.uniform(-0.3, 0.3)\n        y = bounds[0] + np.random.uniform(0, 8) * (bounds[1] - bounds[0]) / 8 + np.random.uniform(-0.3, 0.3)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        candidates.append([x, y, evaluate_function(x, y)])\n    \n    # Sort by value and preserve elite\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    elite = candidates[0]  # Preserve best solution\n    \n    # Simulated annealing parameters\n    temp = 2.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    for i in range(iterations):\n        # Select random candidate for exploration\n        idx = np.random.randint(0, len(candidates))\n        x, y, val = candidates[idx]\n        \n        # Adaptive step size based on temperature and progress\n        step_scale = min(1.0, temp) * (1 - i/iterations * 0.8)\n        step_size = step_scale * (bounds[1] - bounds[0]) / 5\n        \n        # Mix exploration strategies based on temperature and progress\n        if temp > 0.5 or np.random.random() < 0.3:\n            # Heavy-tailed Cauchy distribution for better exploration\n            new_x = np.clip(x + np.random.standard_cauchy() * step_size * 0.5, bounds[0], bounds[1])\n            new_y = np.clip(y + np.random.standard_cauchy() * step_size * 0.5, bounds[0], bounds[1])\n        else:\n            # Gaussian for fine-tuning\n            angle = np.random.uniform(0, 2 * np.pi)\n            distance = np.random.normal(0, step_size)\n            new_x = np.clip(x + distance * np.cos(angle), bounds[0], bounds[1])\n            new_y = np.clip(y + distance * np.sin(angle), bounds[0], bounds[1])\n        new_val = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_val < val or np.random.random() < np.exp(-(new_val - val) / temp):\n            candidates[idx] = [new_x, new_y, new_val]\n            \n            # Update global best\n            if new_val < best_value:\n                best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Enhanced local search with gradient information\n        if temp < 0.2 and i % 15 == 0:\n            # Compute approximate gradient\n            eps = 0.001\n            dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n            dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n            \n            # Multiple local search strategies\n            for strategy in range(5):\n                if strategy < 2:\n                    # Gradient-based steps\n                    step_size = 0.01 * (1 - strategy * 0.5)\n                    lx = best_x - step_size * dx + np.random.normal(0, 0.01)\n                    ly = best_y - step_size * dy + np.random.normal(0, 0.01)\n                else:\n                    # Random local exploration with decreasing radius\n                    radius = 0.05 * (0.5 ** (strategy - 2))\n                    angle = np.random.uniform(0, 2 * np.pi)\n                    distance = np.random.normal(0, radius)\n                    lx = best_x + distance * np.cos(angle)\n                    ly = best_y + distance * np.sin(angle)\n                \n                lx = np.clip(lx, bounds[0], bounds[1])\n                ly = np.clip(ly, bounds[0], bounds[1])\n                lval = evaluate_function(lx, ly)\n                if lval < best_value:\n                    best_x, best_y, best_value = lx, ly, lval\n        \n        # Cool down temperature\n        temp = max(min_temp, temp * temp_decay)\n        \n        # Intelligent restart when stuck\n        if i % 50 == 0 and i > 0:\n            # Check if we're making progress\n            current_best = min(candidates, key=lambda x: x[2])\n            if abs(current_best[2] - best_value) < 1e-6:\n                # Replace worst candidates with explorative jumps\n                candidates.sort(key=lambda x: x[2])\n                for j in range(2):  # Replace 2 worst candidates\n                    # Jump around elite or random location\n                    if np.random.random() < 0.7:\n                        # Jump around current best\n                        jump_x = best_x + np.random.normal(0, 1.0)\n                        jump_y = best_y + np.random.normal(0, 1.0)\n                    else:\n                        # Completely random restart\n                        jump_x = np.random.uniform(bounds[0], bounds[1])\n                        jump_y = np.random.uniform(bounds[0], bounds[1])\n                    \n                    jump_x = np.clip(jump_x, bounds[0], bounds[1])\n                    jump_y = np.clip(jump_y, bounds[0], bounds[1])\n                    candidates[-(j+1)] = [jump_x, jump_y, evaluate_function(jump_x, jump_y)]\n    \n    # Final intensification with gradient-based refinement\n    for refine_iter in range(20):\n        # Mix of random and gradient-directed search\n        if refine_iter < 10:\n            # Random local exploration with decreasing radius\n            current_radius = 0.08 * (1 - refine_iter / 20)\n            int_x = best_x + np.random.normal(0, current_radius)\n            int_y = best_y + np.random.normal(0, current_radius)\n        else:\n            # Gradient-based refinement\n            eps = 0.0005\n            dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n            dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n            # Small gradient step with noise\n            int_x = best_x - 0.005 * dx + np.random.normal(0, 0.01)\n            int_y = best_y - 0.005 * dy + np.random.normal(0, 0.01)\n        \n        int_x = np.clip(int_x, bounds[0], bounds[1])\n        int_y = np.clip(int_y, bounds[0], bounds[1])\n        \n        int_value = evaluate_function(int_x, int_y)\n        if int_value < best_value:\n            best_x, best_y, best_value = int_x, int_y, int_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "e9ad2c44-9fac-42f9-b649-b94771e06c3b", "generation": 3, "timestamp": 1765000883.263086, "iteration_found": 35, "metrics": {"runs_successfully": 1.0, "value_score": 0.9899676371077867, "distance_score": 0.7141307160499387, "combined_score": 1.3638345500533124, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 11 lines\nChange 2: Replace 8 lines with 27 lines\nChange 3: Replace 7 lines with 21 lines\nChange 4: Replace 10 lines with 14 lines\nChange 5: Replace return best_x, best_y, best_value with 25 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.980374729070844, "distance_score": 0.5548923859589909, "combined_score": 1.027986096387743, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.0280\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4982 \u2192 1.0280. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.7103, y=0.6684 with value=-1.5185\n```\n\n### distance_to_global\n```\n0.0115\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.6981, y=0.6754\n```\n\n### average_distance_to_global\n```\n0.8022\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 16 lines with 28 lines\nChange 2: Replace 6 lines with 12 lines\nChange 3: Replace 11 lines with 24 lines\nChange 4: Replace 12 lines with 20 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9965, combined_score: 1.4982, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 4 lines with 5 lines\nChange 2: Replace 10 lines with 20 lines\nChange 3: Replace 7 lines with 12 lines\nChange 4: Replace 3 lines with 4 lines\nChange 5: Replace 9 lines with 20 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9966, combined_score: 1.4982, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 10 lines with 14 lines\nChange 2: Replace 3 lines with 11 lines\nChange 3: Replace 7 lines with 21 lines\nChange 4: Replace 8 lines with 29 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9991, combined_score: 1.4993, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4993)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Efficient hybrid search with multiple candidates and adaptive cooling.\n    \"\"\"\n    # Initialize 8 diverse candidates with Latin Hypercube sampling\n    candidates = []\n    for i in range(8):\n        # Latin Hypercube sampling for better coverage\n        x = bounds[0] + (i + 0.5) * (bounds[1] - bounds[0]) / 8 + np.random.uniform(-0.3, 0.3)\n        y = bounds[0] + np.random.uniform(0, 8) * (bounds[1] - bounds[0]) / 8 + np.random.uniform(-0.3, 0.3)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        candidates.append([x, y, evaluate_function(x, y)])\n    \n    # Sort by value and preserve elite\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    elite = candidates[0]  # Preserve best solution\n    \n    # Simulated annealing parameters\n    temp = 2.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    for i in range(iterations):\n        # Select random candidate for exploration\n        idx = np.random.randint(0, len(candidates))\n        x, y, val = candidates[idx]\n        \n        # Adaptive step size based on temperature and progress\n        step_scale = min(1.0, temp) * (1 - i/iterations * 0.8)\n        step_size = step_scale * (bounds[1] - bounds[0]) / 5\n        \n        # Mix exploration strategies based on temperature and progress\n        if temp > 0.5 or np.random.random() < 0.3:\n            # Heavy-tailed Cauchy distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.5\n            new_y = y + np.random.standard_cauchy() * step_size * 0.5\n        else:\n            # Gaussian for fine-tuning\n            angle = np.random.uniform(0, 2 * np.pi)\n            distance = np.random.normal(0, step_size)\n            new_x = x + distance * np.cos(angle)\n            new_y = y + distance * np.sin(angle)\n        new_val = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_val < val or np.random.random() < np.exp(-(new_val - val) / temp):\n            candidates[idx] = [new_x, new_y, new_val]\n            \n            # Update global best\n            if new_val < best_value:\n                best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Enhanced local search with gradient information\n        if temp < 0.2 and i % 15 == 0:\n            # Compute approximate gradient\n            eps = 0.001\n            dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n            dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n            \n            # Multiple local search strategies\n            for strategy in range(5):\n                if strategy < 2:\n                    # Gradient-based steps\n                    step_size = 0.01 * (1 - strategy * 0.5)\n                    lx = best_x - step_size * dx + np.random.normal(0, 0.01)\n                    ly = best_y - step_size * dy + np.random.normal(0, 0.01)\n                else:\n                    # Random local exploration with decreasing radius\n                    radius = 0.05 * (0.5 ** (strategy - 2))\n                    angle = np.random.uniform(0, 2 * np.pi)\n                    distance = np.random.normal(0, radius)\n                    lx = best_x + distance * np.cos(angle)\n                    ly = best_y + distance * np.sin(angle)\n                \n                lx = np.clip(lx, bounds[0], bounds[1])\n                ly = np.clip(ly, bounds[0], bounds[1])\n                lval = evaluate_function(lx, ly)\n                if lval < best_value:\n                    best_x, best_y, best_value = lx, ly, lval\n                    # Update elite if we found better solution\n                    elite = [best_x, best_y, best_value]\n        \n        # Cool down temperature\n        temp = max(min_temp, temp * temp_decay)\n        \n        # Intelligent restart when stuck\n        if i % 50 == 0 and i > 0:\n            # Check if we're making progress\n            current_best = min(candidates, key=lambda x: x[2])\n            if abs(current_best[2] - best_value) < 1e-6:\n                # Replace worst candidates with explorative jumps\n                candidates.sort(key=lambda x: x[2])\n                for j in range(3):  # Replace 3 worst candidates\n                    # Jump around elite or random location\n                    if np.random.random() < 0.7:\n                        # Jump around elite\n                        jump_x = elite[0] + np.random.normal(0, 1.0)\n                        jump_y = elite[1] + np.random.normal(0, 1.0)\n                    else:\n                        # Completely random restart\n                        jump_x = np.random.uniform(bounds[0], bounds[1])\n                        jump_y = np.random.uniform(bounds[0], bounds[1])\n                    \n                    jump_x = np.clip(jump_x, bounds[0], bounds[1])\n                    jump_y = np.clip(jump_y, bounds[0], bounds[1])\n                    candidates[-(j+1)] = [jump_x, jump_y, evaluate_function(jump_x, jump_y)]\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9991), Performs well on combined_score (1.4993), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4982)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive annealing with proper cooling schedule\n        temp = 1.0  # Start higher for better exploration\n        min_temp = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on progress\n            progress = i / (iterations // starts)\n            adaptive_step = step_size * (1 - 0.8 * progress)\n            \n            # Mix of Gaussian and uniform exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for smooth exploration\n                candidate_x = x + np.random.normal(0, adaptive_step)\n                candidate_y = y + np.random.normal(0, adaptive_step)\n            else:\n                # Uniform random direction for broader exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = adaptive_step * np.random.uniform(0.5, 1.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            \n            # Clip to bounds (more stable than wrap-around for this function)\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance with Metropolis criterion\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with probability based on temperature and difference\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature and adapt step size\n            if i % 10 == 0:\n                temp = max(min_temp, temp * cooling_rate)\n                step_size *= 0.98  # More gradual reduction\n        \n        # Enhanced local search with gradient-aware refinement\n        for refinement_iter in range(30):\n            # Mix of random and gradient-based local search\n            if refinement_iter < 20:\n                # Random local exploration\n                lx = best_x + np.random.normal(0, 0.03)\n                ly = best_y + np.random.normal(0, 0.03)\n            else:\n                # Gradient-based refinement\n                eps = 0.001\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                lx = best_x - 0.01 * dx + np.random.normal(0, 0.01)\n                ly = best_y - 0.01 * dy + np.random.normal(0, 0.01)\n            \n            lx = np.clip(lx, bounds[0], bounds[1])\n            ly = np.clip(ly, bounds[0], bounds[1])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9966), Performs well on combined_score (1.4982), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4982)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization combining simulated annealing, \n    local search, and restart mechanisms to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start initialization\n    num_starts = 5\n    best_global_x = None\n    best_global_y = None\n    best_global_value = float('inf')\n    \n    for start in range(num_starts):\n        # Initialize with random point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        # Local search parameters\n        local_search_freq = 20\n        local_search_radius = 0.1\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size with momentum\n            step_size = min(0.5, 0.3 * temp) * (bounds[1] - bounds[0]) / 10\n            \n            # Mix Gaussian and uniform perturbations for better exploration\n            if np.random.random() < 0.7:\n                # Gaussian perturbation for local search\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Uniform perturbation for exploration\n                candidate_x = x + np.random.uniform(-step_size*2, step_size*2)\n                candidate_y = y + np.random.uniform(-step_size*2, step_size*2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria with adaptive tolerance\n            improvement = value - candidate_value\n            if improvement > 0:\n                # Always accept better solutions\n                x, y, value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Strategic acceptance of worse solutions\n                accept_prob = np.exp(improvement / temp) * (1 - 0.4 * (i / (iterations // num_starts)))\n                if np.random.random() < accept_prob:\n                    x, y, value = candidate_x, candidate_y, candidate_value\n            \n            # Adaptive pattern search local refinement\n            if i % local_search_freq == 0 and i > 0:\n                # Adaptive radius based on progress and success\n                progress = i / (iterations // num_starts)\n                adaptive_radius = local_search_radius * (1 - 0.7 * progress)\n                \n                # Use a more efficient adaptive pattern\n                patterns = [(1, 0), (-1, 0), (0, 1), (0, -1), \n                           (0.7, 0.7), (-0.7, -0.7), (0.7, -0.7), (-0.7, 0.7)]\n                \n                improved = False\n                for dx, dy in patterns:\n                    local_x = best_x + dx * adaptive_radius\n                    local_y = best_y + dy * adaptive_radius\n                    local_x = np.clip(local_x, bounds[0], bounds[1])\n                    local_y = np.clip(local_y, bounds[0], bounds[1])\n                    \n                    local_value = evaluate_function(local_x, local_y)\n                    if local_value < best_value:\n                        best_x, best_y, best_value = local_x, local_y, local_value\n                        x, y, value = local_x, local_y, local_value\n                        improved = True\n                \n                # Expand search if we found improvement\n                if improved:\n                    local_search_radius = min(0.2, local_search_radius * 1.1)\n                else:\n                    local_search_radius *= 0.9\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # Intelligent restart mechanism\n            if i > 50 and i % 30 == 0:\n                # Track improvement history\n                recent_improvement = abs(best_value - evaluate_function(x, y))\n                if recent_improvement < 1e-6:\n                    # Smart restart: combine exploration and exploitation\n                    if np.random.random() < 0.6:\n                        # Explorative restart: jump to new area\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                    else:\n                        # Exploitative restart: perturb around best\n                        x = best_x + np.random.normal(0, 0.2)\n                        y = best_y + np.random.normal(0, 0.2)\n                    \n                    x = np.clip(x, bounds[0], bounds[1])\n                    y = np.clip(y, bounds[0], bounds[1])\n                    value = evaluate_function(x, y)\n                    # Moderate temperature boost\n                    temp = min(0.5, temp * 1.5)\n        \n        # Optimized final intensification with gradient guidance\n        intensification_radius = 0.08\n        for refine_iter in range(25):\n            # Mix of random and gradient-directed search\n            if refine_iter < 15:\n                # Random local exploration with decreasing radius\n                current_radius = intensification_radius * (1 - refine_iter / 25)\n                int_x = best_x + np.random.normal(0, current_radius)\n                int_y = best_y + np.random.normal(0, current_radius)\n            else:\n                # Gradient-based refinement\n                eps = 0.0005\n                dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n                dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n                # Small gradient step with noise\n                int_x = best_x - 0.005 * dx + np.random.normal(0, 0.01)\n                int_y = best_y - 0.005 * dy + np.random.normal(0, 0.01)\n            \n            int_x = np.clip(int_x, bounds[0], bounds[1])\n            int_y = np.clip(int_y, bounds[0], bounds[1])\n            \n            int_value = evaluate_function(int_x, int_y)\n            if int_value < best_value:\n                best_x, best_y, best_value = int_x, int_y, int_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9965), Performs well on combined_score (1.4982), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4909)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4970)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive simulated annealing with temperature schedule\n        temp = 2.0  # Higher initial temperature for better exploration\n        temp_min = 0.01\n        cooling_rate = 0.995\n        step_size = 1.5  # Larger initial step size\n        \n        for i in range(iterations // starts):\n            # Generate candidate with mixed exploration strategy\n            if np.random.random() < 0.7:  # 70% local exploration\n                angle = np.random.uniform(0, 2 * np.pi)\n                step = step_size * np.random.uniform(0.1, 0.5)\n                candidate_x = x + step * np.cos(angle)\n                candidate_y = y + step * np.sin(angle)\n            else:  # 30% global jumps using Cauchy distribution\n                candidate_x = x + np.random.standard_cauchy() * step_size * 0.2\n                candidate_y = y + np.random.standard_cauchy() * step_size * 0.2\n            \n            # Toroidal boundary handling (wrap around)\n            candidate_x = bounds[0] + (candidate_x - bounds[0]) % (bounds[1] - bounds[0])\n            candidate_y = bounds[0] + (candidate_y - bounds[0]) % (bounds[1] - bounds[0])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Simplified acceptance - always accept improvements, sometimes accept worse\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            elif np.random.random() < temp:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Dynamic cooling and step size adjustment\n            if i % 20 == 0:\n                step_size *= 0.95\n                temp = max(temp_min, temp * cooling_rate)\n            \n            # Restart mechanism when stuck\n            if i % 50 == 0 and i > 0:\n                # Check if we've made significant progress recently\n                if current_value == best_value:  # No improvement\n                    # Random restart with high probability\n                    if np.random.random() < 0.3:\n                        x = np.random.uniform(bounds[0], bounds[1])\n                        y = np.random.uniform(bounds[0], bounds[1])\n                        current_value = evaluate_function(x, y)\n                        temp = min(1.0, temp * 2)  # Increase temperature for exploration\n        \n        # Adaptive local search refinement\n        local_radius = 0.3\n        for refinement_iter in range(30):\n            # Gradually shrink search radius\n            current_radius = local_radius * (1 - refinement_iter / 30) * (1 - start/starts)\n            lx = best_x + np.random.normal(0, current_radius)\n            ly = best_y + np.random.normal(0, current_radius)\n            lx = bounds[0] + (lx - bounds[0]) % (bounds[1] - bounds[0])\n            ly = bounds[0] + (ly - bounds[0]) % (bounds[1] - bounds[0])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n                # Reset radius when improvement found\n                local_radius = min(0.3, local_radius * 1.1)\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    # Final gradient-based refinement\n    x, y = best_global_x, best_global_y\n    eps = 0.001\n    for _ in range(100):\n        # Compute gradient\n        dx = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n        dy = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n        \n        # Gradient descent step with adaptive learning rate\n        learning_rate = 0.01 * (1 - _ / 100)\n        new_x = x - learning_rate * dx\n        new_y = y - learning_rate * dy\n        \n        # Handle boundaries with reflection\n        if new_x < bounds[0] or new_x > bounds[1]:\n            dx = -dx  # Reflect gradient\n            new_x = x + learning_rate * dx\n        if new_y < bounds[0] or new_y > bounds[1]:\n            dy = -dy  # Reflect gradient\n            new_y = y + learning_rate * dy\n        \n        new_x = np.clip(new_x, bounds[0], bounds[1])\n        new_y = np.clip(new_y, bounds[0], bounds[1])\n        \n        new_value = evaluate_function(new_x, new_y)\n        if new_value < best_global_value:\n            best_global_value = new_value\n            best_global_x, best_global_y = new_x, new_y\n            x, y = new_x, new_y\n        else:\n            break\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4909, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization with aggressive escape mechanisms.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Increase multi-starts for better coverage\n    starts = 7\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # More aggressive simulated annealing parameters\n        temp = 5.0  # Higher initial temperature for more exploration\n        cooling_rate = 0.98  # Faster cooling but with better escape mechanisms\n        min_temp = 0.001\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Track improvement for restart detection\n        no_improvement_counter = 0\n        last_improvement_iter = 0\n        \n        # Simulated annealing phase\n        for i in range(iterations // starts):\n            # Adaptive step size with temperature and progress-based scaling\n            progress = i / (iterations // starts)\n            step_scale = 2.0 * (1 - progress * 0.8)  # Decrease step size over time\n            step_size = min(1.0, temp * step_scale)\n            \n            # Generate candidate with heavy-tailed distribution for better exploration\n            new_x = x + np.random.standard_cauchy() * step_size * 0.3\n            new_y = y + np.random.standard_cauchy() * step_size * 0.3\n            \n            # Ensure bounds with reflection\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Acceptance criteria with enhanced probability for uphill moves\n            if new_value < current_value:\n                x, y, current_value = new_x, new_y, new_value\n                if new_value < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_value\n                    no_improvement_counter = 0\n                    last_improvement_iter = i\n            else:\n                delta = new_value - current_value\n                acceptance_prob = np.exp(-delta / (temp + 1e-10))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = new_x, new_y, new_value\n            \n            # Cool down temperature\n            temp = max(min_temp, temp * cooling_rate)\n            \n            # More frequent and aggressive local search\n            if i % 20 == 0 and i > 0:\n                local_radius = 0.5 * (1 - progress * 0.7)  # Shrink radius over time\n                for _ in range(15):\n                    # Use Gaussian perturbation for local search\n                    lx = best_x + np.random.normal(0, local_radius)\n                    ly = best_y + np.random.normal(0, local_radius)\n                    lx = np.clip(lx, bounds[0], bounds[1])\n                    ly = np.clip(ly, bounds[0], bounds[1])\n                    lvalue = evaluate_function(lx, ly)\n                    if lvalue < best_value:\n                        best_x, best_y, best_value = lx, ly, lvalue\n                        x, y, current_value = lx, ly, lvalue\n                        no_improvement_counter = 0\n                        last_improvement_iter = i\n            \n            # Aggressive restart when stuck\n            no_improvement_counter += 1\n            if no_improvement_counter > 30 or (i - last_improvement_iter) > 40:\n                # Jump to completely new location with high probability\n                if np.random.random() < 0.7:\n                    x = np.random.uniform(bounds[0], bounds[1])\n                    y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(x, y)\n                    # Reset temperature for renewed exploration\n                    temp = min(2.0, temp * 3)\n                    no_improvement_counter = 0\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 66 lines with 98 lines, Excellent runs_successfully (1.000), Excellent value_score (0.999)\n\n### Inspiration 2 (Score: 1.3584, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization combining multi-start, simulated annealing, and local refinement.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 3 initial points for coverage\n    starts = 3\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Simulated annealing parameters\n        temp = 2.0\n        cooling_rate = 0.995\n        min_temp = 0.01\n        \n        for i in range(iterations // starts):\n            # Adaptive step size based on temperature and progress\n            progress = i / (iterations // starts)\n            step_size = temp * (1 - 0.8 * progress)\n            \n            # Generate candidate with mixed exploration\n            if np.random.random() < 0.7:\n                # Local Gaussian perturbation\n                candidate_x = x + np.random.normal(0, step_size)\n                candidate_y = y + np.random.normal(0, step_size)\n            else:\n                # Global uniform jump\n                candidate_x = x + np.random.uniform(-step_size * 2, step_size * 2)\n                candidate_y = y + np.random.uniform(-step_size * 2, step_size * 2)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance criteria\n            if candidate_value < current_value:\n                # Always accept improvements\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with temperature-based probability\n                delta = candidate_value - current_value\n                acceptance_prob = np.exp(-delta / max(temp, 0.001))\n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool temperature\n            temp = max(min_temp, temp * cooling_rate)\n        \n        # Local refinement around best point\n        for _ in range(20):\n            local_x = best_x + np.random.normal(0, 0.1)\n            local_y = best_y + np.random.normal(0, 0.1)\n            local_x = np.clip(local_x, bounds[0], bounds[1])\n            local_y = np.clip(local_y, bounds[0], bounds[1])\n            local_value = evaluate_function(local_x, local_y)\n            if local_value < best_value:\n                best_x, best_y, best_value = local_x, local_y, local_value\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 80 lines, Excellent runs_successfully (1.000), Excellent value_score (0.989)\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Simplified hybrid optimization with enhanced exploration and strategic local search.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 initial points for better coverage\n    starts = 5\n    best_global_x, best_global_y, best_global_value = None, None, float('inf')\n    \n    for start in range(starts):\n        # Initialize with Latin Hypercube sampling for better spread\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive temperature schedule for better exploration/exploitation balance\n        temp = 2.0  # Higher initial temperature\n        cooling_rate = 0.995\n        min_temp = 0.01\n        step_size = 1.0\n        \n        for i in range(iterations // starts):\n            # Generate candidate with uniform random direction\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * np.random.uniform(0.1, 1.0)\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Reflection-based boundary handling (bounce back)\n            if candidate_x < bounds[0] or candidate_x > bounds[1]:\n                candidate_x = bounds[0] + bounds[1] - candidate_x\n            if candidate_y < bounds[0] or candidate_y > bounds[1]:\n                candidate_y = bounds[0] + bounds[1] - candidate_y\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Enhanced acceptance with occasional large jumps\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            elif np.random.random() < temp:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Occasional large jump for better exploration (1% chance)\n            if np.random.random() < 0.01:\n                jump_x = x + np.random.uniform(-2.0, 2.0)\n                jump_y = y + np.random.uniform(-2.0, 2.0)\n                jump_x = np.clip(jump_x, bounds[0], bounds[1])\n                jump_y = np.clip(jump_y, bounds[0], bounds[1])\n                jump_value = evaluate_function(jump_x, jump_y)\n                if jump_value < current_value or np.random.random() < temp * 0.5:\n                    x, y, current_value = jump_x, jump_y, jump_value\n            \n            # Gradually reduce step size and cool temperature\n            if i % 20 == 0:\n                step_size *= 0.95\n                temp = max(min_temp, temp * cooling_rate)\n            \n            # Check for improvement and restart if stuck\n            if current_value < last_best_value - 1e-6:\n                last_best_value = current_value\n                no_improvement_counter = 0\n            else:\n                no_improvement_counter += 1\n            \n            # Restart if stuck for too long\n            if no_improvement_counter > 50:\n                x = best_x + np.random.normal(0, 0.5)\n                y = best_y + np.random.normal(0, 0.5)\n                x = np.clip(x, bounds[0], bounds[1])\n                y = np.clip(y, bounds[0], bounds[1])\n                current_value = evaluate_function(x, y)\n                temp = min(1.0, temp * 1.5)  # Boost temperature for exploration\n                no_improvement_counter = 0\n        \n        # Enhanced local search with pattern search\n        for radius in [0.1, 0.05, 0.02]:\n            # Pattern search around best point\n            patterns = [(1, 0), (-1, 0), (0, 1), (0, -1), \n                       (0.7, 0.7), (-0.7, -0.7), (0.7, -0.7), (-0.7, 0.7)]\n            \n            for dx, dy in patterns:\n                lx = best_x + dx * radius\n                ly = best_y + dy * radius\n                lx = np.clip(lx, bounds[0], bounds[1])\n                ly = np.clip(ly, bounds[0], bounds[1])\n                lvalue = evaluate_function(lx, ly)\n                if lvalue < best_value:\n                    best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Final random local refinement\n        for _ in range(10):\n            lx = best_x + np.random.normal(0, 0.02)\n            ly = best_y + np.random.normal(0, 0.02)\n            lx = np.clip(lx, bounds[0], bounds[1])\n            ly = np.clip(ly, bounds[0], bounds[1])\n            lvalue = evaluate_function(lx, ly)\n            if lvalue < best_value:\n                best_x, best_y, best_value = lx, ly, lvalue\n        \n        # Update global best\n        if best_value < best_global_value:\n            best_global_x, best_global_y, best_global_value = best_x, best_y, best_value\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Alternative runs_successfully approach, Alternative combined_score approach, NumPy-based implementation\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Efficient hybrid search with multiple candidates and adaptive cooling.\n    \"\"\"\n    # Initialize 5 diverse candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append([x, y, evaluate_function(x, y)])\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    \n    # Simulated annealing parameters\n    temp = 2.0\n    temp_decay = 0.995\n    min_temp = 0.01\n    \n    for i in range(iterations):\n        # Select random candidate for exploration\n        idx = np.random.randint(0, len(candidates))\n        x, y, val = candidates[idx]\n        \n        # Adaptive step size based on temperature and progress\n        step_scale = min(1.0, temp) * (1 - i/iterations * 0.8)\n        step_size = step_scale * (bounds[1] - bounds[0]) / 5\n        \n        # Generate candidate with Gaussian perturbation\n        new_x = np.clip(x + np.random.normal(0, step_size), bounds[0], bounds[1])\n        new_y = np.clip(y + np.random.normal(0, step_size), bounds[0], bounds[1])\n        new_val = evaluate_function(new_x, new_y)\n        \n        # Accept or reject based on simulated annealing\n        if new_val < val or np.random.random() < np.exp(-(new_val - val) / temp):\n            candidates[idx] = [new_x, new_y, new_val]\n            \n            # Update global best\n            if new_val < best_value:\n                best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Local refinement when temperature is low\n        if temp < 0.1 and i % 10 == 0:\n            for _ in range(3):\n                lx = np.clip(best_x + np.random.normal(0, 0.05), bounds[0], bounds[1])\n                ly = np.clip(best_y + np.random.normal(0, 0.05), bounds[0], bounds[1])\n                lval = evaluate_function(lx, ly)\n                if lval < best_value:\n                    best_x, best_y, best_value = lx, ly, lval\n        \n        # Cool down temperature\n        temp = max(min_temp, temp * temp_decay)\n        \n        # Inject diversity occasionally\n        if i % 100 == 0 and i > 0:\n            candidates[np.random.randint(0, len(candidates))] = [\n                np.random.uniform(bounds[0], bounds[1]),\n                np.random.uniform(bounds[0], bounds[1]),\n                0  # Will be updated in next iteration\n            ]\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance metrics, I can see that while it achieves 100% success rate, the fitness score has declined from 1.4982 to 1.0280. The main issues appear to be:\n\n1. **Simplified exploration strategy**: Only uses Gaussian perturbation, missing heavy-tailed distributions that help escape local minima\n2. **Limited local search**: Only 3 random local searches when temperature is low\n3. **Basic restart mechanism**: Just injects random candidates without considering search history\n4. **No gradient information**: Missing gradient-based refinement that could improve precision\n\nLet me implement targeted improvements based on the successful patterns from the top-performing programs:\n\n## Change 1: Enhanced Exploration with Heavy-Tailed Distributions\n\n```python\n<<<<<<< SEARCH\n        # Generate candidate with Gaussian perturbation\n        new_x = np.clip(x + np.random.normal(0, step_size), bounds[0], bounds[1])\n        new_y = np.clip(y + np.random.normal(0, step_size), bounds[0], bounds[1])\n=======\n        # Mix exploration strategies based on temperature and progress\n        if temp > 0.5 or np.random.random() < 0.3:\n            # Heavy-tailed Cauchy distribution for better exploration\n            new_x = np.clip(x + np.random.standard_cauchy() * step_size * 0.5, bounds[0], bounds[1])\n            new_y = np.clip(y + np.random.standard_cauchy() * step_size * 0.5, bounds[0], bounds[1])\n        else:\n            # Gaussian for fine-tuning\n            angle = np.random.uniform(0, 2 * np.pi)\n            distance = np.random.normal(0, step_size)\n            new_x = np.clip(x + distance * np.cos(angle), bounds[0], bounds[1])\n            new_y = np.clip(y + distance * np.sin(angle), bounds[0], bounds[1])\n>>>>>>> REPLACE\n```\n\n## Change 2: Enhanced Local Search with Gradient Information\n\n```python\n<<<<<<< SEARCH\n        # Local refinement when temperature is low\n        if temp < 0.1 and i % 10 == 0:\n            for _ in range(3):\n                lx = np.clip(best_x + np.random.normal(0, 0.05), bounds[0], bounds[1])\n                ly = np.clip(best_y + np.random.normal(0, 0.05), bounds[0], bounds[1])\n                lval = evaluate_function(lx, ly)\n                if lval < best_value:\n                    best_x, best_y, best_value = lx, ly, lval\n=======\n        # Enhanced local search with gradient information\n        if temp < 0.2 and i % 15 == 0:\n            # Compute approximate gradient\n            eps = 0.001\n            dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n            dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n            \n            # Multiple local search strategies\n            for strategy in range(5):\n                if strategy < 2:\n                    # Gradient-based steps\n                    step_size = 0.01 * (1 - strategy * 0.5)\n                    lx = best_x - step_size * dx + np.random.normal(0, 0.01)\n                    ly = best_y - step_size * dy + np.random.normal(0, 0.01)\n                else:\n                    # Random local exploration with decreasing radius\n                    radius = 0.05 * (0.5 ** (strategy - 2))\n                    angle = np.random.uniform(0, 2 * np.pi)\n                    distance = np.random.normal(0, radius)\n                    lx = best_x + distance * np.cos(angle)\n                    ly = best_y + distance * np.sin(angle)\n                \n                lx = np.clip(lx, bounds[0], bounds[1])\n                ly = np.clip(ly, bounds[0], bounds[1])\n                lval = evaluate_function(lx, ly)\n                if lval < best_value:\n                    best_x, best_y, best_value = lx, ly, lval\n>>>>>>> REPLACE\n```\n\n## Change 3: Intelligent Restart with Elite Preservation\n\n```python\n<<<<<<< SEARCH\n        # Inject diversity occasionally\n        if i % 100 == 0 and i > 0:\n            candidates[np.random.randint(0, len(candidates))] = [\n                np.random.uniform(bounds[0], bounds[1]),\n                np.random.uniform(bounds[0], bounds[1]),\n                0  # Will be updated in next iteration\n            ]\n=======\n        # Intelligent restart when stuck\n        if i % 50 == 0 and i > 0:\n            # Check if we're making progress\n            current_best = min(candidates, key=lambda x: x[2])\n            if abs(current_best[2] - best_value) < 1e-6:\n                # Replace worst candidates with explorative jumps\n                candidates.sort(key=lambda x: x[2])\n                for j in range(2):  # Replace 2 worst candidates\n                    # Jump around elite or random location\n                    if np.random.random() < 0.7:\n                        # Jump around current best\n                        jump_x = best_x + np.random.normal(0, 1.0)\n                        jump_y = best_y + np.random.normal(0, 1.0)\n                    else:\n                        # Completely random restart\n                        jump_x = np.random.uniform(bounds[0], bounds[1])\n                        jump_y = np.random.uniform(bounds[0], bounds[1])\n                    \n                    jump_x = np.clip(jump_x, bounds[0], bounds[1])\n                    jump_y = np.clip(jump_y, bounds[0], bounds[1])\n                    candidates[-(j+1)] = [jump_x, jump_y, evaluate_function(jump_x, jump_y)]\n>>>>>>> REPLACE\n```\n\n## Change 4: Add Elite Preservation and Better Initialization\n\n```python\n<<<<<<< SEARCH\n    # Initialize 5 diverse candidates\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append([x, y, evaluate_function(x, y)])\n    \n    # Sort by value\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n=======\n    # Initialize 8 diverse candidates with Latin Hypercube sampling\n    candidates = []\n    for i in range(8):\n        # Latin Hypercube sampling for better coverage\n        x = bounds[0] + (i + 0.5) * (bounds[1] - bounds[0]) / 8 + np.random.uniform(-0.3, 0.3)\n        y = bounds[0] + np.random.uniform(0, 8) * (bounds[1] - bounds[0]) / 8 + np.random.uniform(-0.3, 0.3)\n        x = np.clip(x, bounds[0], bounds[1])\n        y = np.clip(y, bounds[0], bounds[1])\n        candidates.append([x, y, evaluate_function(x, y)])\n    \n    # Sort by value and preserve elite\n    candidates.sort(key=lambda x: x[2])\n    best_x, best_y, best_value = candidates[0]\n    elite = candidates[0]  # Preserve best solution\n>>>>>>> REPLACE\n```\n\n## Change 5: Final Intensification Phase\n\n```python\n<<<<<<< SEARCH\n    return best_x, best_y, best_value\n=======\n    # Final intensification with gradient-based refinement\n    for refine_iter in range(20):\n        # Mix of random and gradient-directed search\n        if refine_iter < 10:\n            # Random local exploration with decreasing radius\n            current_radius = 0.08 * (1 - refine_iter / 20)\n            int_x = best_x + np.random.normal(0, current_radius)\n            int_y = best_y + np.random.normal(0, current_radius)\n        else:\n            # Gradient-based refinement\n            eps = 0.0005\n            dx = (evaluate_function(best_x + eps, best_y) - evaluate_function(best_x - eps, best_y)) / (2 * eps)\n            dy = (evaluate_function(best_x, best_y + eps) - evaluate_function(best_x, best_y - eps)) / (2 * eps)\n            # Small gradient step with noise\n            int_x = best_x - 0.005 * dx + np.random.normal(0, 0.01)\n            int_y = best_y - 0.005 * dy + np.random.normal(0, 0.01)\n        \n        int_x = np.clip(int_x, bounds[0], bounds[1])\n        int_y = np.clip(int_y, bounds[0], bounds[1])\n        \n        int_value = evaluate_function(int_x, int_y)\n        if int_value < best_value:\n            best_x, best_y, best_value = int_x, int_y, int_value\n    \n    return best_x, best_y, best_value\n>>>>>>> REPLACE\n```\n\nThese improvements address the key weaknesses in the current algorithm:\n\n1. **Better exploration**: Heavy-tailed Cauchy distribution helps escape local minima more effectively than Gaussian alone\n2. **Gradient utilization**: Incorporates gradient information for more directed local search\n3. **Smarter restarts**: Intelligently detects when stuck and performs targeted restarts\n4. **Elite preservation**: Maintains the best solution across restarts\n5. **Enhanced initialization**: Latin Hypercube sampling provides better coverage of the search space\n6. **Final intensification**: Dedicated gradient-based refinement phase for precision\n\nThese changes are inspired by the successful patterns observed in the top-performing programs while maintaining a balance between exploration and exploitation."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7035, y=0.6772 with value=-1.5187\", \"distance_to_global\": \"0.0009\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7028, y=0.6773\", \"average_distance_to_global\": \"0.4003\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}