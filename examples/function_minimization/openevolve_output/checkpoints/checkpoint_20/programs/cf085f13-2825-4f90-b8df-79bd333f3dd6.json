{"id": "cf085f13-2825-4f90-b8df-79bd333f3dd6", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining simulated annealing,\n    local search, and multiple restart strategies.\n    \"\"\"\n    def local_search(x, y, step_size=0.1, local_iter=50):\n        \"\"\"Local gradient-free search around current point\"\"\"\n        best_val = evaluate_function(x, y)\n        best_x, best_y = x, y\n        \n        for _ in range(local_iter):\n            # Try small random steps\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n            new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            if new_val < best_val:\n                best_val = new_val\n                best_x, best_y = new_x, new_y\n                \n        return best_x, best_y, best_val\n    \n    # Multi-start strategy with different initial points\n    num_restarts = 5\n    restart_points = [(np.random.uniform(bounds[0], bounds[1]), \n                      np.random.uniform(bounds[0], bounds[1])) for _ in range(num_restarts)]\n    \n    global_best = float('inf')\n    global_best_x, global_best_y = 0, 0\n    \n    for start_x, start_y in restart_points:\n        x, y = start_x, start_y\n        current_value = evaluate_function(x, y)\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = 0.95\n        \n        temp = initial_temp\n        steps_per_temp = iterations // (num_restarts * 20)\n        \n        for step in range(iterations // num_restarts):\n            # Generate candidate point\n            step_size = temp / initial_temp * (bounds[1] - bounds[0]) / 10\n            candidate_x = np.clip(x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            candidate_y = np.clip(y + np.random.normal(0, step_size), bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y = candidate_x, candidate_y\n                current_value = candidate_value\n            else:\n                # Accept worse solution with probability\n                probability = np.exp(-(candidate_value - current_value) / temp)\n                if np.random.random() < probability:\n                    x, y = candidate_x, candidate_y\n                    current_value = candidate_value\n            \n            # Cool down temperature\n            if step % steps_per_temp == 0:\n                temp = max(final_temp, temp * cooling_rate)\n        \n        # Local refinement around best point found\n        x, y, local_value = local_search(x, y)\n        \n        # Update global best\n        if local_value < global_best:\n            global_best = local_value\n            global_best_x, global_best_y = x, y\n    \n    return global_best_x, global_best_y, global_best\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "8511c1a0-9902-4124-b11c-a07a13f246ea", "generation": 1, "timestamp": 1764998761.7957606, "iteration_found": 4, "metrics": {"runs_successfully": 1.0, "value_score": 0.7684160388739495, "distance_score": 0.20174083270950421, "combined_score": 0.43339676512653713}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 77 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9417587299889709, "distance_score": 0.7565915427363771, "combined_score": 1.2147685971231066}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.2148\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 1.2148\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, value_score: 0.9418, distance_score: 0.7566, combined_score: 1.2148\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.2148)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9418), Performs well on distance_score (0.7566), Performs well on combined_score (1.2148)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program, I can see it's using a simple random search that gets stuck in local minima. The fitness score of 1.2148 suggests there's significant room for improvement. Let me implement a more sophisticated optimization algorithm that can escape local minima.\n\nI'll replace the simple random search with a hybrid approach that combines:\n1. Simulated annealing to escape local minima\n2. Local gradient-based refinement\n3. Adaptive step sizes\n4. Multiple restart points\n\n```python\n<<<<<<< SEARCH\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    A simple random search algorithm that often gets stuck in local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Initialize with a random point\n    best_x = np.random.uniform(bounds[0], bounds[1])\n    best_y = np.random.uniform(bounds[0], bounds[1])\n    best_value = evaluate_function(best_x, best_y)\n\n    for _ in range(iterations):\n        # Simple random search\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n\n        if value < best_value:\n            best_value = value\n            best_x, best_y = x, y\n\n    return best_x, best_y, best_value\n=======\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining simulated annealing,\n    local search, and multiple restart strategies.\n    \"\"\"\n    def local_search(x, y, step_size=0.1, local_iter=50):\n        \"\"\"Local gradient-free search around current point\"\"\"\n        best_val = evaluate_function(x, y)\n        best_x, best_y = x, y\n        \n        for _ in range(local_iter):\n            # Try small random steps\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n            new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n            \n            new_val = evaluate_function(new_x, new_y)\n            if new_val < best_val:\n                best_val = new_val\n                best_x, best_y = new_x, new_y\n                \n        return best_x, best_y, best_val\n    \n    # Multi-start strategy with different initial points\n    num_restarts = 5\n    restart_points = [(np.random.uniform(bounds[0], bounds[1]), \n                      np.random.uniform(bounds[0], bounds[1])) for _ in range(num_restarts)]\n    \n    global_best = float('inf')\n    global_best_x, global_best_y = 0, 0\n    \n    for start_x, start_y in restart_points:\n        x, y = start_x, start_y\n        current_value = evaluate_function(x, y)\n        \n        # Simulated annealing parameters\n        initial_temp = 10.0\n        final_temp = 0.01\n        cooling_rate = 0.95\n        \n        temp = initial_temp\n        steps_per_temp = iterations // (num_restarts * 20)\n        \n        for step in range(iterations // num_restarts):\n            # Generate candidate point\n            step_size = temp / initial_temp * (bounds[1] - bounds[0]) / 10\n            candidate_x = np.clip(x + np.random.normal(0, step_size), bounds[0], bounds[1])\n            candidate_y = np.clip(y + np.random.normal(0, step_size), bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept or reject based on simulated annealing\n            if candidate_value < current_value:\n                x, y = candidate_x, candidate_y\n                current_value = candidate_value\n            else:\n                # Accept worse solution with probability\n                probability = np.exp(-(candidate_value - current_value) / temp)\n                if np.random.random() < probability:\n                    x, y = candidate_x, candidate_y\n                    current_value = candidate_value\n            \n            # Cool down temperature\n            if step % steps_per_temp == 0:\n                temp = max(final_temp, temp * cooling_rate)\n        \n        # Local refinement around best point found\n        x, y, local_value = local_search(x, y)\n        \n        # Update global best\n        if local_value < global_best:\n            global_best = local_value\n            global_best_x, global_best_y = x, y\n    \n    return global_best_x, global_best_y, global_best\n>>>>>>> REPLACE\n```\n\nThis improved algorithm addresses the key issues with the original random search:\n\n1. **Simulated Annealing**: Allows the algorithm to accept worse solutions temporarily, helping escape local minima. The temperature schedule gradually reduces exploration as the search progresses.\n\n2. **Local Refinement**: After the global exploration phase, a local search fine-tunes the solution around promising areas.\n\n3. **Multi-start Strategy**: Runs the optimization from multiple random starting points to ensure better coverage of the search space.\n\n4. **Adaptive Step Sizes**: Step sizes are adjusted based on the current temperature, allowing for broader exploration early on and finer search later.\n\n5. **Temperature-based Acceptance**: The probability of accepting worse solutions decreases over time, balancing exploration and exploitation.\n\nThis hybrid approach should significantly improve the fitness score by finding better minima and avoiding getting trapped in local optima, while maintaining reasonable computational efficiency."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=1.8567, y=2.4037 with value=-1.2176\", \"distance_to_global\": \"3.9569\", \"solution_quality\": \"Could be improved\"}", "artifact_dir": null, "embedding": null}