{"id": "7c594730-5be2-408b-9460-4300de81d45b", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining multiple strategies\n    to escape local minima and find the global minimum.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Compact multi-start initialization\n    candidates = [(np.random.uniform(bounds[0], bounds[1]), \n                   np.random.uniform(bounds[0], bounds[1])) for _ in range(5)]\n    values = [evaluate_function(x, y) for x, y in candidates]\n    \n    # Optimized annealing with faster cooling\n    temperature = 5.0\n    cooling_rate = 0.995\n    \n    # Track best solution efficiently\n    best_idx = np.argmin(values)\n    best_x, best_y = candidates[best_idx]\n    best_value = values[best_idx]\n    \n    # Initialize momentum for each candidate\n    momentums = [np.array([0.0, 0.0]) for _ in range(5)]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n        \n        # Adaptive search strategy with momentum\n        progress = iter / iterations\n        adaptive_step = 0.5 * (1 - progress) + 0.05  # Decreasing step size\n        \n        for i in range(5):\n            x, y = candidates[i]\n            current_val = values[i]\n            \n            # Generate candidate with momentum and adaptive noise\n            noise = np.random.normal(0, adaptive_step, 2)\n            momentums[i] = 0.3 * momentums[i] + 0.7 * noise\n            \n            new_x = np.clip(x + momentums[i][0], bounds[0], bounds[1])\n            new_y = np.clip(y + momentums[i][1], bounds[0], bounds[1])\n            new_val = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with temperature and progress-based boosting\n            delta = new_val - current_val\n            acceptance_prob = np.exp(-delta / (temp * (1 + progress)))\n            \n            if delta < 0 or np.random.random() < acceptance_prob:\n                candidates[i] = (new_x, new_y)\n                values[i] = new_val\n                \n                # Update global best efficiently\n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Strategic diversification with adaptive frequency\n        diversification_freq = max(50, iterations // 20)\n        if iter % diversification_freq == 0 and iter > 0:\n            # Replace worst candidate with perturbed best or random exploration\n            worst_idx = np.argmax(values)\n            if np.random.random() < 0.7:\n                # Perturb best solution\n                perturb_scale = min(1.0, 2.0 * temperature / 5.0)  # Scale with temperature\n                perturb = np.random.normal(0, perturb_scale, 2)\n                candidates[worst_idx] = (\n                    np.clip(best_x + perturb[0], bounds[0], bounds[1]),\n                    np.clip(best_y + perturb[1], bounds[0], bounds[1])\n                )\n            else:\n                # Complete random restart for better exploration\n                candidates[worst_idx] = (\n                    np.random.uniform(bounds[0], bounds[1]),\n                    np.random.uniform(bounds[0], bounds[1])\n                )\n            values[worst_idx] = evaluate_function(*candidates[worst_idx])\n            \n            # Reset momentum for this candidate\n            momentums[worst_idx] = np.array([0.0, 0.0])\n    \n    # Enhanced final local refinement phase\n    refinement_steps = min(150, iterations // 8)\n    temp_final = 0.01  # Very low temperature for final refinement\n    \n    for step in range(refinement_steps):\n        # Adaptive step size that decreases over refinement\n        fine_step = 0.02 * (1 - step/refinement_steps) * (1 - step/refinement_steps)\n        \n        # Multiple perturbation attempts around best solution\n        for attempt in range(3):\n            local_perturb = np.random.normal(0, fine_step, 2)\n            local_x = np.clip(best_x + local_perturb[0], bounds[0], bounds[1])\n            local_y = np.clip(best_y + local_perturb[1], bounds[0], bounds[1])\n            local_val = evaluate_function(local_x, local_y)\n            \n            if local_val < best_value:\n                best_x, best_y, best_value = local_x, local_y, local_val\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "c28cf63d-7570-4128-9803-2433009567fa", "generation": 3, "timestamp": 1765020686.0919945, "iteration_found": 23, "metrics": {"runs_successfully": 0.0, "combined_score": 0.0, "error": "name 'temp' is not defined"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 12 lines with 15 lines\nChange 2: Replace 11 lines with 7 lines\nChange 3: Replace 2 lines with 2 lines\nChange 4: Replace 10 lines with 23 lines\nChange 5: Replace 11 lines with 17 lines", "parent_metrics": {"runs_successfully": 0.0, "combined_score": 0.0, "error": "local variable 'temperature' referenced before assignment"}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 0.0000\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4967 \u2192 0.0000. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### error_type\n```\nUnboundLocalError\n```\n\n### error_message\n```\nStage 1: local variable 'temperature' referenced before assignment\n```\n\n### full_traceback\n```\nTraceback (most recent call last):\n  File \"D:\\Users\\Administrator\\openevolve\\examples/function_minimization/evaluator.py\", line 294, in evaluate_stage1\n    result = run_with_timeout(program.run_search, timeout_seconds=5)\n  File \"D:\\Users\\Administrator\\openevolve\\examples/function_minimization/evaluator.py\", line 30, in run_with_timeout\n    result = future.result(timeout=timeout_seconds)\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 458, in result\n    return self.__get_result()\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"D:\\TEMP\\tmpj25vjhk6.py\", line 104, in run_search\n    x, y, value = search_algorithm()\n  File \"D:\\TEMP\\tmpj25vjhk6.py\", line 34, in search_algorithm\n    temperature *= cooling_rate\nUnboundLocalError: local variable 'temperature' referenced before assignment\n\n```\n\n### suggestion\n```\nUnexpected error occurred. Check the traceback for specific issue\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 56 lines with 76 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9996, distance_score: 0.9934, combined_score: 1.4967, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 6 lines with 19 lines\nChange 2: Replace 11 lines with 24 lines\nChange 3: Replace 3 lines with 7 lines\nChange 4: Replace 2 lines with 9 lines\nChange 5: Replace 2 lines with 20 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9976, combined_score: 1.4987, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 78 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive differential evolution with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Population size\n    pop_size = 20\n    \n    # Initialize population\n    population = np.random.uniform(bounds[0], bounds[1], (pop_size, 2))\n    values = np.array([evaluate_function(x, y) for x, y in population])\n    \n    # Track best solution\n    best_idx = np.argmin(values)\n    best_x, best_y = population[best_idx]\n    best_value = values[best_idx]\n    \n    # Adaptive parameters\n    mutation_factor = 0.8\n    crossover_prob = 0.7\n    \n    for i in range(iterations):\n        # Adaptive parameters based on progress\n        progress = i / iterations\n        mutation_factor = 0.5 + 0.3 * (1 - progress)\n        crossover_prob = 0.5 + 0.2 * (1 - progress)\n        \n        for j in range(pop_size):\n            # Select three random individuals (different from j)\n            candidates = list(range(pop_size))\n            candidates.remove(j)\n            a, b, c = np.random.choice(candidates, 3, replace=False)\n            \n            # Differential mutation\n            mutant = population[a] + mutation_factor * (population[b] - population[c])\n            \n            # Crossover\n            trial = population[j].copy()\n            crossover_mask = np.random.random(2) < crossover_prob\n            trial[crossover_mask] = mutant[crossover_mask]\n            \n            # Ensure bounds\n            trial = np.clip(trial, bounds[0], bounds[1])\n            \n            # Evaluate trial\n            trial_value = evaluate_function(trial[0], trial[1])\n            \n            # Selection\n            if trial_value < values[j]:\n                population[j] = trial\n                values[j] = trial_value\n                \n                # Update global best\n                if trial_value < best_value:\n                    best_x, best_y = trial[0], trial[1]\n                    best_value = trial_value\n        \n        # Periodic local search around best solution\n        if i % 50 == 0 and i > 0:\n            local_step = 0.1 * (1 - progress)\n            for _ in range(10):\n                local_x = best_x + np.random.normal(0, local_step)\n                local_y = best_y + np.random.normal(0, local_step)\n                local_x = np.clip(local_x, bounds[0], bounds[1])\n                local_y = np.clip(local_y, bounds[0], bounds[1])\n                local_value = evaluate_function(local_x, local_y)\n                \n                if local_value < best_value:\n                    best_x, best_y = local_x, local_y\n                    best_value = local_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4987)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Enhanced simulated annealing parameters\n        temp = 2.0  # Higher initial temperature for better exploration\n        initial_temp = temp\n        # Adaptive cooling: faster cooling when temperature is high, slower when low\n        base_cooling = 0.995\n        # Also add occasional reheating to escape deep local minima\n        reheat_frequency = iterations // (num_starts * 4)\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        # Initialize momentum\n        momentum_x, momentum_y = 0, 0\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on iteration progress and temperature\n            progress = i / (iterations // num_starts)\n            temp_ratio = temp  # Temperature affects step size\n            current_step_size = step_size * (1 - progress) * temp_ratio\n            \n            # Generate candidate with momentum and Gaussian perturbation\n            random_dx = np.random.normal(0, current_step_size)\n            random_dy = np.random.normal(0, current_step_size)\n            \n            # Combine momentum with random walk (momentum helps escape local minima)\n            dx = 0.3 * momentum_x + 0.7 * random_dx\n            dy = 0.3 * momentum_y + 0.7 * random_dy\n            \n            candidate_x = x + dx\n            candidate_y = y + dy\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with enhanced temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n                # Update momentum in direction of improvement\n                momentum_x = dx * 0.5\n                momentum_y = dy * 0.5\n            else:\n                # Enhanced acceptance for escaping local minima\n                delta = candidate_value - current_value\n                # Adaptive acceptance based on search progress and temperature\n                progress_factor = 1.0 + (iterations // num_starts - i) / (iterations // num_starts)\n                acceptance_prob = np.exp(-delta / (temp * progress_factor))\n                \n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n                    # Reduce momentum on uphill moves but don't zero it\n                    momentum_x *= 0.3\n                    momentum_y *= 0.3\n                else:\n                    # Decay momentum when move is rejected\n                    momentum_x *= 0.8\n                    momentum_y *= 0.8\n            \n            # Adaptive cooling based on temperature level\n            if temp > initial_temp * 0.5:\n                temp *= base_cooling  # Normal cooling\n            else:\n                temp *= (base_cooling + 0.002)  # Slower cooling at low temperatures\n            \n            # Occasional reheating to escape deep local minima\n            if i % reheat_frequency == 0 and i > 0 and temp < initial_temp * 0.1:\n                temp = min(temp * 2.0, initial_temp * 0.3)\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Get the best result from all starts\n    best_x, best_y, best_value = min(best_results, key=lambda x: x[2])\n    \n    # Final local refinement phase\n    refinement_steps = iterations // 20\n    refinement_step_size = (bounds[1] - bounds[0]) * 0.005\n    \n    for i in range(refinement_steps):\n        # Very small Gaussian perturbations for fine-tuning\n        dx = np.random.normal(0, refinement_step_size * (1 - i/refinement_steps))\n        dy = np.random.normal(0, refinement_step_size * (1 - i/refinement_steps))\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9976), Performs well on combined_score (1.4987), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4967)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 candidates for better coverage (increased from 3)\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters with better cooling schedule\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Track global best for better convergence\n    best_idx = min(range(5), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for i in range(iterations):\n        # Update temperature with exponential cooling\n        temperature *= cooling_rate\n        \n        for j, (x, y, current_value) in enumerate(candidates):\n            # Adaptive step size based on iteration progress\n            if i < iterations // 3:\n                # Early exploration: larger steps\n                step_scale = 2.0 * (1 - i/iterations)\n            else:\n                # Late refinement: smaller steps focused around best\n                step_scale = max(0.1, 0.5 * (1 - i/iterations))\n            \n            # Local perturbation with Gaussian noise\n            new_x = x + np.random.normal(0, step_scale)\n            new_y = y + np.random.normal(0, step_scale)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability if worse (simulated annealing)\n            delta = new_value - current_value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[j] = (new_x, new_y, new_value)\n                \n                # Update global best when we find a better solution\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n        \n        # Strategic long-range exploration: replace worst candidate\n        if i % (iterations // 8) == 0 and i > 0:\n            worst_idx = max(range(5), key=lambda i: candidates[i][2])\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            value = evaluate_function(x, y)\n            candidates[worst_idx] = (x, y, value)\n            \n            # Update global best if needed\n            if value < best_value:\n                best_value = value\n                best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9996), Performs well on distance_score (0.9934), Performs well on combined_score (1.4967), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4910)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining multiple strategies\n    to escape local minima and find the global minimum.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start strategy: maintain multiple candidates\n    num_candidates = 5\n    candidates = []\n    for _ in range(num_candidates):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Adaptive step size\n    step_size = (bounds[1] - bounds[0]) * 0.1\n    min_step_size = (bounds[1] - bounds[0]) * 0.001\n    \n    # Track best solution\n    best_idx = min(range(num_candidates), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n        \n        # Try to improve each candidate\n        for i in range(num_candidates):\n            x, y, value = candidates[i]\n            \n            # Local refinement with adaptive step\n            if iter > iterations // 4:  # Start local refinement after initial exploration\n                dx = np.random.normal(0, step_size)\n                dy = np.random.normal(0, step_size)\n            else:\n                # Global search with larger steps\n                dx = np.random.uniform(-step_size * 3, step_size * 3)\n                dy = np.random.uniform(-step_size * 3, step_size * 3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            delta = new_value - value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[i] = (new_x, new_y, new_value)\n                \n                # Update global best\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n                    # Reduce step size when we find a better solution\n                    step_size = max(step_size * 0.9, min_step_size)\n        \n        # Occasional random restart to escape deep local minima\n        if iter % (iterations // 10) == 0 and iter > 0:\n            worst_idx = max(range(num_candidates), key=lambda i: candidates[i][2])\n            candidates[worst_idx] = (\n                np.random.uniform(bounds[0], bounds[1]),\n                np.random.uniform(bounds[0], bounds[1]),\n                evaluate_function(candidates[worst_idx][0], candidates[worst_idx][1])\n            )\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4962)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4995, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive differential evolution with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Population size\n    pop_size = 20\n    \n    # Initialize population\n    population = np.random.uniform(bounds[0], bounds[1], (pop_size, 2))\n    values = np.array([evaluate_function(x, y) for x, y in population])\n    \n    # Track best solution\n    best_idx = np.argmin(values)\n    best_x, best_y = population[best_idx]\n    best_value = values[best_idx]\n    \n    # Adaptive parameters\n    mutation_factor = 0.8\n    crossover_prob = 0.7\n    \n    for i in range(iterations):\n        # Adaptive parameters based on progress\n        progress = i / iterations\n        mutation_factor = 0.5 + 0.3 * (1 - progress)\n        crossover_prob = 0.5 + 0.2 * (1 - progress)\n        \n        for j in range(pop_size):\n            # Select three random individuals (different from j)\n            candidates = list(range(pop_size))\n            candidates.remove(j)\n            a, b, c = np.random.choice(candidates, 3, replace=False)\n            \n            # Differential mutation\n            mutant = population[a] + mutation_factor * (population[b] - population[c])\n            \n            # Crossover\n            trial = population[j].copy()\n            crossover_mask = np.random.random(2) < crossover_prob\n            trial[crossover_mask] = mutant[crossover_mask]\n            \n            # Ensure bounds\n            trial = np.clip(trial, bounds[0], bounds[1])\n            \n            # Evaluate trial\n            trial_value = evaluate_function(trial[0], trial[1])\n            \n            # Selection\n            if trial_value < values[j]:\n                population[j] = trial\n                values[j] = trial_value\n                \n                # Update global best\n                if trial_value < best_value:\n                    best_x, best_y = trial[0], trial[1]\n                    best_value = trial_value\n        \n        # Periodic local search around best solution\n        if i % 50 == 0 and i > 0:\n            local_step = 0.1 * (1 - progress)\n            for _ in range(10):\n                local_x = best_x + np.random.normal(0, local_step)\n                local_y = best_y + np.random.normal(0, local_step)\n                local_x = np.clip(local_x, bounds[0], bounds[1])\n                local_y = np.clip(local_y, bounds[0], bounds[1])\n                local_value = evaluate_function(local_x, local_y)\n                \n                if local_value < best_value:\n                    best_x, best_y = local_x, local_y\n                    best_value = local_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 78 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 2 (Score: 0.9702, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters with adaptive cooling\n        initial_temp = 2.0\n        final_temp = 0.01\n        temp = initial_temp\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Occasional random restart to escape deep local minima\n            if i > 0 and i % (iterations // (num_starts * 5)) == 0 and np.random.random() < 0.3:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n                current_value = evaluate_function(x, y)\n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            # Generate candidate with adaptive Gaussian perturbations\n            step_scale = step_size * (1 - i / (iterations // num_starts))\n            candidate_x = x + np.random.normal(0, step_scale)\n            candidate_y = y + np.random.normal(0, step_scale)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Adaptive cooling based on progress\n            progress = i / (iterations // num_starts)\n            temp = initial_temp * (1 - progress) + final_temp * progress\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Add local refinement around the best solution found so far\n    if start % 2 == 0 and start > 0:  # Every other restart, do local refinement\n        best_current = min(best_results, key=lambda x: x[2])\n        local_step = 0.05\n        for _ in range(20):\n            local_candidate_x = best_current[0] + np.random.normal(0, local_step)\n            local_candidate_y = best_current[1] + np.random.normal(0, local_step)\n            local_candidate_x = np.clip(local_candidate_x, bounds[0], bounds[1])\n            local_candidate_y = np.clip(local_candidate_y, bounds[0], bounds[1])\n            local_value = evaluate_function(local_candidate_x, local_candidate_y)\n            if local_value < best_current[2]:\n                best_results[-1] = (local_candidate_x, local_candidate_y, local_value)\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.955), Excellent combined_score (0.970)\n\n### Inspiration 3 (Score: 0.5011, Type: Experimental)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing with adaptive scheduling\n        temp = 5.0\n        initial_temp = temp\n        # Adaptive temperature decay based on search progress\n        temp_decay = 0.98 if restart == 0 else 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on both iteration progress and temperature\n            temp_ratio = temp / initial_temp\n            step_scale = max(0.05, 2.0 * (1 - i/restart_iterations) * temp_ratio)\n            \n            # Generate candidate with adaptive search strategy\n            if i < restart_iterations * 0.6:\n                # Global exploration\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            elif i < restart_iterations * 0.9:\n                # Local refinement\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            else:\n                # Very local fine-tuning\n                dx = np.random.normal(0, step_scale * 0.1)\n                dy = np.random.normal(0, step_scale * 0.1)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Final local refinement around the best solution\n    refinement_iterations = iterations // 10\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    for i in range(refinement_iterations):\n        # Very small steps for fine-tuning\n        step_size = 0.01 * (1 - i/refinement_iterations)\n        dx = np.random.normal(0, step_size)\n        dy = np.random.normal(0, step_size)\n        \n        new_x = np.clip(x + dx, bounds[0], bounds[1])\n        new_y = np.clip(y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            x, y, best_value = new_x, new_y, new_value\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.910), Alternative distance_score approach\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining multiple strategies\n    to escape local minima and find the global minimum.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Compact multi-start initialization\n    candidates = [(np.random.uniform(bounds[0], bounds[1]), \n                   np.random.uniform(bounds[0], bounds[1])) for _ in range(5)]\n    values = [evaluate_function(x, y) for x, y in candidates]\n    \n    # Optimized annealing with faster cooling\n    temp = 5.0\n    cooling = 0.995\n    \n    # Track best solution efficiently\n    best_idx = np.argmin(values)\n    best_x, best_y = candidates[best_idx]\n    best_value = values[best_idx]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n        \n        # Adaptive search strategy with momentum\n        progress = iter / iterations\n        adaptive_step = 0.5 * (1 - progress) + 0.05  # Decreasing step size\n        \n        for i in range(5):\n            x, y = candidates[i]\n            current_val = values[i]\n            \n            # Smart candidate generation with momentum\n            if iter == 0:\n                momentum = np.array([0.0, 0.0])\n            \n            # Generate candidate with momentum and adaptive noise\n            noise = np.random.normal(0, adaptive_step, 2)\n            momentum = 0.3 * momentum + 0.7 * noise\n            \n            new_x = np.clip(x + momentum[0], bounds[0], bounds[1])\n            new_y = np.clip(y + momentum[1], bounds[0], bounds[1])\n            new_val = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with temperature and progress-based boosting\n            delta = new_val - current_val\n            acceptance_prob = np.exp(-delta / (temp * (1 + progress)))\n            \n            if delta < 0 or np.random.random() < acceptance_prob:\n                candidates[i] = (new_x, new_y)\n                values[i] = new_val\n                \n                # Update global best efficiently\n                if new_val < best_value:\n                    best_x, best_y, best_value = new_x, new_y, new_val\n        \n        # Strategic diversification every 100 iterations\n        if iter % 100 == 0 and iter > 0:\n            # Replace worst candidate with perturbed best\n            worst_idx = np.argmax(values)\n            perturb = np.random.normal(0, 0.5, 2)\n            candidates[worst_idx] = (\n                np.clip(best_x + perturb[0], bounds[0], bounds[1]),\n                np.clip(best_y + perturb[1], bounds[0], bounds[1])\n            )\n            values[worst_idx] = evaluate_function(*candidates[worst_idx])\n    \n    # Final local refinement phase\n    refinement_steps = min(100, iterations // 10)\n    for _ in range(refinement_steps):\n        fine_step = 0.01 * (1 - _/refinement_steps)\n        local_perturb = np.random.normal(0, fine_step, 2)\n        local_x = np.clip(best_x + local_perturb[0], bounds[0], bounds[1])\n        local_y = np.clip(best_y + local_perturb[1], bounds[0], bounds[1])\n        local_val = evaluate_function(local_x, local_y)\n        \n        if local_val < best_value:\n            best_x, best_y, best_value = local_x, local_y, local_val\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program, I can see there's a critical bug: the variable `temperature` is referenced before assignment in line 34, causing the `UnboundLocalError`. Additionally, there are several other issues that need to be fixed for the algorithm to work properly.\n\nHere are the key improvements needed:\n\n1. **Fix the temperature variable bug** - `temperature` is used before being defined\n2. **Fix momentum variable scope** - `momentum` is only defined when `iter == 0` but used throughout\n3. **Fix cooling rate variable name mismatch** - `cooling_rate` vs `cooling`\n4. **Improve the search strategy** to better escape local minima\n\nLet me implement these fixes:\n\n```python\n<<<<<<< SEARCH\n    # Optimized annealing with faster cooling\n    temp = 5.0\n    cooling = 0.995\n    \n    # Track best solution efficiently\n    best_idx = np.argmin(values)\n    best_x, best_y = candidates[best_idx]\n    best_value = values[best_idx]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n=======\n    # Optimized annealing with faster cooling\n    temperature = 5.0\n    cooling_rate = 0.995\n    \n    # Track best solution efficiently\n    best_idx = np.argmin(values)\n    best_x, best_y = candidates[best_idx]\n    best_value = values[best_idx]\n    \n    # Initialize momentum for each candidate\n    momentums = [np.array([0.0, 0.0]) for _ in range(5)]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n>>>>>>> REPLACE\n```\n\n```python\n<<<<<<< SEARCH\n        for i in range(5):\n            x, y = candidates[i]\n            current_val = values[i]\n            \n            # Smart candidate generation with momentum\n            if iter == 0:\n                momentum = np.array([0.0, 0.0])\n            \n            # Generate candidate with momentum and adaptive noise\n            noise = np.random.normal(0, adaptive_step, 2)\n            momentum = 0.3 * momentum + 0.7 * noise\n=======\n        for i in range(5):\n            x, y = candidates[i]\n            current_val = values[i]\n            \n            # Generate candidate with momentum and adaptive noise\n            noise = np.random.normal(0, adaptive_step, 2)\n            momentums[i] = 0.3 * momentums[i] + 0.7 * noise\n>>>>>>> REPLACE\n```\n\n```python\n<<<<<<< SEARCH\n            new_x = np.clip(x + momentum[0], bounds[0], bounds[1])\n            new_y = np.clip(y + momentum[1], bounds[0], bounds[1])\n=======\n            new_x = np.clip(x + momentums[i][0], bounds[0], bounds[1])\n            new_y = np.clip(y + momentums[i][1], bounds[0], bounds[1])\n>>>>>>> REPLACE\n```\n\n```python\n<<<<<<< SEARCH\n        # Strategic diversification every 100 iterations\n        if iter % 100 == 0 and iter > 0:\n            # Replace worst candidate with perturbed best\n            worst_idx = np.argmax(values)\n            perturb = np.random.normal(0, 0.5, 2)\n            candidates[worst_idx] = (\n                np.clip(best_x + perturb[0], bounds[0], bounds[1]),\n                np.clip(best_y + perturb[1], bounds[0], bounds[1])\n            )\n            values[worst_idx] = evaluate_function(*candidates[worst_idx])\n=======\n        # Strategic diversification with adaptive frequency\n        diversification_freq = max(50, iterations // 20)\n        if iter % diversification_freq == 0 and iter > 0:\n            # Replace worst candidate with perturbed best or random exploration\n            worst_idx = np.argmax(values)\n            if np.random.random() < 0.7:\n                # Perturb best solution\n                perturb_scale = min(1.0, 2.0 * temperature / 5.0)  # Scale with temperature\n                perturb = np.random.normal(0, perturb_scale, 2)\n                candidates[worst_idx] = (\n                    np.clip(best_x + perturb[0], bounds[0], bounds[1]),\n                    np.clip(best_y + perturb[1], bounds[0], bounds[1])\n                )\n            else:\n                # Complete random restart for better exploration\n                candidates[worst_idx] = (\n                    np.random.uniform(bounds[0], bounds[1]),\n                    np.random.uniform(bounds[0], bounds[1])\n                )\n            values[worst_idx] = evaluate_function(*candidates[worst_idx])\n            \n            # Reset momentum for this candidate\n            momentums[worst_idx] = np.array([0.0, 0.0])\n>>>>>>> REPLACE\n```\n\n```python\n<<<<<<< SEARCH\n    # Final local refinement phase\n    refinement_steps = min(100, iterations // 10)\n    for _ in range(refinement_steps):\n        fine_step = 0.01 * (1 - _/refinement_steps)\n        local_perturb = np.random.normal(0, fine_step, 2)\n        local_x = np.clip(best_x + local_perturb[0], bounds[0], bounds[1])\n        local_y = np.clip(best_y + local_perturb[1], bounds[0], bounds[1])\n        local_val = evaluate_function(local_x, local_y)\n        \n        if local_val < best_value:\n            best_x, best_y, best_value = local_x, local_y, local_val\n=======\n    # Enhanced final local refinement phase\n    refinement_steps = min(150, iterations // 8)\n    temp_final = 0.01  # Very low temperature for final refinement\n    \n    for step in range(refinement_steps):\n        # Adaptive step size that decreases over refinement\n        fine_step = 0.02 * (1 - step/refinement_steps) * (1 - step/refinement_steps)\n        \n        # Multiple perturbation attempts around best solution\n        for attempt in range(3):\n            local_perturb = np.random.normal(0, fine_step, 2)\n            local_x = np.clip(best_x + local_perturb[0], bounds[0], bounds[1])\n            local_y = np.clip(best_y + local_perturb[1], bounds[0], bounds[1])\n            local_val = evaluate_function(local_x, local_y)\n            \n            if local_val < best_value:\n                best_x, best_y, best_value = local_x, local_y, local_val\n>>>>>>> REPLACE\n```\n\nThese changes address the critical bugs while improving the algorithm's ability to escape local minima through:\n\n1. **Fixed variable scoping issues** - temperature, momentum, and cooling rate variables are properly initialized and used\n2. **Enhanced diversification strategy** - more intelligent replacement of worst candidates with both perturbed best solutions and random restarts\n3. **Improved momentum handling** - each candidate maintains its own momentum vector\n4. **Better final refinement** - more thorough local search with multiple attempts per step\n5. **Adaptive parameters** - temperature-dependent perturbation scaling and dynamic diversification frequency\n\nThe algorithm should now run without errors and have better exploration capabilities to escape local minima in the complex function landscape."]}}, "artifacts_json": "{\"error_type\": \"NameError\", \"error_message\": \"Stage 1: name 'temp' is not defined\", \"full_traceback\": \"Traceback (most recent call last):\\n  File \\\"D:\\\\Users\\\\Administrator\\\\openevolve\\\\examples/function_minimization/evaluator.py\\\", line 294, in evaluate_stage1\\n    result = run_with_timeout(program.run_search, timeout_seconds=5)\\n  File \\\"D:\\\\Users\\\\Administrator\\\\openevolve\\\\examples/function_minimization/evaluator.py\\\", line 30, in run_with_timeout\\n    result = future.result(timeout=timeout_seconds)\\n  File \\\"C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\concurrent\\\\futures\\\\_base.py\\\", line 451, in result\\n    return self.__get_result()\\n  File \\\"C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\concurrent\\\\futures\\\\_base.py\\\", line 403, in __get_result\\n    raise self._exception\\n  File \\\"C:\\\\Users\\\\Administrator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\concurrent\\\\futures\\\\thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n  File \\\"D:\\\\TEMP\\\\tmph5jc6l_o.py\\\", line 122, in run_search\\n    x, y, value = search_algorithm()\\n  File \\\"D:\\\\TEMP\\\\tmph5jc6l_o.py\\\", line 57, in search_algorithm\\n    acceptance_prob = np.exp(-delta / (temp * (1 + progress)))\\nNameError: name 'temp' is not defined\\n\", \"suggestion\": \"Unexpected error occurred. Check the traceback for specific issue\"}", "artifact_dir": null, "embedding": null}