{"id": "804b5b0f-0531-4dd4-8b90-cd946c821f3c", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Adaptive restart initialization (simplified)\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Gradually shift from exploration to exploitation\n            explore_prob = max(0.2, 1.0 - restart/num_restarts)\n            if np.random.random() < explore_prob:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                perturb_scale = 0.3 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Optimized temperature schedule for this function\n        temp = 2.0 - 0.2 * restart  # More moderate starting temperature\n        temp_decay = 0.94 + 0.04 * (restart / num_restarts)  # Balanced decay rate\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size with gradient-aware adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.5 * (1 - progress ** 0.8))\n            \n            # Increase step size if we're stuck\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.5\n            \n            # Adaptive step scaling based on acceptance ratio (simpler and more efficient)\n            if i > 50 and accept_count < max(1, reject_count // 10):\n                # We're stuck - increase exploration\n                base_scale *= 1.4\n            elif i > restart_iterations * 0.8:\n                # Late refinement phase - reduce step size\n                base_scale *= 0.4\n            \n            step_scale = base_scale\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Efficient final refinement with adaptive pattern search\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Multi-resolution pattern search with adaptive step reduction\n    for step_size in [0.06, 0.03, 0.015]:\n        max_inner_iters = 20 if step_size > 0.03 else 10\n        for _ in range(max_inner_iters):\n            best_improvement = 0\n            best_dx, best_dy = 0, 0\n            \n            # Cross pattern search (more efficient than diamond)\n            for dx, dy in [(-step_size, 0), (step_size, 0), (0, -step_size), (0, step_size)]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                improvement = best_value - new_value\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    best_dx, best_dy = dx, dy\n            \n            if best_improvement > 0:\n                x += best_dx\n                y += best_dy\n                best_value -= best_improvement\n            else:\n                break  # No improvement in this step size\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "8cfb289b-0188-4070-a664-231122e3ea48", "generation": 5, "timestamp": 1765023565.8978279, "iteration_found": 46, "metrics": {"runs_successfully": 1.0, "value_score": 0.9996659710778515, "distance_score": 0.996260108799726, "combined_score": 1.4980665272682652, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 12 lines with 7 lines\nChange 2: Replace 20 lines with 27 lines\nChange 3: Replace 11 lines with 14 lines\nChange 4: Replace 3 lines with 3 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9996736286166608, "distance_score": 0.9966535547797306, "combined_score": 1.4982493211133743, "reliability_score": 1.0}, "island": 1}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.4982\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4990 \u2192 1.4982. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.7034, y=0.6797 with value=-1.5187\n```\n\n### distance_to_global\n```\n0.0019\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.7037, y=0.6761\n```\n\n### average_distance_to_global\n```\n0.0034\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 15 lines with 15 lines\nChange 2: Replace 45 lines with 23 lines\nChange 3: Replace 20 lines with 18 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9982, combined_score: 1.4990, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 3 lines with 5 lines\nChange 2: Replace 2 lines with 9 lines\nChange 3: Replace 12 lines with 20 lines\nChange 4: Replace 6 lines with 14 lines\nChange 5: Replace return best_global_x, best_global_y, best_global_value with 20 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9993, combined_score: 1.4994, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 20 lines with 28 lines\nChange 2: Replace 9 lines with 23 lines\nChange 3: Replace 3 lines with 6 lines\nChange 4: Replace 4 lines with 10 lines\nChange 5: Replace 14 lines with 22 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    # Memory of promising regions (store top candidates)\n    promising_regions = [(best_global_x, best_global_y, best_global_value)]\n    \n    for restart in range(num_restarts):\n        # Smart restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Mix of random restart, perturbation, and promising region exploration\n            rand_val = np.random.random()\n            if restart <= 2 or rand_val < 0.5:\n                # Random restart for exploration\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            elif rand_val < 0.8 and len(promising_regions) > 1:\n                # Explore around promising regions\n                region_idx = np.random.randint(0, len(promising_regions))\n                region_x, region_y, _ = promising_regions[region_idx]\n                perturb_scale = 0.3 / (restart + 1)\n                x = np.clip(region_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(region_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n            else:\n                # Perturbation around best for exploitation\n                perturb_scale = 0.5 / (restart + 1)  # Smaller perturbations in later restarts\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule\n        temp = 2.5 - 0.3 * restart  # Start cooler in later restarts\n        temp_decay = 0.92 + 0.06 * (restart / num_restarts)  # Slower decay in later restarts\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size with momentum-based adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.5 * (1 - progress ** 0.8))\n            \n            # Increase step size if we're stuck (not accepting many moves)\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.5\n            \n            # Adaptive step size based on local gradient\n            if i > 10:\n                # Estimate local gradient magnitude\n                eps = 0.01\n                grad_x = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                grad_y = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n                \n                # Smaller steps in steep regions, larger in flat regions\n                if grad_magnitude > 1.0:\n                    base_scale *= 0.7\n                elif grad_magnitude < 0.1:\n                    base_scale *= 1.3\n            \n            step_scale = base_scale\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n        \n        # Store promising regions for future restarts\n        if best_restart_value < -1.0:  # Threshold for \"good\" solutions\n            promising_regions.append((best_restart_x, best_restart_y, best_restart_value))\n            # Keep only top 5 promising regions\n            promising_regions = sorted(promising_regions, key=lambda x: x[2])[:5]\n    \n    # Final local refinement around best solution\n    final_iterations = max(50, iterations // 20)\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Coordinate descent for more systematic refinement\n    for i in range(final_iterations):\n        step_size = 0.01 * (1 - i/final_iterations)\n        \n        # Alternate between x and y dimensions\n        if i % 2 == 0:\n            # Refine x coordinate\n            for dx in [-step_size, 0, step_size]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, y)\n                if new_value < best_value:\n                    x = new_x\n                    best_value = new_value\n        else:\n            # Refine y coordinate\n            for dy in [-step_size, 0, step_size]:\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(x, new_y)\n                if new_value < best_value:\n                    y = new_y\n                    best_value = new_value\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4994)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Smart restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Mix of random restart and perturbation around best\n            if restart <= 2 or np.random.random() < 0.7:\n                # Random restart for exploration\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Perturbation around best for exploitation\n                perturb_scale = 0.5 / (restart + 1)  # Smaller perturbations in later restarts\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule\n        temp = 2.5 - 0.3 * restart  # Start cooler in later restarts\n        temp_decay = 0.92 + 0.06 * (restart / num_restarts)  # Slower decay in later restarts\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size with momentum-based adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.5 * (1 - progress ** 0.8))\n            \n            # Increase step size if we're stuck (not accepting many moves)\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.5\n            \n            step_scale = base_scale\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Final local refinement around best solution\n    final_iterations = max(50, iterations // 20)\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    for i in range(final_iterations):\n        # Very small steps for fine-tuning\n        step_size = 0.01 * (1 - i/final_iterations)\n        dx = np.random.normal(0, step_size)\n        dy = np.random.normal(0, step_size)\n        \n        new_x = np.clip(x + dx, bounds[0], bounds[1])\n        new_y = np.clip(y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            x, y = new_x, new_y\n            best_value = new_value\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9993), Performs well on combined_score (1.4994), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4990)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Efficient restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Adaptive restart: more exploitation as we converge\n            if np.random.random() < 0.6 + 0.1 * restart:\n                # Perturbation around best for exploitation\n                perturb_scale = 0.4 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n            else:\n                # Random restart for exploration\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule\n        temp = 2.5 - 0.3 * restart  # Start cooler in later restarts\n        temp_decay = 0.92 + 0.06 * (restart / num_restarts)  # Slower decay in later restarts\n        accept_count = 0\n        reject_count = 0\n        \n        accept_count = 0\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on progress\n            progress = i / restart_iterations\n            step_scale = max(0.08, 2.2 * (1 - progress ** 0.9))\n            \n            # Shift to local refinement in later stages\n            if i > restart_iterations * 0.7:\n                step_scale *= 0.3\n            \n            dx = np.random.normal(0, step_scale)\n            dy = np.random.normal(0, step_scale)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Efficient acceptance criteria\n            if new_value < current_value or np.random.random() < np.exp(-(new_value - current_value) / temp):\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Efficient final refinement\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    for step_size in [0.02, 0.01, 0.005]:\n        for _ in range(15):\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < best_value:\n                x, y = new_x, new_y\n                best_value = new_value\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9982), Performs well on combined_score (1.4990), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4990)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Smart restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Adaptive balance of exploration and exploitation\n            explore_ratio = max(0.3, 1.0 - restart/num_restarts)\n            if np.random.random() < explore_ratio:\n                # Random restart for exploration\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                # Perturbation around best for exploitation\n                perturb_scale = 0.4 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule\n        temp = 2.0 - 0.2 * restart  # Start cooler in later restarts\n        temp_decay = 0.96\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.2 * (1 - progress ** 0.9))\n            \n            # Increase step size if stuck, decrease for local refinement\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.3\n            elif progress > 0.7:\n                base_scale *= 0.3  # Local refinement\n            \n            dx = np.random.normal(0, base_scale)\n            dy = np.random.normal(0, base_scale)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Final local refinement around best solution\n    final_iterations = max(50, iterations // 20)\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Efficient pattern search refinement\n    for step_size in [0.02, 0.01, 0.005]:\n        improved = True\n        while improved:\n            improved = False\n            # Search in cross pattern\n            for dx, dy in [(-step_size, 0), (step_size, 0), (0, -step_size), (0, step_size)]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                if new_value < best_value:\n                    x, y = new_x, new_y\n                    best_value = new_value\n                    improved = True\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4986)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Adaptive restart: balance exploration and exploitation\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Gradually shift from exploration to exploitation\n            explore_ratio = max(0.3, 1.0 - restart/num_restarts)\n            if np.random.random() < explore_ratio:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                perturb_scale = 0.4 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule\n        temp = 2.5 - 0.3 * restart  # Start cooler in later restarts\n        temp_decay = 0.92 + 0.06 * (restart / num_restarts)  # Slower decay in later restarts\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size with gradient-aware adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.5 * (1 - progress ** 0.8))\n            \n            # Increase step size if we're stuck\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.5\n            \n            # Gradient-aware step adjustment for complex terrain\n            if i > 20 and i % 10 == 0:\n                eps = 0.01\n                grad_x = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                grad_y = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n                \n                # Smaller steps in steep regions, larger in flat regions\n                if grad_mag > 2.0:\n                    base_scale *= 0.6\n                elif grad_mag < 0.2:\n                    base_scale *= 1.4\n            \n            step_scale = base_scale\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Enhanced final refinement with systematic pattern search\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Multi-scale pattern search for thorough local optimization\n    for step_size in [0.1, 0.05, 0.02, 0.01, 0.005]:\n        improved = True\n        while improved:\n            improved = False\n            # Search in cross pattern and diagonals\n            for dx in [-step_size, 0, step_size]:\n                for dy in [-step_size, 0, step_size]:\n                    if dx == 0 and dy == 0:\n                        continue\n                    new_x = np.clip(x + dx, bounds[0], bounds[1])\n                    new_y = np.clip(y + dy, bounds[0], bounds[1])\n                    new_value = evaluate_function(new_x, new_y)\n                    if new_value < best_value:\n                        x, y = new_x, new_y\n                        best_value = new_value\n                        improved = True\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.4914, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    # Memory of promising regions\n    promising_regions = [(best_global_x, best_global_y, best_global_value)]\n    \n    for restart in range(num_restarts):\n        # Smart restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Mix of random restart, perturbation, and promising region exploration\n            rand_val = np.random.random()\n            if restart <= 2 or rand_val < 0.5:\n                # Random restart for exploration\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            elif rand_val < 0.8 and len(promising_regions) > 1:\n                # Explore around promising regions\n                region_idx = np.random.randint(0, len(promising_regions))\n                region_x, region_y, _ = promising_regions[region_idx]\n                perturb_scale = 0.3 / (restart + 1)\n                x = np.clip(region_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(region_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n            else:\n                # Perturbation around best for exploitation\n                perturb_scale = 0.5 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Optimized temperature schedule\n        temp = max(0.5, 2.0 - 0.3 * restart)\n        temp_decay = 0.95\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size with momentum-based adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.5 * (1 - progress ** 0.8))\n            \n            # Increase step size if we're stuck\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.5\n            \n            # Local vs global search based on progress\n            local_scale = 0.3 if progress > 0.7 else 1.0\n            dx = np.random.normal(0, base_scale * local_scale)\n            dy = np.random.normal(0, base_scale * local_scale)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n        \n        # Store promising regions for future restarts\n        if best_restart_value < -1.0:\n            promising_regions.append((best_restart_x, best_restart_y, best_restart_value))\n            promising_regions = sorted(promising_regions, key=lambda x: x[2])[:5]\n    \n    # Final local refinement around best solution\n    final_iterations = max(50, iterations // 20)\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Coordinate descent for systematic refinement\n    for i in range(final_iterations):\n        step_size = 0.01 * (1 - i/final_iterations)\n        \n        # Alternate between x and y dimensions\n        if i % 2 == 0:\n            # Refine x coordinate\n            for dx in [-step_size, 0, step_size]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, y)\n                if new_value < best_value:\n                    x = new_x\n                    best_value = new_value\n        else:\n            # Refine y coordinate\n            for dy in [-step_size, 0, step_size]:\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(x, new_y)\n                if new_value < best_value:\n                    y = new_y\n                    best_value = new_value\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (0.998), Excellent distance_score (0.984)\n\n### Inspiration 2 (Score: 1.4962, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (1.000), Excellent distance_score (0.992)\n\n### Inspiration 3 (Score: 1.4990, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Efficient restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Adaptive restart: more exploitation as we converge\n            if np.random.random() < 0.6 + 0.1 * restart:\n                # Perturbation around best for exploitation\n                perturb_scale = 0.4 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n            else:\n                # Random restart for exploration\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule\n        temp = 2.5 - 0.3 * restart  # Start cooler in later restarts\n        temp_decay = 0.92 + 0.06 * (restart / num_restarts)  # Slower decay in later restarts\n        accept_count = 0\n        reject_count = 0\n        \n        accept_count = 0\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on progress\n            progress = i / restart_iterations\n            step_scale = max(0.08, 2.2 * (1 - progress ** 0.9))\n            \n            # Shift to local refinement in later stages\n            if i > restart_iterations * 0.7:\n                step_scale *= 0.3\n            \n            dx = np.random.normal(0, step_scale)\n            dy = np.random.normal(0, step_scale)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Efficient acceptance criteria\n            if new_value < current_value or np.random.random() < np.exp(-(new_value - current_value) / temp):\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Efficient final refinement\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    for step_size in [0.02, 0.01, 0.005]:\n        for _ in range(15):\n            dx = np.random.normal(0, step_size)\n            dy = np.random.normal(0, step_size)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < best_value:\n                x, y = new_x, new_y\n                best_value = new_value\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Excellent runs_successfully (1.000), Excellent value_score (1.000), Excellent distance_score (0.998)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Streamlined restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        elif np.random.random() < max(0.3, 1.0 - restart/num_restarts):\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            perturb_scale = 0.4 / (restart + 1)\n            x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n            y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Adaptive temperature schedule with progress-based adjustment\n        temp = 2.2 - 0.25 * restart  # Slightly reduced starting temperature\n        temp_decay = 0.93 + 0.05 * (restart / num_restarts)  # More conservative decay\n        accept_count = 0\n        reject_count = 0\n        \n        for i in range(restart_iterations):\n            # Dynamic step size with gradient-aware adaptation\n            progress = i / restart_iterations\n            base_scale = max(0.05, 2.5 * (1 - progress ** 0.8))\n            \n            # Increase step size if we're stuck\n            if i > 50 and accept_count < reject_count * 0.1:\n                base_scale *= 1.5\n            \n            # Gradient-aware step adjustment for complex terrain (only when stuck)\n            if i > 50 and accept_count < reject_count * 0.05 and i % 15 == 0:\n                eps = 0.01\n                grad_x = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                grad_y = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n                \n                # Smaller steps in steep regions, larger in flat regions\n                if grad_mag > 1.5:\n                    base_scale *= 0.7\n                elif grad_mag < 0.3:\n                    base_scale *= 1.3\n            \n            step_scale = base_scale\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance with adaptive criteria\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n                accept_count += 1\n            else:\n                # Adaptive acceptance for worse solutions\n                delta = new_value - current_value\n                \n                # Increase acceptance probability if we're stuck\n                stuck_factor = max(1.0, (reject_count + 1) / (accept_count + 1))\n                prob = np.exp(-delta / (temp * stuck_factor))\n                \n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    accept_count += 1\n                else:\n                    reject_count += 1\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Enhanced final refinement with systematic pattern search\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Optimized pattern search - more efficient coverage\n    for step_size in [0.08, 0.04, 0.02, 0.01]:\n        improved = True\n        while improved:\n            improved = False\n            # Diamond pattern search (4 directions + center check via improvements)\n            for dx, dy in [(-step_size, 0), (step_size, 0), (0, -step_size), (0, step_size),\n                          (-step_size*0.7, -step_size*0.7), (-step_size*0.7, step_size*0.7),\n                          (step_size*0.7, -step_size*0.7), (step_size*0.7, step_size*0.7)]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                if new_value < best_value:\n                    x, y = new_x, new_y\n                    best_value = new_value\n                    improved = True\n    \n    return x, y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance metrics, I can see that while it has good reliability (100% success rate), there's a slight decline in fitness from 1.4990 to 1.4982. The program is already quite sophisticated with hybrid simulated annealing, but I notice a few areas for improvement:\n\n1. **Code complexity reduction** - The program is quite long and could be simplified\n2. **Gradient computation efficiency** - The gradient calculations are done with finite differences which could be optimized\n3. **Pattern search optimization** - The current diamond pattern could be made more efficient\n4. **Temperature schedule refinement** - Could be better tuned for this specific function\n\nHere are my targeted improvements:\n\n## Change 1: Simplify gradient-aware adaptation with memoization\n\n```python\n<<<<<<< SEARCH\n            # Gradient-aware step adjustment for complex terrain (only when stuck)\n            if i > 50 and accept_count < reject_count * 0.05 and i % 15 == 0:\n                eps = 0.01\n                grad_x = (evaluate_function(x + eps, y) - evaluate_function(x - eps, y)) / (2 * eps)\n                grad_y = (evaluate_function(x, y + eps) - evaluate_function(x, y - eps)) / (2 * eps)\n                grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n                \n                # Smaller steps in steep regions, larger in flat regions\n                if grad_mag > 1.5:\n                    base_scale *= 0.7\n                elif grad_mag < 0.3:\n                    base_scale *= 1.3\n=======\n            # Adaptive step scaling based on acceptance ratio (simpler and more efficient)\n            if i > 50 and accept_count < max(1, reject_count // 10):\n                # We're stuck - increase exploration\n                base_scale *= 1.4\n            elif i > restart_iterations * 0.8:\n                # Late refinement phase - reduce step size\n                base_scale *= 0.4\n>>>>>>> REPLACE\n```\n\n## Change 2: Optimize the pattern search with early termination\n\n```python\n<<<<<<< SEARCH\n    # Enhanced final refinement with systematic pattern search\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Optimized pattern search - more efficient coverage\n    for step_size in [0.08, 0.04, 0.02, 0.01]:\n        improved = True\n        while improved:\n            improved = False\n            # Diamond pattern search (4 directions + center check via improvements)\n            for dx, dy in [(-step_size, 0), (step_size, 0), (0, -step_size), (0, step_size),\n                          (-step_size*0.7, -step_size*0.7), (-step_size*0.7, step_size*0.7),\n                          (step_size*0.7, -step_size*0.7), (step_size*0.7, step_size*0.7)]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                if new_value < best_value:\n                    x, y = new_x, new_y\n                    best_value = new_value\n                    improved = True\n=======\n    # Efficient final refinement with adaptive pattern search\n    x, y = best_global_x, best_global_y\n    best_value = best_global_value\n    \n    # Multi-resolution pattern search with adaptive step reduction\n    for step_size in [0.06, 0.03, 0.015]:\n        max_inner_iters = 20 if step_size > 0.03 else 10\n        for _ in range(max_inner_iters):\n            best_improvement = 0\n            best_dx, best_dy = 0, 0\n            \n            # Cross pattern search (more efficient than diamond)\n            for dx, dy in [(-step_size, 0), (step_size, 0), (0, -step_size), (0, step_size)]:\n                new_x = np.clip(x + dx, bounds[0], bounds[1])\n                new_y = np.clip(y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                improvement = best_value - new_value\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    best_dx, best_dy = dx, dy\n            \n            if best_improvement > 0:\n                x += best_dx\n                y += best_dy\n                best_value -= best_improvement\n            else:\n                break  # No improvement in this step size\n>>>>>>> REPLACE\n```\n\n## Change 3: Simplify restart initialization logic\n\n```python\n<<<<<<< SEARCH\n    for restart in range(num_restarts):\n        # Streamlined restart initialization\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        elif np.random.random() < max(0.3, 1.0 - restart/num_restarts):\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        else:\n            perturb_scale = 0.4 / (restart + 1)\n            x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n            y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n=======\n    for restart in range(num_restarts):\n        # Adaptive restart initialization (simplified)\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            # Gradually shift from exploration to exploitation\n            explore_prob = max(0.2, 1.0 - restart/num_restarts)\n            if np.random.random() < explore_prob:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n            else:\n                perturb_scale = 0.3 / (restart + 1)\n                x = np.clip(best_global_x + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n                y = np.clip(best_global_y + np.random.normal(0, perturb_scale), bounds[0], bounds[1])\n>>>>>>> REPLACE\n```\n\n## Change 4: Optimize temperature schedule\n\n```python\n<<<<<<< SEARCH\n        # Adaptive temperature schedule with progress-based adjustment\n        temp = 2.2 - 0.25 * restart  # Slightly reduced starting temperature\n        temp_decay = 0.93 + 0.05 * (restart / num_restarts)  # More conservative decay\n=======\n        # Optimized temperature schedule for this function\n        temp = 2.0 - 0.2 * restart  # More moderate starting temperature\n        temp_decay = 0.94 + 0.04 * (restart / num_restarts)  # Balanced decay rate\n>>>>>>> REPLACE\n```\n\nThese changes focus on:\n\n1. **Simplification**: Removed complex gradient calculations that require multiple function evaluations, replacing with simpler adaptive mechanisms\n2. **Efficiency**: Optimized pattern search with early termination and better step size selection\n3. **Code length reduction**: Streamlined several sections while maintaining functionality\n4. **Performance**: The changes should improve both the fitness score and reduce computational overhead\n\nThe key insight is that the gradient calculations were expensive and may not be providing enough benefit for this specific function. The simplified acceptance-ratio-based adaptation should be more efficient while still providing good exploration behavior."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7036, y=0.6835 with value=-1.5186\", \"distance_to_global\": \"0.0055\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7067, y=0.6754\", \"average_distance_to_global\": \"0.0038\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}