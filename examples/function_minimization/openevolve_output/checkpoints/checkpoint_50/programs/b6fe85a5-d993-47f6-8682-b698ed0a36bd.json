{"id": "b6fe85a5-d993-47f6-8682-b698ed0a36bd", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters with adaptive cooling\n        initial_temp = 2.0\n        final_temp = 0.01\n        temp = initial_temp\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Occasional random restart to escape deep local minima\n            if i > 0 and i % (iterations // (num_starts * 5)) == 0 and np.random.random() < 0.3:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n                current_value = evaluate_function(x, y)\n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n            # Generate candidate with adaptive Gaussian perturbations\n            step_scale = step_size * (1 - i / (iterations // num_starts))\n            candidate_x = x + np.random.normal(0, step_scale)\n            candidate_y = y + np.random.normal(0, step_scale)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Adaptive cooling based on progress\n            progress = i / (iterations // num_starts)\n            temp = initial_temp * (1 - progress) + final_temp * progress\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Add local refinement around the best solution found so far\n    if start % 2 == 0 and start > 0:  # Every other restart, do local refinement\n        best_current = min(best_results, key=lambda x: x[2])\n        local_step = 0.05\n        for _ in range(20):\n            local_candidate_x = best_current[0] + np.random.normal(0, local_step)\n            local_candidate_y = best_current[1] + np.random.normal(0, local_step)\n            local_candidate_x = np.clip(local_candidate_x, bounds[0], bounds[1])\n            local_candidate_y = np.clip(local_candidate_y, bounds[0], bounds[1])\n            local_value = evaluate_function(local_candidate_x, local_candidate_y)\n            if local_value < best_current[2]:\n                best_results[-1] = (local_candidate_x, local_candidate_y, local_value)\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "3b46b243-4e8e-4afb-8d4f-6fdfe0960663", "generation": 2, "timestamp": 1765020355.1692784, "iteration_found": 13, "metrics": {"runs_successfully": 1.0, "value_score": 0.9548069124896942, "distance_score": 0.436865338069858, "combined_score": 0.9701556691989653, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 4 lines\nChange 2: Replace 3 lines with 4 lines\nChange 3: Replace 2 lines with 3 lines\nChange 4: Replace 4 lines with 17 lines\nChange 5: Replace for i in range(iterations // num_starts): with 8 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9823971624301019, "distance_score": 0.6829181492498293, "combined_score": 1.3441110389849995, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.3441\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4910 \u2192 1.3441. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.7296, y=0.6643 with value=-1.5181\n```\n\n### distance_to_global\n```\n0.0290\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.5875, y=0.6857\n```\n\n### average_distance_to_global\n```\n0.4643\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 27 lines with 77 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9993, distance_score: 0.9813, combined_score: 1.4910, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 80 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9996, distance_score: 0.9922, combined_score: 1.4962, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 27 lines with 78 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive differential evolution with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Population size\n    pop_size = 20\n    \n    # Initialize population\n    population = np.random.uniform(bounds[0], bounds[1], (pop_size, 2))\n    values = np.array([evaluate_function(x, y) for x, y in population])\n    \n    # Track best solution\n    best_idx = np.argmin(values)\n    best_x, best_y = population[best_idx]\n    best_value = values[best_idx]\n    \n    # Adaptive parameters\n    mutation_factor = 0.8\n    crossover_prob = 0.7\n    \n    for i in range(iterations):\n        # Adaptive parameters based on progress\n        progress = i / iterations\n        mutation_factor = 0.5 + 0.3 * (1 - progress)\n        crossover_prob = 0.5 + 0.2 * (1 - progress)\n        \n        for j in range(pop_size):\n            # Select three random individuals (different from j)\n            candidates = list(range(pop_size))\n            candidates.remove(j)\n            a, b, c = np.random.choice(candidates, 3, replace=False)\n            \n            # Differential mutation\n            mutant = population[a] + mutation_factor * (population[b] - population[c])\n            \n            # Crossover\n            trial = population[j].copy()\n            crossover_mask = np.random.random(2) < crossover_prob\n            trial[crossover_mask] = mutant[crossover_mask]\n            \n            # Ensure bounds\n            trial = np.clip(trial, bounds[0], bounds[1])\n            \n            # Evaluate trial\n            trial_value = evaluate_function(trial[0], trial[1])\n            \n            # Selection\n            if trial_value < values[j]:\n                population[j] = trial\n                values[j] = trial_value\n                \n                # Update global best\n                if trial_value < best_value:\n                    best_x, best_y = trial[0], trial[1]\n                    best_value = trial_value\n        \n        # Periodic local search around best solution\n        if i % 50 == 0 and i > 0:\n            local_step = 0.1 * (1 - progress)\n            for _ in range(10):\n                local_x = best_x + np.random.normal(0, local_step)\n                local_y = best_y + np.random.normal(0, local_step)\n                local_x = np.clip(local_x, bounds[0], bounds[1])\n                local_y = np.clip(local_y, bounds[0], bounds[1])\n                local_value = evaluate_function(local_x, local_y)\n                \n                if local_value < best_value:\n                    best_x, best_y = local_x, local_y\n                    best_value = local_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4962)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9996), Performs well on distance_score (0.9922), Performs well on combined_score (1.4962), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4910)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid optimization algorithm combining multiple strategies\n    to escape local minima and find the global minimum.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start strategy: maintain multiple candidates\n    num_candidates = 5\n    candidates = []\n    for _ in range(num_candidates):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        candidates.append((x, y, evaluate_function(x, y)))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Adaptive step size\n    step_size = (bounds[1] - bounds[0]) * 0.1\n    min_step_size = (bounds[1] - bounds[0]) * 0.001\n    \n    # Track best solution\n    best_idx = min(range(num_candidates), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for iter in range(iterations):\n        # Update temperature\n        temperature *= cooling_rate\n        \n        # Try to improve each candidate\n        for i in range(num_candidates):\n            x, y, value = candidates[i]\n            \n            # Local refinement with adaptive step\n            if iter > iterations // 4:  # Start local refinement after initial exploration\n                dx = np.random.normal(0, step_size)\n                dy = np.random.normal(0, step_size)\n            else:\n                # Global search with larger steps\n                dx = np.random.uniform(-step_size * 3, step_size * 3)\n                dy = np.random.uniform(-step_size * 3, step_size * 3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            delta = new_value - value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[i] = (new_x, new_y, new_value)\n                \n                # Update global best\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n                    # Reduce step size when we find a better solution\n                    step_size = max(step_size * 0.9, min_step_size)\n        \n        # Occasional random restart to escape deep local minima\n        if iter % (iterations // 10) == 0 and iter > 0:\n            worst_idx = max(range(num_candidates), key=lambda i: candidates[i][2])\n            candidates[worst_idx] = (\n                np.random.uniform(bounds[0], bounds[1]),\n                np.random.uniform(bounds[0], bounds[1]),\n                evaluate_function(candidates[worst_idx][0], candidates[worst_idx][1])\n            )\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9993), Performs well on distance_score (0.9813), Performs well on combined_score (1.4910), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.3441)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * (1 - i / (iterations // num_starts))  # Decrease step size\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down temperature\n            temp *= cooling_rate\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.2721)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 3 candidates for better coverage\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    cooling_rate = 0.995\n    \n    for i in range(iterations):\n        temp = initial_temp * (cooling_rate ** i)\n        \n        for j, (x, y, current_value) in enumerate(candidates):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/iterations))\n            \n            # Local perturbation with Gaussian noise\n            new_x = x + np.random.normal(0, step_scale)\n            new_y = y + np.random.normal(0, step_scale)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability if worse (simulated annealing)\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                candidates[j] = (new_x, new_y, new_value)\n        \n        # Occasional long-range exploration\n        if i % 50 == 0 and i > 0:\n            idx = np.random.randint(0, 3)\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            value = evaluate_function(x, y)\n            candidates[idx] = (x, y, value)\n    \n    # Return the best candidate found\n    best_candidate = min(candidates, key=lambda x: x[2])\n    return best_candidate\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 1.2721, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 3 candidates for better coverage\n    candidates = []\n    for _ in range(3):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters\n    initial_temp = 10.0\n    cooling_rate = 0.995\n    \n    for i in range(iterations):\n        temp = initial_temp * (cooling_rate ** i)\n        \n        for j, (x, y, current_value) in enumerate(candidates):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/iterations))\n            \n            # Local perturbation with Gaussian noise\n            new_x = x + np.random.normal(0, step_scale)\n            new_y = y + np.random.normal(0, step_scale)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability if worse (simulated annealing)\n            delta = new_value - current_value\n            if delta < 0 or np.random.random() < np.exp(-delta / temp):\n                candidates[j] = (new_x, new_y, new_value)\n        \n        # Occasional long-range exploration\n        if i % 50 == 0 and i > 0:\n            idx = np.random.randint(0, 3)\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            value = evaluate_function(x, y)\n            candidates[idx] = (x, y, value)\n    \n    # Return the best candidate found\n    best_candidate = min(candidates, key=lambda x: x[2])\n    return best_candidate\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 56 lines, Excellent runs_successfully (1.000), Excellent value_score (0.953)\n\n### Inspiration 2 (Score: 1.4995, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive differential evolution with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Population size\n    pop_size = 20\n    \n    # Initialize population\n    population = np.random.uniform(bounds[0], bounds[1], (pop_size, 2))\n    values = np.array([evaluate_function(x, y) for x, y in population])\n    \n    # Track best solution\n    best_idx = np.argmin(values)\n    best_x, best_y = population[best_idx]\n    best_value = values[best_idx]\n    \n    # Adaptive parameters\n    mutation_factor = 0.8\n    crossover_prob = 0.7\n    \n    for i in range(iterations):\n        # Adaptive parameters based on progress\n        progress = i / iterations\n        mutation_factor = 0.5 + 0.3 * (1 - progress)\n        crossover_prob = 0.5 + 0.2 * (1 - progress)\n        \n        for j in range(pop_size):\n            # Select three random individuals (different from j)\n            candidates = list(range(pop_size))\n            candidates.remove(j)\n            a, b, c = np.random.choice(candidates, 3, replace=False)\n            \n            # Differential mutation\n            mutant = population[a] + mutation_factor * (population[b] - population[c])\n            \n            # Crossover\n            trial = population[j].copy()\n            crossover_mask = np.random.random(2) < crossover_prob\n            trial[crossover_mask] = mutant[crossover_mask]\n            \n            # Ensure bounds\n            trial = np.clip(trial, bounds[0], bounds[1])\n            \n            # Evaluate trial\n            trial_value = evaluate_function(trial[0], trial[1])\n            \n            # Selection\n            if trial_value < values[j]:\n                population[j] = trial\n                values[j] = trial_value\n                \n                # Update global best\n                if trial_value < best_value:\n                    best_x, best_y = trial[0], trial[1]\n                    best_value = trial_value\n        \n        # Periodic local search around best solution\n        if i % 50 == 0 and i > 0:\n            local_step = 0.1 * (1 - progress)\n            for _ in range(10):\n                local_x = best_x + np.random.normal(0, local_step)\n                local_y = best_y + np.random.normal(0, local_step)\n                local_x = np.clip(local_x, bounds[0], bounds[1])\n                local_y = np.clip(local_y, bounds[0], bounds[1])\n                local_value = evaluate_function(local_x, local_y)\n                \n                if local_value < best_value:\n                    best_x, best_y = local_x, local_y\n                    best_value = local_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 78 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 3 (Score: 1.4962, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 80 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * (1 - i / (iterations // num_starts))  # Decrease step size\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down temperature\n            temp *= cooling_rate\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance data, I can see that the fitness score has declined from 1.4910 to 1.3441. The current algorithm uses a multi-start simulated annealing approach with radial step generation, but there are several areas where we can improve to escape local minima more effectively and find better solutions.\n\nHere are my suggested improvements:\n\n## 1. Replace Radial Step Generation with Gaussian Perturbations\n\nThe current radial step generation (using angle and cosine/sine) is limiting exploration to circular patterns. Gaussian perturbations provide more flexible exploration:\n\n```python\n<<<<<<< SEARCH\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * (1 - i / (iterations // num_starts))  # Decrease step size\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n=======\n            # Generate candidate with adaptive Gaussian perturbations\n            step_scale = step_size * (1 - i / (iterations // num_starts))\n            candidate_x = x + np.random.normal(0, step_scale)\n            candidate_y = y + np.random.normal(0, step_scale)\n>>>>>>> REPLACE\n```\n\n## 2. Add Adaptive Temperature Schedule Based on Search Progress\n\nThe current fixed cooling rate doesn't adapt to the search progress. Let's make it adaptive:\n\n```python\n<<<<<<< SEARCH\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n=======\n        # Simulated annealing parameters with adaptive cooling\n        initial_temp = 2.0\n        final_temp = 0.01\n        temp = initial_temp\n>>>>>>> REPLACE\n```\n\n```python\n<<<<<<< SEARCH\n            # Cool down temperature\n            temp *= cooling_rate\n=======\n            # Adaptive cooling based on progress\n            progress = i / (iterations // num_starts)\n            temp = initial_temp * (1 - progress) + final_temp * progress\n>>>>>>> REPLACE\n```\n\n## 3. Add Periodic Local Refinement Around Best Solution\n\nThe algorithm currently only does global exploration. Adding periodic local refinement will help fine-tune promising solutions:\n\n```python\n<<<<<<< SEARCH\n        best_results.append((best_x, best_y, best_value))\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n=======\n        best_results.append((best_x, best_y, best_value))\n    \n    # Add local refinement around the best solution found so far\n    if start % 2 == 0 and start > 0:  # Every other restart, do local refinement\n        best_current = min(best_results, key=lambda x: x[2])\n        local_step = 0.05\n        for _ in range(20):\n            local_candidate_x = best_current[0] + np.random.normal(0, local_step)\n            local_candidate_y = best_current[1] + np.random.normal(0, local_step)\n            local_candidate_x = np.clip(local_candidate_x, bounds[0], bounds[1])\n            local_candidate_y = np.clip(local_candidate_y, bounds[0], bounds[1])\n            local_value = evaluate_function(local_candidate_x, local_candidate_y)\n            if local_value < best_current[2]:\n                best_results[-1] = (local_candidate_x, local_candidate_y, local_value)\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n>>>>>>> REPLACE\n```\n\n## 4. Add Occasional Random Restarts for Better Exploration\n\nTo escape deep local minima, we should occasionally do random restarts within each search trajectory:\n\n```python\n<<<<<<< SEARCH\n        for i in range(iterations // num_starts):\n=======\n        for i in range(iterations // num_starts):\n            # Occasional random restart to escape deep local minima\n            if i > 0 and i % (iterations // (num_starts * 5)) == 0 and np.random.random() < 0.3:\n                x = np.random.uniform(bounds[0], bounds[1])\n                y = np.random.uniform(bounds[0], bounds[1])\n                current_value = evaluate_function(x, y)\n                if current_value < best_value:\n                    best_x, best_y, best_value = x, y, current_value\n>>>>>>> REPLACE\n```\n\n## Reasoning for These Changes:\n\n1. **Gaussian perturbations** provide more natural exploration that isn't constrained to circular patterns, allowing better navigation of the complex landscape.\n\n2. **Adaptive temperature schedule** ensures we start with high exploration and gradually shift to exploitation, but maintain some ability to escape local minima throughout the search.\n\n3. **Local refinement** helps fine-tune promising solutions once we've found good regions, which is especially important for the complex function we're optimizing.\n\n4. **Random restarts** within trajectories provide additional escape mechanisms from deep local minima that might trap individual search paths.\n\nThese changes maintain the multi-start strategy while adding more sophisticated exploration and exploitation mechanisms that should help find better solutions more reliably, bringing the fitness score back up toward the 1.49+ range we saw in the top performers."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.8233, y=0.6540 with value=-1.5102\", \"distance_to_global\": \"0.1216\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=1.6203, y=2.8876\", \"average_distance_to_global\": \"1.2890\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}