{"id": "54a1f452-8415-4bff-b3e5-2fab1c805244", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initialize momentum for better escape from local minima\n        momentum_x, momentum_y = 0, 0\n        \n        # Enhanced temperature schedule with reheating\n        temp = 10.0  # Higher initial temperature for better exploration\n        initial_temp = temp\n        cooling_rate = 0.995  # Slower cooling\n        stagnation_counter = 0\n        last_best = float('inf')\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with momentum and adaptive noise\n            random_dx = np.random.normal(0, step_scale)\n            random_dy = np.random.normal(0, step_scale)\n            \n            # Combine momentum with random walk for better exploration\n            dx = 0.3 * momentum_x + 0.7 * random_dx\n            dy = 0.3 * momentum_y + 0.7 * random_dy\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance criteria with progress awareness\n            if new_value < current_value:\n                # Always accept better solutions and update momentum\n                x, y = new_x, new_y\n                current_value = new_value\n                # Update momentum in direction of improvement\n                momentum_x = dx * 0.5\n                momentum_y = dy * 0.5\n            else:\n                # Enhanced acceptance for escaping local minima\n                delta = new_value - current_value\n                # Adaptive acceptance based on search progress\n                progress_factor = 1.0 + (restart_iterations - i) / restart_iterations\n                acceptance_prob = np.exp(-delta / (temp * progress_factor))\n                \n                if np.random.random() < acceptance_prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    # Reduce momentum on uphill moves but don't zero it\n                    momentum_x *= 0.3\n                    momentum_y *= 0.3\n                else:\n                    # Decay momentum when move is rejected\n                    momentum_x *= 0.8\n                    momentum_y *= 0.8\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n                stagnation_counter = 0  # Reset stagnation counter\n            else:\n                stagnation_counter += 1\n            \n            # Intelligent temperature management\n            if stagnation_counter > restart_iterations // 10 and temp < initial_temp * 0.1:\n                # Reheat when stagnated\n                temp = min(temp * 2.0, initial_temp * 0.3)\n                stagnation_counter = 0\n            else:\n                # Adaptive cooling based on temperature level\n                if temp > initial_temp * 0.5:\n                    temp *= cooling_rate  # Normal cooling\n                else:\n                    temp *= (cooling_rate + 0.002)  # Slower cooling at low temperatures\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    # Multi-scale refinement phase for better precision\n    refinement_budget = min(100, iterations // 10)\n    scales = [0.1, 0.03, 0.01, 0.003]\n    \n    for scale_idx, base_scale in enumerate(scales):\n        scale_iterations = refinement_budget // len(scales)\n        \n        for i in range(scale_iterations):\n            # Adaptive step size with decay\n            progress = i / scale_iterations\n            step_size = base_scale * (1 - progress * 0.7)\n            \n            # Generate multiple candidates in small neighborhood\n            for attempt in range(3):\n                if attempt < 2:\n                    # Gaussian perturbation\n                    dx = np.random.normal(0, step_size)\n                    dy = np.random.normal(0, step_size)\n                else:\n                    # Uniform perturbation for diversity\n                    dx = np.random.uniform(-step_size, step_size)\n                    dy = np.random.uniform(-step_size, step_size)\n                \n                new_x = np.clip(best_global_x + dx, bounds[0], bounds[1])\n                new_y = np.clip(best_global_y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                \n                if new_value < best_global_value:\n                    best_global_value = new_value\n                    best_global_x, best_global_y = new_x, new_y\n                    # Found improvement, can be more aggressive\n                    break\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n", "language": "python", "parent_id": "542fc262-1df9-4fa8-8838-52cc92c0146f", "generation": 2, "timestamp": 1765023696.606117, "iteration_found": 49, "metrics": {"runs_successfully": 1.0, "value_score": 0.9899688346690031, "distance_score": 0.7145502561864305, "combined_score": 1.3640242412856463, "reliability_score": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 6 lines\nChange 2: Replace 3 lines with 6 lines\nChange 3: Replace 9 lines with 7 lines\nChange 4: Replace 12 lines with 25 lines\nChange 5: Replace 7 lines with 19 lines\nChange 6: Replace return best_global_x, best_global_y, best_global_value with 34 lines", "parent_metrics": {"runs_successfully": 1.0, "value_score": 0.9996084538159868, "distance_score": 0.9921575100905551, "combined_score": 1.4961772199027399, "reliability_score": 1.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.", "user": "# Current Program Information\n- Fitness: 1.4962\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 1.4994 \u2192 1.4962. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### stage1_result\n```\nFound solution at x=-1.6990, y=0.6800 with value=-1.5187\n```\n\n### distance_to_global\n```\n0.0054\n```\n\n### solution_quality\n```\nDistance < 0.5: Very close\n```\n\n### convergence_info\n```\nConverged in 10 trials with 10 successes\n```\n\n### best_position\n```\nFinal position: x=-1.7136, y=0.6869\n```\n\n### average_distance_to_global\n```\n0.0079\n```\n\n### search_efficiency\n```\nSuccess rate: 100.00%\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 76 lines with 57 lines\nChange 2: Replace return best_x, best_y, best_value with 14 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9992, combined_score: 1.4994, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 27 lines with 78 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 8 lines with 12 lines\nChange 2: Replace 5 lines with 13 lines\nChange 3: Replace 12 lines with 34 lines\nChange 4: Replace 7 lines with 17 lines\nChange 5: Replace 3 lines with 6 lines\n- Metrics: runs_successfully: 1.0000, value_score: 0.9997, distance_score: 0.9995, combined_score: 1.4995, reliability_score: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Efficient hybrid optimization with adaptive cooling and strategic exploration.\n    \"\"\"\n    # Initialize 5 candidates and find initial best\n    candidates = [(np.random.uniform(*bounds), np.random.uniform(*bounds)) for _ in range(5)]\n    values = [evaluate_function(x, y) for x, y in candidates]\n    \n    best_idx = np.argmin(values)\n    best_x, best_y = candidates[best_idx]\n    best_value = values[best_idx]\n    \n    # Enhanced temperature schedule with reheating\n    temp = 10.0\n    initial_temp = temp\n    cooling_rate = 0.002  # Slower cooling for better exploration\n    stagnation_counter = 0\n    last_best = float('inf')\n    \n    for i in range(iterations):\n        # Progress-based step scaling with temperature influence\n        progress = i / iterations\n        temp_ratio = temp / initial_temp\n        step_scale = max(0.05, 1.5 * (1 - progress) * temp_ratio)\n        \n        for j in range(5):\n            x, y = candidates[j]\n            current_value = values[j]\n            \n            # Generate candidate with adaptive noise and momentum\n            # Add small momentum component for more efficient search\n            momentum_factor = 0.2 if i > iterations // 10 else 0.0\n            \n            if i > 0 and hasattr(search_algorithm, '_last_dx'):\n                dx = np.random.normal(0, step_scale) + momentum_factor * search_algorithm._last_dx\n                dy = np.random.normal(0, step_scale) + momentum_factor * search_algorithm._last_dy\n            else:\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            \n            new_x = np.clip(x + dx, *bounds)\n            new_y = np.clip(y + dy, *bounds)\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance criteria\n            delta = new_value - current_value\n            if delta < 0 or (temp > 0.5 and np.random.random() < np.exp(-delta / temp)):\n                # Store momentum for next iteration\n                search_algorithm._last_dx = dx\n                search_algorithm._last_dy = dy\n                candidates[j] = (new_x, new_y)\n                values[j] = new_value\n                \n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n                    stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    stagnation_counter += 1\n        \n        # Intelligent reheating when stagnated\n        if stagnation_counter > iterations // 20 and temp < initial_temp * 0.3:\n            temp = min(temp * 3.0, initial_temp * 0.8)\n            stagnation_counter = 0\n        \n        # Strategic exploration: replace worst with probability\n        if i % max(1, iterations // 10) == 0 and i > 0:\n            if np.random.random() < 0.3:  # 30% chance to explore\n                worst_idx = np.argmax(values)\n                candidates[worst_idx] = (np.random.uniform(*bounds), np.random.uniform(*bounds))\n                values[worst_idx] = evaluate_function(*candidates[worst_idx])\n                \n                if values[worst_idx] < best_value:\n                    best_value = values[worst_idx]\n                    best_x, best_y = candidates[worst_idx]\n        \n        # Adaptive cooling with restart protection\n        temp *= (1 - cooling_rate)\n        if temp < 0.1 and i < iterations * 0.8:  # Prevent premature convergence\n            temp = 0.5\n    \n    # Enhanced multi-scale refinement with adaptive search\n    refinement_budget = min(100, iterations // 10)\n    \n    # Multi-scale approach: coarse to fine\n    scales = [0.1, 0.03, 0.01, 0.003]\n    for scale_idx, base_scale in enumerate(scales):\n        scale_iterations = refinement_budget // len(scales)\n        \n        for i in range(scale_iterations):\n            # Adaptive step size with decay\n            progress = i / scale_iterations\n            step_size = base_scale * (1 - progress * 0.7)\n            \n            # Generate multiple candidates in a small neighborhood\n            for attempt in range(5):\n                # Use both Gaussian and uniform perturbations\n                if attempt < 3:\n                    # Gaussian perturbation\n                    dx = np.random.normal(0, step_size)\n                    dy = np.random.normal(0, step_size)\n                else:\n                    # Uniform perturbation for diversity\n                    dx = np.random.uniform(-step_size, step_size)\n                    dy = np.random.uniform(-step_size, step_size)\n                \n                new_x = np.clip(best_x + dx, *bounds)\n                new_y = np.clip(best_y + dy, *bounds)\n                new_value = evaluate_function(new_x, new_y)\n                \n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n                    # Found improvement, can be more aggressive\n                    break\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 2 (Score: 1.4995)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Adaptive differential evolution with local search hybrid.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n        \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Population size\n    pop_size = 20\n    \n    # Initialize population\n    population = np.random.uniform(bounds[0], bounds[1], (pop_size, 2))\n    values = np.array([evaluate_function(x, y) for x, y in population])\n    \n    # Track best solution\n    best_idx = np.argmin(values)\n    best_x, best_y = population[best_idx]\n    best_value = values[best_idx]\n    \n    # Adaptive parameters\n    mutation_factor = 0.8\n    crossover_prob = 0.7\n    \n    for i in range(iterations):\n        # Adaptive parameters based on progress\n        progress = i / iterations\n        mutation_factor = 0.5 + 0.3 * (1 - progress)\n        crossover_prob = 0.5 + 0.2 * (1 - progress)\n        \n        for j in range(pop_size):\n            # Select three random individuals (different from j)\n            candidates = list(range(pop_size))\n            candidates.remove(j)\n            a, b, c = np.random.choice(candidates, 3, replace=False)\n            \n            # Differential mutation\n            mutant = population[a] + mutation_factor * (population[b] - population[c])\n            \n            # Crossover\n            trial = population[j].copy()\n            crossover_mask = np.random.random(2) < crossover_prob\n            trial[crossover_mask] = mutant[crossover_mask]\n            \n            # Ensure bounds\n            trial = np.clip(trial, bounds[0], bounds[1])\n            \n            # Evaluate trial\n            trial_value = evaluate_function(trial[0], trial[1])\n            \n            # Selection\n            if trial_value < values[j]:\n                population[j] = trial\n                values[j] = trial_value\n                \n                # Update global best\n                if trial_value < best_value:\n                    best_x, best_y = trial[0], trial[1]\n                    best_value = trial_value\n        \n        # Periodic local search around best solution\n        if i % 50 == 0 and i > 0:\n            local_step = 0.1 * (1 - progress)\n            for _ in range(10):\n                local_x = best_x + np.random.normal(0, local_step)\n                local_y = best_y + np.random.normal(0, local_step)\n                local_x = np.clip(local_x, bounds[0], bounds[1])\n                local_y = np.clip(local_y, bounds[0], bounds[1])\n                local_value = evaluate_function(local_x, local_y)\n                \n                if local_value < best_value:\n                    best_x, best_y = local_x, local_y\n                    best_value = local_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9995), Performs well on combined_score (1.4995), Performs well on reliability_score (1.0000)\n\n### Program 3 (Score: 1.4994)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Efficient hybrid optimization with adaptive cooling and strategic exploration.\n    \"\"\"\n    # Initialize 5 candidates and find initial best\n    candidates = [(np.random.uniform(*bounds), np.random.uniform(*bounds)) for _ in range(5)]\n    values = [evaluate_function(x, y) for x, y in candidates]\n    \n    best_idx = np.argmin(values)\n    best_x, best_y = candidates[best_idx]\n    best_value = values[best_idx]\n    \n    # Adaptive temperature schedule\n    temp = 10.0\n    cooling_rate = 0.003  # More aggressive cooling\n    \n    for i in range(iterations):\n        # Progress-based step scaling\n        progress = i / iterations\n        step_scale = max(0.05, 1.5 * (1 - progress) * (temp / 10.0))\n        \n        for j in range(5):\n            x, y = candidates[j]\n            current_value = values[j]\n            \n            # Generate candidate with adaptive noise\n            new_x = np.clip(x + np.random.normal(0, step_scale), *bounds)\n            new_y = np.clip(y + np.random.normal(0, step_scale), *bounds)\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Enhanced acceptance criteria\n            delta = new_value - current_value\n            if delta < 0 or (temp > 0.5 and np.random.random() < np.exp(-delta / temp)):\n                candidates[j] = (new_x, new_y)\n                values[j] = new_value\n                \n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n        \n        # Strategic exploration: replace worst with probability\n        if i % max(1, iterations // 10) == 0 and i > 0:\n            if np.random.random() < 0.3:  # 30% chance to explore\n                worst_idx = np.argmax(values)\n                candidates[worst_idx] = (np.random.uniform(*bounds), np.random.uniform(*bounds))\n                values[worst_idx] = evaluate_function(*candidates[worst_idx])\n                \n                if values[worst_idx] < best_value:\n                    best_value = values[worst_idx]\n                    best_x, best_y = candidates[worst_idx]\n        \n        # Adaptive cooling with restart protection\n        temp *= (1 - cooling_rate)\n        if temp < 0.1 and i < iterations * 0.8:  # Prevent premature convergence\n            temp = 0.5\n    \n    # Final focused refinement around best solution\n    refinement_steps = min(50, iterations // 20)\n    for i in range(refinement_steps):\n        step_size = 0.02 * (1 - i/refinement_steps)\n        for _ in range(3):  # Multiple tries per step\n            new_x = np.clip(best_x + np.random.normal(0, step_size), *bounds)\n            new_y = np.clip(best_y + np.random.normal(0, step_size), *bounds)\n            new_value = evaluate_function(new_x, new_y)\n            \n            if new_value < best_value:\n                best_value = new_value\n                best_x, best_y = new_x, new_y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on value_score (0.9997), Performs well on distance_score (0.9992), Performs well on combined_score (1.4994), Performs well on reliability_score (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.4987)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Enhanced simulated annealing parameters\n        temp = 2.0  # Higher initial temperature for better exploration\n        initial_temp = temp\n        # Adaptive cooling: faster cooling when temperature is high, slower when low\n        base_cooling = 0.995\n        # Also add occasional reheating to escape deep local minima\n        reheat_frequency = iterations // (num_starts * 4)\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        # Initialize momentum\n        momentum_x, momentum_y = 0, 0\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on iteration progress and temperature\n            progress = i / (iterations // num_starts)\n            temp_ratio = temp  # Temperature affects step size\n            current_step_size = step_size * (1 - progress) * temp_ratio\n            \n            # Generate candidate with momentum and Gaussian perturbation\n            random_dx = np.random.normal(0, current_step_size)\n            random_dy = np.random.normal(0, current_step_size)\n            \n            # Combine momentum with random walk (momentum helps escape local minima)\n            dx = 0.3 * momentum_x + 0.7 * random_dx\n            dy = 0.3 * momentum_y + 0.7 * random_dy\n            \n            candidate_x = x + dx\n            candidate_y = y + dy\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with enhanced temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n                # Update momentum in direction of improvement\n                momentum_x = dx * 0.5\n                momentum_y = dy * 0.5\n            else:\n                # Enhanced acceptance for escaping local minima\n                delta = candidate_value - current_value\n                # Adaptive acceptance based on search progress and temperature\n                progress_factor = 1.0 + (iterations // num_starts - i) / (iterations // num_starts)\n                acceptance_prob = np.exp(-delta / (temp * progress_factor))\n                \n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n                    # Reduce momentum on uphill moves but don't zero it\n                    momentum_x *= 0.3\n                    momentum_y *= 0.3\n                else:\n                    # Decay momentum when move is rejected\n                    momentum_x *= 0.8\n                    momentum_y *= 0.8\n            \n            # Adaptive cooling based on temperature level\n            if temp > initial_temp * 0.5:\n                temp *= base_cooling  # Normal cooling\n            else:\n                temp *= (base_cooling + 0.002)  # Slower cooling at low temperatures\n            \n            # Occasional reheating to escape deep local minima\n            if i % reheat_frequency == 0 and i > 0 and temp < initial_temp * 0.1:\n                temp = min(temp * 2.0, initial_temp * 0.3)\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Get the best result from all starts\n    best_x, best_y, best_value = min(best_results, key=lambda x: x[2])\n    \n    # Final local refinement phase\n    refinement_steps = iterations // 20\n    refinement_step_size = (bounds[1] - bounds[0]) * 0.005\n    \n    for i in range(refinement_steps):\n        # Very small Gaussian perturbations for fine-tuning\n        dx = np.random.normal(0, refinement_step_size * (1 - i/refinement_steps))\n        dy = np.random.normal(0, refinement_step_size * (1 - i/refinement_steps))\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n### Program D2 (Score: 1.4967)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 candidates for better coverage (increased from 3)\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters with better cooling schedule\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Track global best for better convergence\n    best_idx = min(range(5), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for i in range(iterations):\n        # Update temperature with exponential cooling\n        temperature *= cooling_rate\n        \n        for j, (x, y, current_value) in enumerate(candidates):\n            # Adaptive step size based on iteration progress\n            if i < iterations // 3:\n                # Early exploration: larger steps\n                step_scale = 2.0 * (1 - i/iterations)\n            else:\n                # Late refinement: smaller steps focused around best\n                step_scale = max(0.1, 0.5 * (1 - i/iterations))\n            \n            # Local perturbation with Gaussian noise\n            new_x = x + np.random.normal(0, step_scale)\n            new_y = y + np.random.normal(0, step_scale)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability if worse (simulated annealing)\n            delta = new_value - current_value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[j] = (new_x, new_y, new_value)\n                \n                # Update global best when we find a better solution\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n        \n        # Strategic long-range exploration: replace worst candidate\n        if i % (iterations // 8) == 0 and i > 0:\n            worst_idx = max(range(5), key=lambda i: candidates[i][2])\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            value = evaluate_function(x, y)\n            candidates[worst_idx] = (x, y, value)\n            \n            # Update global best if needed\n            if value < best_value:\n                best_value = value\n                best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to value_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.7962, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with momentum-based local search.\n    Escapes local minima through adaptive temperature, momentum, and multi-stage refinement.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Enhanced simulated annealing parameters\n        temp = 2.0  # Higher initial temperature for better exploration\n        initial_temp = temp\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        # Initialize momentum for better escape from local minima\n        momentum_x, momentum_y = 0, 0\n        \n        for i in range(iterations // num_starts):\n            # Adaptive step size based on iteration progress and temperature\n            progress = i / (iterations // num_starts)\n            temp_ratio = temp / initial_temp\n            current_step_size = step_size * (1 - progress) * temp_ratio\n            \n            # Generate candidate with momentum and Gaussian perturbation\n            random_dx = np.random.normal(0, current_step_size)\n            random_dy = np.random.normal(0, current_step_size)\n            \n            # Combine momentum with random walk for better exploration\n            dx = 0.3 * momentum_x + 0.7 * random_dx\n            dy = 0.3 * momentum_y + 0.7 * random_dy\n            \n            candidate_x = x + dx\n            candidate_y = y + dy\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with enhanced temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n                # Update momentum in direction of improvement\n                momentum_x = dx * 0.5\n                momentum_y = dy * 0.5\n            else:\n                # Enhanced acceptance for escaping local minima\n                delta = candidate_value - current_value\n                # Adaptive acceptance based on search progress\n                progress_factor = 1.0 + (iterations // num_starts - i) / (iterations // num_starts)\n                acceptance_prob = np.exp(-delta / (temp * progress_factor))\n                \n                if np.random.random() < acceptance_prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n                    # Reduce momentum on uphill moves but don't zero it\n                    momentum_x *= 0.3\n                    momentum_y *= 0.3\n                else:\n                    # Decay momentum when move is rejected\n                    momentum_x *= 0.8\n                    momentum_y *= 0.8\n            \n            # Cool down temperature\n            temp *= cooling_rate\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Get the best result from all starts\n    best_x, best_y, best_value = min(best_results, key=lambda x: x[2])\n    \n    # Final local refinement phase for fine-tuning\n    refinement_steps = iterations // 20\n    refinement_step_size = (bounds[1] - bounds[0]) * 0.005\n    \n    for i in range(refinement_steps):\n        # Very small Gaussian perturbations for fine-tuning\n        dx = np.random.normal(0, refinement_step_size * (1 - i/refinement_steps))\n        dy = np.random.normal(0, refinement_step_size * (1 - i/refinement_steps))\n        \n        new_x = np.clip(best_x + dx, bounds[0], bounds[1])\n        new_y = np.clip(best_y + dy, bounds[0], bounds[1])\n        new_value = evaluate_function(new_x, new_y)\n        \n        if new_value < best_value:\n            best_x, best_y, best_value = new_x, new_y, new_value\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 56 lines with 101 lines, Excellent runs_successfully (1.000), Excellent value_score (0.962)\n\n### Inspiration 2 (Score: 1.4967, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Advanced hybrid search combining simulated annealing, local search, and multi-start strategy.\n    \n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n    \n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multi-start with 5 candidates for better coverage (increased from 3)\n    candidates = []\n    for _ in range(5):\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        value = evaluate_function(x, y)\n        candidates.append((x, y, value))\n    \n    # Simulated annealing parameters with better cooling schedule\n    initial_temp = 10.0\n    final_temp = 0.01\n    cooling_rate = (final_temp / initial_temp) ** (1.0 / iterations)\n    temperature = initial_temp\n    \n    # Track global best for better convergence\n    best_idx = min(range(5), key=lambda i: candidates[i][2])\n    best_x, best_y, best_value = candidates[best_idx]\n    \n    for i in range(iterations):\n        # Update temperature with exponential cooling\n        temperature *= cooling_rate\n        \n        for j, (x, y, current_value) in enumerate(candidates):\n            # Adaptive step size based on iteration progress\n            if i < iterations // 3:\n                # Early exploration: larger steps\n                step_scale = 2.0 * (1 - i/iterations)\n            else:\n                # Late refinement: smaller steps focused around best\n                step_scale = max(0.1, 0.5 * (1 - i/iterations))\n            \n            # Local perturbation with Gaussian noise\n            new_x = x + np.random.normal(0, step_scale)\n            new_y = y + np.random.normal(0, step_scale)\n            \n            # Keep within bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n            \n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept if better, or with probability if worse (simulated annealing)\n            delta = new_value - current_value\n            if delta < 0 or (temperature > 0.1 and np.random.random() < np.exp(-delta / temperature)):\n                candidates[j] = (new_x, new_y, new_value)\n                \n                # Update global best when we find a better solution\n                if new_value < best_value:\n                    best_value = new_value\n                    best_x, best_y = new_x, new_y\n        \n        # Strategic long-range exploration: replace worst candidate\n        if i % (iterations // 8) == 0 and i > 0:\n            worst_idx = max(range(5), key=lambda i: candidates[i][2])\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n            value = evaluate_function(x, y)\n            candidates[worst_idx] = (x, y, value)\n            \n            # Update global best if needed\n            if value < best_value:\n                best_value = value\n                best_x, best_y = x, y\n    \n    return best_x, best_y, best_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 56 lines with 76 lines, Excellent runs_successfully (1.000), Excellent value_score (1.000)\n\n### Inspiration 3 (Score: 1.3441, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid optimization algorithm combining simulated annealing with local search.\n    Escapes local minima through temperature-based acceptance and adaptive step sizes.\n    \"\"\"\n    # Initialize with multiple starting points for better coverage\n    num_starts = 5\n    best_results = []\n    \n    for start in range(num_starts):\n        # Simulated annealing parameters\n        temp = 1.0\n        cooling_rate = 0.995\n        \n        # Initialize current point\n        x = np.random.uniform(bounds[0], bounds[1])\n        y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(x, y)\n        \n        best_x, best_y, best_value = x, y, current_value\n        \n        # Adaptive step size based on bounds\n        step_size = (bounds[1] - bounds[0]) * 0.1\n        \n        for i in range(iterations // num_starts):\n            # Generate candidate with adaptive step size\n            angle = np.random.uniform(0, 2 * np.pi)\n            step = step_size * (1 - i / (iterations // num_starts))  # Decrease step size\n            candidate_x = x + step * np.cos(angle)\n            candidate_y = y + step * np.sin(angle)\n            \n            # Ensure bounds\n            candidate_x = np.clip(candidate_x, bounds[0], bounds[1])\n            candidate_y = np.clip(candidate_y, bounds[0], bounds[1])\n            \n            candidate_value = evaluate_function(candidate_x, candidate_y)\n            \n            # Accept if better, or with temperature-based probability\n            if candidate_value < current_value:\n                x, y, current_value = candidate_x, candidate_y, candidate_value\n                if candidate_value < best_value:\n                    best_x, best_y, best_value = candidate_x, candidate_y, candidate_value\n            else:\n                # Accept worse solutions with decreasing probability\n                delta = candidate_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y, current_value = candidate_x, candidate_y, candidate_value\n            \n            # Cool down temperature\n            temp *= cooling_rate\n        \n        best_results.append((best_x, best_y, best_value))\n    \n    # Return the best result from all starts\n    return min(best_results, key=lambda x: x[2])\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\nUnique approach: Modification: Change 1: Replace 27 lines with 56 lines, Excellent runs_successfully (1.000), Excellent value_score (0.982)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid simulated annealing with local refinement to escape local minima.\n\n    Args:\n        iterations: Number of iterations to run\n        bounds: Bounds for the search space (min, max)\n\n    Returns:\n        Tuple of (best_x, best_y, best_value)\n    \"\"\"\n    # Multiple restarts from different starting points\n    num_restarts = 5\n    restart_iterations = iterations // num_restarts\n    \n    best_global_x = np.random.uniform(bounds[0], bounds[1])\n    best_global_y = np.random.uniform(bounds[0], bounds[1])\n    best_global_value = evaluate_function(best_global_x, best_global_y)\n    \n    for restart in range(num_restarts):\n        # Initialize restart point\n        if restart == 0:\n            x, y = best_global_x, best_global_y\n        else:\n            x = np.random.uniform(bounds[0], bounds[1])\n            y = np.random.uniform(bounds[0], bounds[1])\n        \n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n        \n        for i in range(restart_iterations):\n            # Adaptive step size based on iteration progress\n            step_scale = max(0.1, 2.0 * (1 - i/restart_iterations))\n            \n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n            \n            new_x = np.clip(x + dx, bounds[0], bounds[1])\n            new_y = np.clip(y + dy, bounds[0], bounds[1])\n            new_value = evaluate_function(new_x, new_y)\n            \n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n            \n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n        \n        # Update global best\n        if best_restart_value < best_global_value:\n            best_global_value = best_restart_value\n            best_global_x, best_global_y = best_restart_x, best_restart_y\n    \n    return best_global_x, best_global_y, best_global_value\n\n\n# EVOLVE-BLOCK-END\n\n\n# This part remains fixed (not evolved)\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n\n\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program and the performance history, I can see that while the algorithm achieves 100% success rate, there's a slight decline in fitness from 1.4994 to 1.4962. The current approach uses multiple restarts with simulated annealing, but I notice several areas for improvement based on the top-performing programs.\n\nHere are my targeted improvements:\n\n## 1. Enhanced Temperature Schedule and Adaptive Parameters\n\nThe current temperature decay is too aggressive (0.95) and doesn't adapt to search progress. I'll implement a more sophisticated cooling schedule with reheating capabilities.\n\n```python\n<<<<<<< SEARCH\n        # Initial temperature for simulated annealing\n        temp = 2.0\n        temp_decay = 0.95\n=======\n        # Enhanced temperature schedule with reheating\n        temp = 10.0  # Higher initial temperature for better exploration\n        initial_temp = temp\n        cooling_rate = 0.995  # Slower cooling\n        stagnation_counter = 0\n        last_best = float('inf')\n>>>>>>> REPLACE\n```\n\n## 2. Add Momentum-Based Search\n\nThe top performers use momentum to escape local minima more effectively. I'll add momentum tracking to the search.\n\n```python\n<<<<<<< SEARCH\n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n=======\n        current_value = evaluate_function(x, y)\n        best_restart_value = current_value\n        best_restart_x, best_restart_y = x, y\n        \n        # Initialize momentum for better escape from local minima\n        momentum_x, momentum_y = 0, 0\n>>>>>>> REPLACE\n```\n\n## 3. Improved Candidate Generation with Momentum\n\nReplace the simple Gaussian perturbation with momentum-enhanced movement for better exploration.\n\n```python\n<<<<<<< SEARCH\n            # Generate candidate with local search bias\n            if i < restart_iterations * 0.7:\n                # Global exploration with simulated annealing\n                dx = np.random.normal(0, step_scale)\n                dy = np.random.normal(0, step_scale)\n            else:\n                # Local refinement around best point\n                dx = np.random.normal(0, step_scale * 0.3)\n                dy = np.random.normal(0, step_scale * 0.3)\n=======\n            # Generate candidate with momentum and adaptive noise\n            random_dx = np.random.normal(0, step_scale)\n            random_dy = np.random.normal(0, step_scale)\n            \n            # Combine momentum with random walk for better exploration\n            dx = 0.3 * momentum_x + 0.7 * random_dx\n            dy = 0.3 * momentum_y + 0.7 * random_dy\n>>>>>>> REPLACE\n```\n\n## 4. Enhanced Acceptance Criteria with Progress Awareness\n\nImprove the acceptance probability calculation to be more adaptive to search progress.\n\n```python\n<<<<<<< SEARCH\n            # Accept or reject based on simulated annealing\n            if new_value < current_value:\n                # Always accept better solutions\n                x, y = new_x, new_y\n                current_value = new_value\n            else:\n                # Sometimes accept worse solutions to escape local minima\n                delta = new_value - current_value\n                prob = np.exp(-delta / temp)\n                if np.random.random() < prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n=======\n            # Enhanced acceptance criteria with progress awareness\n            if new_value < current_value:\n                # Always accept better solutions and update momentum\n                x, y = new_x, new_y\n                current_value = new_value\n                # Update momentum in direction of improvement\n                momentum_x = dx * 0.5\n                momentum_y = dy * 0.5\n            else:\n                # Enhanced acceptance for escaping local minima\n                delta = new_value - current_value\n                # Adaptive acceptance based on search progress\n                progress_factor = 1.0 + (restart_iterations - i) / restart_iterations\n                acceptance_prob = np.exp(-delta / (temp * progress_factor))\n                \n                if np.random.random() < acceptance_prob:\n                    x, y = new_x, new_y\n                    current_value = new_value\n                    # Reduce momentum on uphill moves but don't zero it\n                    momentum_x *= 0.3\n                    momentum_y *= 0.3\n                else:\n                    # Decay momentum when move is rejected\n                    momentum_x *= 0.8\n                    momentum_y *= 0.8\n>>>>>>> REPLACE\n```\n\n## 5. Intelligent Temperature Management\n\nAdd stagnation detection and reheating to prevent premature convergence.\n\n```python\n<<<<<<< SEARCH\n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n            \n            # Cool down temperature\n            temp *= temp_decay\n=======\n            # Update best for this restart\n            if current_value < best_restart_value:\n                best_restart_value = current_value\n                best_restart_x, best_restart_y = x, y\n                stagnation_counter = 0  # Reset stagnation counter\n            else:\n                stagnation_counter += 1\n            \n            # Intelligent temperature management\n            if stagnation_counter > restart_iterations // 10 and temp < initial_temp * 0.1:\n                # Reheat when stagnated\n                temp = min(temp * 2.0, initial_temp * 0.3)\n                stagnation_counter = 0\n            else:\n                # Adaptive cooling based on temperature level\n                if temp > initial_temp * 0.5:\n                    temp *= cooling_rate  # Normal cooling\n                else:\n                    temp *= (cooling_rate + 0.002)  # Slower cooling at low temperatures\n>>>>>>> REPLACE\n```\n\n## 6. Add Multi-Scale Refinement Phase\n\nImplement a final refinement phase with decreasing step sizes for better precision.\n\n```python\n<<<<<<< SEARCH\n    return best_global_x, best_global_y, best_global_value\n=======\n    # Multi-scale refinement phase for better precision\n    refinement_budget = min(100, iterations // 10)\n    scales = [0.1, 0.03, 0.01, 0.003]\n    \n    for scale_idx, base_scale in enumerate(scales):\n        scale_iterations = refinement_budget // len(scales)\n        \n        for i in range(scale_iterations):\n            # Adaptive step size with decay\n            progress = i / scale_iterations\n            step_size = base_scale * (1 - progress * 0.7)\n            \n            # Generate multiple candidates in small neighborhood\n            for attempt in range(3):\n                if attempt < 2:\n                    # Gaussian perturbation\n                    dx = np.random.normal(0, step_size)\n                    dy = np.random.normal(0, step_size)\n                else:\n                    # Uniform perturbation for diversity\n                    dx = np.random.uniform(-step_size, step_size)\n                    dy = np.random.uniform(-step_size, step_size)\n                \n                new_x = np.clip(best_global_x + dx, bounds[0], bounds[1])\n                new_y = np.clip(best_global_y + dy, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                \n                if new_value < best_global_value:\n                    best_global_value = new_value\n                    best_global_x, best_global_y = new_x, new_y\n                    # Found improvement, can be more aggressive\n                    break\n    \n    return best_global_x, best_global_y, best_global_value\n>>>>>>> REPLACE\n```\n\nThese improvements combine the best features from the top-performing programs:\n1. **Momentum-based search** for better escape from local minima\n2. **Adaptive temperature schedule** with reheating to prevent stagnation\n3. **Progress-aware acceptance criteria** that adapts throughout the search\n4. **Multi-scale refinement** for fine-tuning the final solution\n5. **Intelligent stagnation detection** and recovery mechanisms\n\nThe changes maintain the multi-restart structure while adding sophisticated mechanisms for escaping local minima and improving convergence precision, which should help recover the lost fitness and potentially exceed previous performance levels."]}}, "artifacts_json": "{\"stage1_result\": \"Found solution at x=-1.7044, y=0.6775 with value=-1.5187\", \"distance_to_global\": \"0.0007\", \"solution_quality\": \"Distance < 0.5: Very close\", \"convergence_info\": \"Converged in 10 trials with 10 successes\", \"best_position\": \"Final position: x=-1.7042, y=0.6776\", \"average_distance_to_global\": \"0.3995\", \"search_efficiency\": \"Success rate: 100.00%\"}", "artifact_dir": null, "embedding": null}